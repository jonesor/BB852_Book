[
["index.html", "BB852 - Data handling, visualisation and statistics Chapter 1 Preface 1.1 Data wrangling 1.2 Data visualisation 1.3 Statistics 1.4 General approach 1.5 Data sources 1.6 Acknowledgements 1.7 Error reporting", " BB852 - Data handling, visualisation and statistics Owen Jones 2020-08-26 Chapter 1 Preface This book has been written to accompany the course, BB852 - Data Handling, Visualisation and Statistics. It is available as a website (https://jonesor.github.io/BB852_Book/) or as a PDF (http://bit.ly/BB852Book, or click the link at the top of the web page). I recommend to use the website where possible because the formatting is sometimes messy on the PDF, but the PDF is useful if you want a copy for offline use. The book is a “work in progress” and will change during the course. The latest version can always be found by downloading it again. It is designed to be followed along with the course lectures. Most subject chapters finish with some exercises to apply the material covered. The course is divided into three parts: Data wrangling Data visualisation Statistics 1.1 Data wrangling The term data wrangling covers manipulation of data, for example collected from an experiment or observational study, from its raw form to a form that is ready for analysis, or summarised into tables. It includes reshaping, transforming, filtering and augmenting from other data. This book covers these processes in R mainly using the tools from the dplyr and tidyr packages. 1.2 Data visualisation Graphing data is a crucial analytical step that can both highlight problems with the data (e.g. errors and outliers) and can inform on appropriate statistical approaches to take. This book covers the use of ggplot2 to make high quality, publication-ready plots. 1.3 Statistics Statistics is a HUGE field and this book does not attempt to cover more than a small fraction of it. Instead it focusses on (ordinary) linear models and generalised linear models. In a nutshell, linear models model the effects of explanatory variables on a continuous response variable with a gaussian (normal) error distribution while generalised linear models (GLMs) offer a more flexible approach that allows the response variable to have non-normal error distributions. This flexibility allows the more-appropriate modelling of phenomena including integer counts (e.g. number of individuals, or species, or events), binary (0/1) data (e.g. survived/died) or binomial count data (e.g. counts of successess and failures). It is important to realise that most commonly-used statistical methods including t-tests, ANOVA, ANCOVA, n-way ANOVA, and of course linear and multiple regression are all special cases of linear models. 1.4 General approach My general approach with communicating these methods and ideas is to teach using examples. Therefore, the bulk of the text here consists of walk-throughs of manipulating, plotting and analysing real data. For the statistics section I focus on communicating the “gist” of the underlying mathematical machinery rather than the nitty-gritty details. If you find yourself interested in these details then there are more specialist textbooks available. 1.5 Data sources This book uses numerous data sets in examples, most of which are real datasets obtained from published works, or collected by me. The data sets can be found at the following link: https://www.dropbox.com/sh/z8iv9fl9l00bm0w/AAD1WjFwcrr1ERugpunp_YH-a?dl=0 1.6 Acknowledgements These materials are inspired by the excellent textbook, “Getting Started With R” (2nd edition) by Andrew Beckerman, Dylan Childs and Owen Petchey and Dylan Childs’, which is the recommended textbook for BB852, and by materials for the Sheffield University course “AP 240 - Data Analysis And Statistics With R” ([https://dzchilds.github.io/stats-for-bio/]). 1.7 Error reporting Please report any errors you find (even small ones) to me at jones@biology.sdu.dk "],
["getting-acquainted-with-r.html", "Chapter 2 Getting acquainted with R 2.1 Introduction 2.2 Getting started with R 2.3 Getting help 2.4 R as a fancy calculator 2.5 Objects in R 2.6 Missing values, infinity and “non-numbers” 2.7 Basic information about objects 2.8 Data frames 2.9 Organising your work 2.10 “Classes” in R 2.11 Tables and summary statistics 2.12 Plotting data 2.13 Exercises - Getting acquainted with R", " Chapter 2 Getting acquainted with R 2.1 Introduction In this course we will be learning how manipulate, visualise and analyse data statistically using R. R is a programming language for data analysis and statistics. It is free and very widely used. One of its strengths is its very wide user base which means that there are hundreds of contributed packages for every concievable type of analysis. The aim of these introductory sections is to give a basic introduction to the programming language as a tool for importing, manipulating, and exploring data. In later sections we will learn more about statistical analysis. Before proceeding you will need to ensure you have a recent version of R installed on your computer (the current version is 4.0.3). Do this: Check your R version, and/or install R on your own computer now. In this course we will not be using R on its own. Instead, we will be using it with RStudio. R and RStudio are not the same thing. It is possible to run R without RStudio, but RStudio will not work if R is not installed. So what is RStudio? RStudio, essentially, is a helpful piece of software that makes R easier to use. The three most useful features are: The R Console - this is where R runs inside RStudio. We can work directly with R by typing commands into this “console”. It is also where outputs (results) from R are printed to the screen. The Code Editor - this is where you can write R programs (called “scripts”)“, which are a set of commands/instructions in the R language saved to a text file. It is much easier to work with scripts using RStudio than with ordinary text editors like Notepad. For example, it colour codes the text to make it easier to read and it will”autocomplete\" some text to speed up your work. Useful “point-and-click” tools - RStudio can help with tasks like importing data, managing files, reading help files, and managing/installing packages. Doing these things is trickier in just R - RStudio just makes things easier! You should do your coding from within RStudio. You can download the RStudio Desktop from https://rstudio.com/products/rstudio/download/. Select the correct version for your computer (Mac/Windows) and follow the usual instructions. Do this: Install RStudio Desktop on your computer. 2.2 Getting started with R In RStudio, create a new “R Script” file. Scripts are essentially programs that can be saved to allow you to return to your work in the future. They also make debugging of errors much easier. You can use the menu to do create a new R Script (File &gt; New File &gt; R Script), but there’s also a keyboard shortcut (Windows: Ctrl+Shift+N; Mac: Cmd+Shift+N). If you save (Windows: Ctrl+S; Mac: Cmd+S), you will be prompted for a file name. Make sure it has the suffix “.R” which denotes an R script file. Save the file in a folder with a memorable name (e.g. BB852_Work). When you double click on this file in future, it should automatically open in RStudio (if it doesn’t you should be able to right-click and select Open with...). In RStudio you can execute commands using the “run” icon at the top of the script window, or by selecting the text and typing the shortcut Ctrl+Enter (Windows) or Cmd+Enter (Mac). Another helpful feature of RStudio is that it will colour-code the syntax that you type, making it easier to read and debug. Note that the colours you see may be different from the ones shown in this handout. You can customise the look of RStudio using by clicking Tools ▶ Options menu on Windows or RStudio ▶ Preferences` on a Mac. I will point out some of this in the lecture, or you can ask me to show you. Over the next few pages I will introduce the basics of the R programming language. Try typing them into the scripting window (top left) in RStudio and ensuring that you understand what the commands are doing. It is impossible to “break” R by typing the wrong command so I encourage you to experiment and explore the R language I introduce to you here as much as possible - it really is the best way to learn! 2.3 Getting help R features a wealth of commands, which are more properly termed functions. You will learn many of these over the next few weeks. Functions often feature a several options which are specified with arguments. For example, the function sum, has the argument ..., which is intended to be one or more vectors of numbers (see below), and the argument na.rm, which is a logical argument specifying whether or not missing values should be removed or not. Usually the arguments have default options which are used you choose not to specify them. In addition, you don’t necessarily need to fully-specify the argument if they are specified in the correct order. You can get help on R functions from within R/RStudio with the ? and help.search commands. ? requires that you know the function name while help.search will search all the available help files for a particular word or phrase. ?? is a synonym for help.search: ?rep help.search(&quot;bar plot&quot;) ??&quot;bar plot&quot; In RStudio, the help results will appear in the lower right hand area. 2.4 R as a fancy calculator R features the usual arithmetic operations for addition, subtraction, division, multiplication: 4+3 ## [1] 7 9-12 ## [1] -3 6/3 ## [1] 2 7*3 ## [1] 21 (2*7)+2-0.4 ## [1] 15.6 R also has commands for square root (sqrt), raising to powers (^), taking the absolute value (abs), and rounding (round), natural log (log), antilog (exp), log to base-10 (log10): sqrt(945) ## [1] 30.74085 3^5 ## [1] 243 abs(-23.4) ## [1] 23.4 round(2.35425,digits=2) ## [1] 2.35 log(1.2) ## [1] 0.1823216 exp(1) ## [1] 2.718282 log10(6) ## [1] 0.7781513 Another thing you can do is evaluate TRUE/FALSE conditions: 3&lt;10 ## [1] TRUE 5&gt;7 ## [1] FALSE 5==5 ## [1] TRUE 6!=5 ## [1] TRUE 3 %in% c(1,2,3,4,5) ## [1] TRUE 6 %in% c(1,2,3,4,5) ## [1] FALSE 2.5 Objects in R R is an object oriented programming language. This means that it represents concepts as objects that have data fields describing the object. These objects can be manipulated by functions. Objects can include data, but also models. Don’t worry about these distinctions too much for now - all will become clear as you proceed! Objects are assigned names in R like this. The “&lt;-” command is pronounced “gets” so I would pronounce the following as “x gets four”: x &lt;- 4 To look at any object (function or data), just type its name. x ## [1] 4 The main data object types in R are: vectors, data frames, lists and matrices. We will focus on the first two of these during this course. A vector is simply a series of data (e.g. the sequence 1, 2, 3, 4, 5 is a vector, so is the non-numeric sequence Male, Female, Female, Male, Male ). Each item in a vector is called an element. Therefore, both of these examples contain 5 elements. There are several ways to create vectors in R. For example, you can make vectors of integers using the colon (:) function (e.g. 1:5), or vectors of any kind of variable using the c function. c stands for concatenate, which means to join (things) together in a chain or series. Other convenient functions for making vectors are seq, which builds a sequence of numbers according to some rules, and rep which builds a vector by repeating elements a specified number of times. Try the following: A &lt;- 1:5 B &lt;- c(1,3,6,1,7,9) C &lt;- seq(1,12,2) D &lt;- seq(1,5,0.1) E &lt;- rep(c(&quot;Male&quot;,&quot;Female&quot;),each = 3) G &lt;- rep(c(&quot;Male&quot;,&quot;Female&quot;),c(2,4)) Try modifying the commands to make sure you know what the commands are doing. #Manipulating objects Objects can be manipulated (just like in real life). In R, we use functions to manipulate objects. For example, we can use the basic arithmetic functions (*, +, /,-) on a vector: B ## [1] 1 3 6 1 7 9 B*3 ## [1] 3 9 18 3 21 27 B-2 ## [1] -1 1 4 -1 5 7 You can concatonate entire vectors together using the c function. E.g. concatonating the vectors A and B from above: c(A,B) ## [1] 1 2 3 4 5 1 3 6 1 7 9 Other manipulations are also done “element-by-element”. For example, here we multiply the first element of B by 1, the second by 2, the 3rd by 3 and so on…: B * c(1,2,3,4,5,6) ## [1] 1 6 18 4 35 54 If the length of the vectors match, we can also multiply (or add/subtract/divide etc.) multiple vectors: A / B ## [1] 1.0000000 0.6666667 0.5000000 4.0000000 0.7142857 ## [6] 0.1111111 2.6 Missing values, infinity and “non-numbers” By convention, missing values in R are coded by the value “NA”. The way that particular functions handle missing values varies: sometimes the NA values are stripped out of the data, other times the function may fail. For example, if we asked for the mean value of a vector of numbers with an NA value, it will fail: mean(c(1,3,6,1,7,9,NA)) ## [1] NA In this case you need to specify that any NA values should be removed before calculating the mean: mean(c(1,3,6,1,7,9,NA),na.rm=TRUE) ## [1] 4.5 Calculations can sometimes lead to answers that are plus, or minus, infinity. These values are represented in R by Inf or -Inf: 5/0 ## [1] Inf -4/0 ## [1] -Inf Other calculations lead to answers that are not numbers, and these are represented by NaN in R: 0/0 ## [1] NaN Inf-Inf ## [1] NaN 2.7 Basic information about objects You can obtain information about most objects using the summary function: summary(B) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 1.50 4.50 4.50 6.75 9.00 The functions max, min, range, and length are also useful: max(B) ## [1] 9 min(B) ## [1] 1 range(B) ## [1] 1 9 length(B) ## [1] 6 2.8 Data frames Data frames are the usual way of storing data in R. It is more-or-less the same as a worksheet in Excel. A data frame is usually made up of a number of vectors (of the same length) bound together in a single object. You can make a data frame by binding together vectors, or you can import them from outside R. This example shows the creation of a data frame in R, from 3 vectors: height &lt;- c(173, 145, 187, 155, 179, 133) sex &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;) age &lt;- c(17, 22, 32, 20, 27, 30) mydata &lt;- data.frame(height = height, age = age, sex = sex) mydata ## height age sex ## 1 173 17 Male ## 2 145 22 Female ## 3 187 32 Male ## 4 155 20 Female ## 5 179 27 Male ## 6 133 30 Female Data frames can be summarised using the summary function (or the str function, which gives you a different view of the same data): summary(mydata) ## height age sex ## Min. :133.0 Min. :17.00 Length:6 ## 1st Qu.:147.5 1st Qu.:20.50 Class :character ## Median :164.0 Median :24.50 Mode :character ## Mean :162.0 Mean :24.67 ## 3rd Qu.:177.5 3rd Qu.:29.25 ## Max. :187.0 Max. :32.00 str(mydata) ## &#39;data.frame&#39;: 6 obs. of 3 variables: ## $ height: num 173 145 187 155 179 133 ## $ age : num 17 22 32 20 27 30 ## $ sex : chr &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; ... Data frames can be subsetted using the square brackets [], or subset functions. With the square brackets, the first number specifies the row number, while the second number specifies the column number: mydata[1,] ## height age sex ## 1 173 17 Male mydata[,2] ## [1] 17 22 32 20 27 30 mydata[1,2] ## [1] 17 subset(mydata,sex == &quot;Female&quot;) ## height age sex ## 2 145 22 Female ## 4 155 20 Female ## 6 133 30 Female 2.9 Organising your work It would be incredibly tedious to enter real data by typing it in like this. There are eas Thankfully, R can import data from a several data formats, and it understands the file structure of your computer. Thus, you can use spreadsheet software (like Excel) to enter and store your data, and you can organise your project work in a sensible way in folders (sometimes called directories) on your computer. The most commonly used data format is comma separated value (CSV) so I will use that. You can also import from Excel, but the data must be formatted in a particular way to enable this (I’ll cover this in a later class). For this course, I suggest that you make a folder somewhere on your computer called “IntroToR”. We will use this as the working directory for the remainder of the session. In RStudio you can set the working directory by clicking through the menu items **Session ▶ Set Working Directory ▶ Choose Directory*`**. You can also using the setwd function to do this, if you know where your files are stored (the file path). File paths in Windows and Mac computers are expressed differently. Apple systems use the forward-slash (/) to separate folders whereas Windows can use the forward-slash (/) or double-backslash (\\). In windows you also need to define the drive (e.g. C:). So, to set the working directory in Apple OSX you would use something like this (obviously, you need to put your path!): setwd(&quot;/Users/orj/Desktop/IntroToR&quot;) While in Windows the equivalent command would be something like this (both of the following should work): setwd(&quot;C:\\\\Users\\\\orj\\\\Desktop\\\\IntroToR&quot;) setwd(&quot;C:/Users/orj/Desktop/IntroToR&quot;) Typing the path in can be annoying but there are ways to speed it up. In Windows you can copy paths from the Windows Explorer location/address bar, or you can hold down the Shift key as you right-click the file, and then choose Copy As Path. On a Mac you can copy file paths from Finder: Select your file/folder, Right click, Press the option key (on my keyboard this is the alt key) and click “Copy X as Pathname” I can check what the current working directory is using the getwd function: getwd() On the Blackboard site for BB852 I have put a link to a Dropbox folder containing data files for use in the course. In there you should be able to find a file called “carnivora.csv”. Download this to your new working directory. You can check the contents of your working directory with the list.files function: list.files() You can now import this file into R using the read.csv function. The specification of the argument header = TRUE signifies that the fiest row of our CSV file contains the column names. Note that your file path will be different to mine: carni &lt;- read.csv(&quot;CourseData/carnivora.csv&quot;,header = TRUE) We can get some basic information on the carni data frame using the summary function, but also the dim and nrow/ncol functions: summary(carni) dim(carni) ## [1] 112 17 nrow(carni) ## [1] 112 ncol(carni) ## [1] 17 We can find the names of the columns of a data frame with the names function: names(carni) ## [1] &quot;Order&quot; &quot;SuperFamily&quot; &quot;Family&quot; &quot;Genus&quot; ## [5] &quot;Species&quot; &quot;FW&quot; &quot;SW&quot; &quot;FB&quot; ## [9] &quot;SB&quot; &quot;LS&quot; &quot;GL&quot; &quot;BW&quot; ## [13] &quot;WA&quot; &quot;AI&quot; &quot;LY&quot; &quot;AM&quot; ## [17] &quot;IB&quot; The first few columns are to do with the taxonomic placement of the species (Order, SuperFamily, Family, Genus and Species). There then follow several columns of life history variables: FW = Female body weight (kg), SW = Average body weight of adult male and adult female (kg), FB = Female brain weight (g), SB = Average brain weight of adult male and adult female (g), LS = Litter size, GL = Gestation length (days), BW = Birth weight (g), WA = Weaning age (days), AI = Age of independance (days), LY = Longevity (months), AM = Age of sexual maturity (days), IB = Inter-birth interval (months). You can refer to the sub-parts of a data.frame(the columns) using the $ syntax: summary(carni$FW) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.050 1.245 3.400 18.099 10.363 320.000 2.10 “Classes” in R I have already mentioned the different object types in R (e.g. vectors and data frames). The object types are technically known as “classes”. You can find out what “class” an object is by using the class function: class(carni) ## [1] &quot;data.frame&quot; In this case, the data frame is, unsurprisingly, of class “data.frame”. However, the vectors that compose the data frame also have classes. There are several classes of vectors including “integer” (whole numbers), “numeric” (real numbers), “factor” (categorical variables) and “logical” (true/false values). I expect you have heard of the first two data types, but “factor” might be puzzling. Factors are defined as variables which can take on a limited number of different values. They are often referred to as categorical variables. For example, in the carnivore dataset, the taxonomic variables are factors. The different values that a factor can take are known as levels and you can check on the levels of a vector with the levels function. class(carni$Family) ## [1] &quot;character&quot; levels(carni$Family) ## NULL 2.11 Tables and summary statistics For vectors of class “factor” you can use the table function to give the counts for each level: table(carni$Family) ## ## Ailuridae Canidae Felidae Hyaenidae Mustelidae ## 1 18 19 4 30 ## Procyonidae Ursidae Viverridae ## 4 4 32 You can use the function tapply (“table apply”), to get more complex summary information. For example, I could ask what the mean female weight (FW) is in each of the families using the argument mean: tapply(carni$FW, carni$Family, mean) ## Ailuridae Canidae Felidae Hyaenidae Mustelidae ## 120.000000 9.050000 31.432105 33.540000 3.989000 ## Procyonidae Ursidae Viverridae ## 3.642500 198.250000 2.672813 2.12 Plotting data Basic plots can be made using the plot command. For example, let’s have a look at the relationship between log gestation length and log female body weight (see Figure 1, below): plot(log(carni$FW), log(carni$GL)) Figure 2.1: A simple scatter plot 2.13 Exercises - Getting acquainted with R 2.13.1 Background In the 1950s-1970s there was rapid growth in the number of houses being built in California, with suburbs sprawling out into the new sites in the countryside. What effect would this have on local bird communities? Surveys on bird abundances were carried out in several locations near Oakland, California.1 The locations were of different ages, enabling us to investigate what changes might happen through time. Although there were no surveys before the developments, we can regard the bird abundance in the very youngest developments as the baseline predevelopment condition. Think about what you might expect to happen to bird species diversity through time in a newly developing suburb. 2.13.2 The data The relevant data file is called suburbanBirds.csv. This file contains data on bird abundances surveyed in 1975. The columns of the data are Name (name of the suburb), Year (the year that the suburb was built), HabitatIndex (an index of habitat quality, related to tree height, garden maturity etc.), nIndividuals (number of indivual birds seen in a standard survey) and nSpecies (number of species seen in a standard survey). Additional surveys found an average species richess of 3.5 in nearby undisturbed habitats of grassland savanna. 2.13.3 Try the following First import the data. Check that the columns look as they should. (e.g. use summary or str functions). Tip: use the “Wizard” in RStudio to guide you. What is the mean, minimum, and maximum number of species seen? (there is more than one way to do this) How old are the youngest and oldest suburbs? (hint: the survey was carried out in 1975, do the math!) Plot the relationship between Year and nSpecies as a scatterplot using base-R graphics (using the plot function). The patern might be easier to see if you could replace YearBuilt with suburb age. Create a new vector in your data frame for this variable (e.g. df$Age &lt;- 1975 - Year)). Replot your results. What do the data show? What might be the mechanisms for the patterns you see? Do they match your expectations? Export your plots and paste them into a Word Document. If you get this far, try plotting the other variables in the dataset. Vale, T. R., &amp; Vale, G. R. (1976). Suburban bird populations in west-central California. Journal of Biogeography, 157–165.↩︎ "],
["data-wrangling-with-dplyr.html", "Chapter 3 Data wrangling with dplyr 3.1 Introduction 3.2 select 3.3 filter 3.4 arrange 3.5 summarise and group_by 3.6 Using pipes, saving data. 3.7 Exercises - data wrangling", " Chapter 3 Data wrangling with dplyr 3.1 Introduction We are now going to use the package dplyr which is designed to make working with data in R easier. The package has several key “workhorse” functions, sometimes called verbs. These are: filter, select, mutate, arrange and summarise. I covered these in the lecture, and they are also discussed in the textbook. This worksheet guides you throigh worked examples to illustrate their use. We will also be using pipes from the magrittr package. These are implemented using the command %&gt;%. First you must install the packages (unless you have already done so). Conveniently, everything you need is already in the Tidyverse family of packages, so you can install them all like this: install.packages(&quot;tidyverse&quot;) Note that you only need to do this ONCE! After you have installed them you only need to load them on each session. I usually do this at the start of a script I am writing. R will report some information to the screen telling you that it has attached a bunch of packages, and that there are some conflicts for some of them. This is nothing to worry about. It means that there are functions in different packages with the same name so R will get confused. For example, there is a function called filter in both the dplyr and stats package. The dplyr package will take priority, but you can still access the other one by using :: double colons like this to specify which package the function is in (stats::filter or dplyr::filter). library(tidyverse) To get to know dplyr and its functions we’ll use a data set collected from the university campus at University of Southern Denmark (SDU) The SDU bird project follows the fate of mainly great tits (musvit) and blue tits (blåmejse) in about 100 nest boxes in the woods around the main SDU campus. We will address two questions concerning clutch size (the number of eggs laid into the nest) - How does clutch size differ between blue tits and great tits? How does average clutch size vary among years? To answer these questions we need to calculate the average clutch size (number of eggs) for each nest in each year. The data are in a file called (sduBirds.csv) and are raw data collected while visiting the nests. The data will need to be processed to answer those questions. Let’s import the data and take a look at it. Make sure your data looks OK before moving on. You should first set up your working directory (e.g. a folder for the course, with a subfolder for course data etc.), and set it (with setwd)). See the earlier material for how to do this, or ask for help. df &lt;- read.csv(&quot;CourseData/sduBirds.csv&quot;) str(df) ## &#39;data.frame&#39;: 9357 obs. of 15 variables: ## $ Timestamp : chr &quot;2013-05-14&quot; &quot;2013-05-03&quot; &quot;2013-06-25&quot; &quot;2013-06-18&quot; ... ## $ Year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ Day : int 134 123 176 169 112 112 116 183 143 107 ... ## $ boxNumber : int 1 1 1 1 1 1 1 1 1 1 ... ## $ species : chr &quot;BT&quot; &quot;BT&quot; &quot;BT&quot; &quot;BT&quot; ... ## $ stage : chr &quot;NL&quot; &quot;NL&quot; &quot;NE&quot; &quot;NE&quot; ... ## $ nEggs : int 12 8 0 0 0 0 0 0 NA 0 ... ## $ nLiveChicks: int 0 0 0 0 0 0 0 0 0 0 ... ## $ nDeadChicks: int 0 0 0 0 0 0 0 0 0 0 ... ## $ eggStatus : chr &quot;WA&quot; &quot;CO, CV&quot; NA NA ... ## $ chickStatus: chr NA NA NA NA ... ## $ adultStatus: chr &quot;FN&quot; &quot;FN&quot; NA NA ... ## $ finalStatus: chr NA NA &quot;NE&quot; &quot;NE&quot; ... ## $ Comments : chr NA &quot;MA&quot; NA NA ... ## $ observerID : chr &quot;AMK&quot; &quot;AMK&quot; &quot;AMK&quot; &quot;AMK&quot; ... 3.2 select From the str summary (above) you can see that thereare many columns in the data set and we only need some of them. Let’s select only the columns that we need for our calculations to make things a bit easier to handle. We need the species, Year, Day, boxNumber and nEggs: df &lt;- select(df, species, Year, Day, boxNumber, nEggs) head(df) ## species Year Day boxNumber nEggs ## 1 BT 2013 134 1 12 ## 2 BT 2013 123 1 8 ## 3 BT 2013 176 1 0 ## 4 BT 2013 169 1 0 ## 5 BT 2013 112 1 0 ## 6 BT 2013 112 1 0 The output of head shows you the first few rows of the data set. You can see that each row represents a visit of a researcher to a particular nest. The researcher records the bird species if it is known (GT = Great tit, BT = Blue tit, NH = Nuthatch etc.), and then records the number of eggs, number of chicks, activity of the adults and so on. We need to convert this huge dataset into one which contains clutch size for each nest, for each year of the study. The information given by str (above) shows that there are data on species other than our target species. We are only interested in the great tits and blue tits so we can first filter the others out using the species variable. We can check what the make up of this part of the data is using the table function which will count up all of the entries. table(df$species) ## ## BT GT MT NH WR ## 992 4612 73 31 5 3.3 filter Now let’s filter this data and double check that this has worked: df &lt;- filter(df,species %in% c(&quot;GT&quot;,&quot;BT&quot;)) table(df$species) ## ## BT GT ## 992 4612 You will notice that all the levels of the variable are retained. This is not a problem, and can usually be ignored. You can also tidy this up using the droplevels function, which removes all unused factor levels. df &lt;- droplevels(df) table(df$species) ## ## BT GT ## 992 4612 3.4 arrange Recall that the data are records of visits to each nest a few times per week. To ensure that the data are in time order I can first arrange by first Year and then Day. To illustrate this we can make a temporary data set (called temp) to look at a particular nest in a particular year to get a record of the progress for that particular nest, and then plot it (this is an ugly plot and we will learn how to make beautiful ones soon): df &lt;- arrange(df,Year,Day) temp &lt;- filter(df,boxNumber == 1,Year == 2014) max(temp$nEggs) # get the max value ## [1] 12 plot(temp$Day,temp$nEggs,type=&quot;b&quot;) Eggs are usually laid one per day, and the clutch size is the maximum number of eggs reached for each nest box. In this case, the clutch size is 12 eggs. The rapid decline in number of eggs after this peak value shows when the eggs have hatched and the researcher finds chicks instead of eggs! 3.5 summarise and group_by The next part is the crucial part of our investigation. We need to get the maximum number of eggs seen at each nest. Of course we could repeatedly use filter, followed by max, for each nest-year combination but this would be incredibly tedious. Instead, we will use the dplyr summarise function to do this by asking for the maximum value of nEggs. To make this work we need to first use the group_by function tell R to group the data by the variables we are interested in. If we don’t do this we just get the overall maximum. We can ungroup the data using the ungroup function. Because there are missing data (NA values) we need to specify na.rm = TRUE in the argument. So first, let’s get the max per species, just to illustrate how this works: df &lt;- group_by(df,species) summarise(df,clutchSize = max(nEggs,na.rm= TRUE)) ## # A tibble: 2 x 2 ## species clutchSize ## &lt;chr&gt; &lt;int&gt; ## 1 BT 14 ## 2 GT 14 We can see how the data are grouped by asking for a summary: summary(df) ## species Year Day ## Length:5604 Min. :2013 Min. : 60.0 ## Class :character 1st Qu.:2014 1st Qu.:116.0 ## Mode :character Median :2014 Median :133.0 ## Mean :2015 Mean :133.2 ## 3rd Qu.:2017 3rd Qu.:148.0 ## Max. :2019 Max. :212.0 ## ## boxNumber nEggs ## Min. : 1.00 Min. : 0.000 ## 1st Qu.: 29.75 1st Qu.: 0.000 ## Median : 57.00 Median : 0.000 ## Mean : 55.83 Mean : 2.326 ## 3rd Qu.: 85.00 3rd Qu.: 4.000 ## Max. :101.00 Max. :14.000 ## NA&#39;s :613 We can ungroup the data again like this: df &lt;- ungroup(df) So both species lay the same maximum number of eggs, but maybe this is just caused by outliers for one of the species. We’ll need to dig deeper. How can we calculate the average? We cannot simply ask for the mean because the data run through time following the development in each nest. We need to calculate the maximum nEggs for each nest, and then calculate the average of those. We can do this in two steps. We first calculate the clutch size for each box for each species in each year: df &lt;- group_by(df, species, Year, boxNumber) df &lt;- summarise(df, clutchSize = max(nEggs,na.rm= TRUE)) Let’s first look at all the clutch size data: hist(df$clutchSize) You can see here that there are a lot of zero values. This is because nests were recorded even if they did not attempt to lay eggs. We should remove these from our data using filter again: df &lt;- filter(df, clutchSize &gt; 0) hist(df$clutchSize) That looks better. Now we can plot them again but this time split apart the species (again - this plot is ugly and we’ll learn to plot nicer ones soon). plot(as.factor(df$species),df$clutchSize) From these distributions it looks like the average clutch size is greater in the blue tit. We can use summarise to calculate the means. df &lt;- group_by(df,species) summarise(df, mean = mean(clutchSize),sd = sd(clutchSize)) ## # A tibble: 2 x 3 ## species mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BT 9.64 2.64 ## 2 GT 7.89 2.19 Lets now turn to the other question - how does the clutch size vary with year? df &lt;- group_by(df,species,Year) df2 &lt;- summarise(df, meanClutchSize = mean(clutchSize)) head(df2) ## # A tibble: 6 x 3 ## # Groups: species [1] ## species Year meanClutchSize ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BT 2013 8.33 ## 2 BT 2014 8.94 ## 3 BT 2016 8.86 ## 4 BT 2017 9.2 ## 5 BT 2018 10.1 ## 6 BT 2019 11 We can plot this bt first making a plot for blue tits, and then adding the points for great tits. I have used the pch argument to use filled circles for the great tits: plot(df2$Year[df2$species == &quot;BT&quot;],df2$meanClutchSize[df2$species == &quot;BT&quot;],type=&quot;b&quot;, ylim=c(0,12),xlab=&quot;Year&quot;,ylab=&quot;Clutch Size&quot;) points(df2$Year[df2$species == &quot;GT&quot;],df2$meanClutchSize[df2$species == &quot;GT&quot;], pch = 16, type=&quot;b&quot;) So it looks like the clutch size varies a fair amount from year to year, but that generally blue tits have large clutch sizes than great tits. 3.6 Using pipes, saving data. I have walked you through a step-by-step data manipulation. During that process you made made (and replaced) new data sets at each step. In practice this can be dones more smoothly using pipes (%&gt;%) to pass the result of one function into the next, and the next, and the next… You’ll get some more practice with this as we go on. Below I show how do do this to create a clutch size data set from the raw data (note that your file path will differ from mine): #Import and process data using pipes SDUClutchSize &lt;- read.csv(&quot;CourseData/sduBirds.csv&quot;) %&gt;% filter(species %in% c(&quot;GT&quot;,&quot;BT&quot;)) %&gt;% #include only GT and BT droplevels() %&gt;% #drop unwanted factor levels select(species, Year, Day, boxNumber, nEggs) %&gt;% #select columns needed group_by(species, Year, boxNumber) %&gt;% #group data summarise(clutchSize = max(nEggs,na.rm=TRUE)) %&gt;% #calculate clutch size (max eggs) filter(clutchSize &gt; 0) head(SDUClutchSize) ## # A tibble: 6 x 4 ## # Groups: species, Year [1] ## species Year boxNumber clutchSize ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BT 2013 1 12 ## 2 BT 2013 5 8 ## 3 BT 2013 27 10 ## 4 BT 2013 31 6 ## 5 BT 2013 35 10 ## 6 BT 2013 37 8 You can save this out using the write.csv function. You will need to set the argument row.names = FALSE to stop the data including row names. write.csv(x = SDUClutchSize,file = &quot;CourseData/SDUClutchSize.csv&quot;,row.names = FALSE) 3.7 Exercises - data wrangling 3.7.1 Exploring the Amniote Life History Database In this exercise the aim is to use the “Amniote Life History Database”2 to investigate some questions about life history evolution. The questions are: (1) what are the records and typical life spans in different taxonomic classes? [what is the longest, shortest and median life span in birds, mammals and reptiles?] (2) is there a positive relationship between body mass and life span? [do big species live longer than small ones?]; (3) is there a trade-off between reproductive effort and life span? [do species that reproduce a lot have short lives, so there is a negative relationship between reproduction and life span?]; (4) is this trade-off universal across all Classes? [does the trade-off exist in birds, reptiles and amphibians?] The database is in a file called Amniote_Database_Aug_2015.csv in the course data folder. The missing values (which are normally coded as NA in R) are coded as “-999”. The easiest way to take care of this is to specify this when we import the data using the na.strings argument of the read.csv function. Thus we can import the data like this: amniote &lt;- read.csv(&quot;CourseData/Amniote_Database_Aug_2015.csv&quot;,na.strings = &quot;-999&quot;) Let’s make a start… When you have imported the data, use dim to check the dimensions of the whole data frame (you should see that there are 36 columns and 21322 rows). Use names to look at the names of all columns in the data in amniote. We are interested in longevity (lifespan) and body size and reproductive effort and how this might vary depending on the taxonomy (specifically, with Class). Use select to pick relevent columns of the dataset and discard the others. Call the new data frame x. The relevant columns are the taxonomic variables (class, species) and longevity_y, litter_or_clutch_size_n, litters_or_clutches_per_y, and adult_body_mass_g. Take a look at the first few entries in the species column. You will see that it is only the epithet, the second part of the Genus_species name, that is given. Use mutate and paste to convert the species column to a Genus_species by pasting the data in genus and species together. To see how this works, try out the following command, paste(1:3, 4:6). After you have created the new column, remove the genus column (using select and -genus). What is the longest living species in the record? Use arrange to sort the data from longest to shortest longevity (longevity_y), and then look at the top of the file using head to find out. (hint: you will need to use reverse sort (-)). Cut and paste the species name into Google to find out more! Do the same thing but this time find the shortest lived species. Use summarise and group_by to make a table summarising min, median and max life spans (longevity_y) for the three taxonomic classes in the database. Remember that you need to tell R to remove the NA values using a rm.na = TRUE argument. Body size is thought to be associated with life span. Let’s treat that as a hypothesis and test it graphically. Sketch what would the graph would look like if the hypothesis were true, and if it was false. Plot adult_body_mass_g vs. longevity_y (using base R graphics). You should notice that this looks a bit messy. Use mutate to create a new log-transformed variables, logMass and logLongevity. Use these to make a “log-log” plot. You should see that makes the relationship more linear, and easier to “read”. Is there a trade-off between reproductive effort and life span? Think about this as a hypothesis - sketch what would the graph would look like if that were true, and if it was false. Now use the data to test that hypothesis: Use mutate to create a variable called logOffspring which is the logarithm of number of litters/clutches per year multiplied by the number of babies in each litter/clutch . Then plot logOffspring vs. logLongevity. To answer the final question (differences between taxonomic classes) you could now use filter to subset to particular classes and repeat the plot to see whether the relationships holds universally. Remember that if you struggle you can check back to previous work where you have used dplyr commands to manipulate data in a similar way. If you get truly stuck, ask for help from instructors or fellow students. https://esajournals.onlinelibrary.wiley.com/doi/10.1890/15-0846R.1↩︎ "],
["combining-data-sets.html", "Chapter 4 Combining data sets 4.1 Exercise - joining datasets", " Chapter 4 Combining data sets [INSERT PREAMBLE ABOUT DATA MANAGEMENT] In this exercise you will learn how to combine two data sets to create a new one of combined data to answer a conservation-related question: “Does threat status vary with species’ generation times?” This question is crucial to conservation biologists because it helps us to generalise our ideas about what drives extinction risks. In other words, if we can say “species with slow life histories tend to be more threatened” then this gives useful information that can help with planning. For example, imagine we have some species that have not yet been assessed (we don’t know if they are threatened or not). Should we focus attention on the one with a short generation time, or long generation time? To answer this question we will need to import two large data sets, tidy them up a bit and then combine them for analysis. Let’s start with the “Amniote Life History Database,”3 which is a good source of life history data. We have encountered this database before. Recall that the missing values (which are normally coded as NA in R) are coded as “-999”. The easiest way to take care of this is to specify this when we import the data using the na.strings argument of the read.csv function. Thus we can import the data like this: amniote &lt;- read.csv(&quot;CourseData/Amniote_Database_Aug_2015.csv&quot;,na.strings = &quot;-999&quot;) We first need to load the tidyverse packages. library(tidyverse) We can filter on the taxonomic class to subset to only mammals. Then, to address our question, we want data on generation time for mammals. Generation time is often measured as the average age at which females reproduce so we can get close to that with female_maturity_d. We will first select these columns, along with genus and species. We can combine these two taxonomic variables using mutate and paste to get our latin binomial species name. We have previously learned that log transforming such variables is a good thing to do, so we can use mutate again to do this transformation. Finally, we can use na.omit to get rid of entries with missing values (which we cannot use). This is not essential, but keeps things more manageable. mammal &lt;- amniote %&gt;% filter(class == &quot;Mammalia&quot;) %&gt;% #get the mammals only select(genus, species, female_maturity_d) %&gt;% #get useful columns mutate(species = paste(genus, species)) %&gt;% select(-genus) %&gt;% mutate(logMaturity = log(female_maturity_d)) %&gt;% na.omit() Let’s take a quick look at what we have: head(mammal) ## species female_maturity_d logMaturity ## 20 Echinops telfairi 278.42000 5.629131 ## 22 Hemicentetes nigriceps 48.57000 3.883006 ## 23 Hemicentetes semispinosus 46.19892 3.832956 ## 27 Microgale dobsoni 669.59200 6.506669 ## 40 Microgale talazaci 639.00000 6.459904 ## 47 Setifer setosus 198.00000 5.288267 Looks good. Now let’s import the IUCN Red List data. redlist &lt;- read.csv(&quot;CourseData/MammalRedList.csv&quot;) Let’s take a look at that. names(redlist) ## [1] &quot;Species.ID&quot; &quot;Kingdom&quot; ## [3] &quot;Phylum&quot; &quot;Class&quot; ## [5] &quot;Order&quot; &quot;Family&quot; ## [7] &quot;Genus&quot; &quot;Species&quot; ## [9] &quot;Authority&quot; &quot;Infraspecific.rank&quot; ## [11] &quot;Infraspecific.name&quot; &quot;Infraspecific.authority&quot; ## [13] &quot;Stock.subpopulation&quot; &quot;Synonyms&quot; ## [15] &quot;Common.names..Eng.&quot; &quot;Common.names..Fre.&quot; ## [17] &quot;Common.names..Spa.&quot; &quot;Red.List.status&quot; ## [19] &quot;Red.List.criteria&quot; &quot;Red.List.criteria.version&quot; ## [21] &quot;Year.assessed&quot; &quot;Population.trend&quot; ## [23] &quot;Petitioned&quot; unique(redlist$Red.List.status) ## [1] &quot;DD&quot; &quot;LC&quot; &quot;CR&quot; &quot;NT&quot; &quot;EN&quot; &quot;VU&quot; &quot;EX&quot; &quot;EW&quot; There’s a lot of information there but what we really need is simply the latin binomial (for which we need genus and species) and the threat status Red.List.status. R treats categorical variables (factor variables) as alphabetical, but in this case the red list status has a meaning going from low threat (Least Concern - LC) to Critically Endangered (CR) and even Extinct in the Wild (EX) at the other end of the spectrum. We can define this ordering using mutate with the factor function. redlist &lt;- redlist %&gt;% mutate(species = paste(Genus, Species))%&gt;% select(species, Red.List.status) %&gt;% mutate(Red.List.status = factor(Red.List.status, levels = c(&quot;LC&quot;,&quot;NT&quot;,&quot;VU&quot;,&quot;EN&quot;,&quot;CR&quot;,&quot;EW&quot;,&quot;EX&quot;))) head(redlist) ## species Red.List.status ## 1 Abditomys latidens &lt;NA&gt; ## 2 Abeomelomys sevia LC ## 3 Abrawayaomys ruschii LC ## 4 Abrocoma bennettii LC ## 5 Abrocoma boliviensis CR ## 6 Abrocoma budini &lt;NA&gt; Now we can combine this with the life history data from above using left_join. x &lt;- left_join(mammal,redlist,by = &quot;species&quot;) Let’s take a look at what we have now: head(x) ## species female_maturity_d logMaturity ## 1 Echinops telfairi 278.42000 5.629131 ## 2 Hemicentetes nigriceps 48.57000 3.883006 ## 3 Hemicentetes semispinosus 46.19892 3.832956 ## 4 Microgale dobsoni 669.59200 6.506669 ## 5 Microgale talazaci 639.00000 6.459904 ## 6 Setifer setosus 198.00000 5.288267 ## Red.List.status ## 1 LC ## 2 LC ## 3 LC ## 4 LC ## 5 LC ## 6 LC summary(x) ## species female_maturity_d logMaturity ## Length:2000 Min. : 23.81 Min. :3.170 ## Class :character 1st Qu.: 121.53 1st Qu.:4.800 ## Mode :character Median : 344.12 Median :5.841 ## Mean : 574.92 Mean :5.745 ## 3rd Qu.: 696.38 3rd Qu.:6.546 ## Max. :6391.56 Max. :8.763 ## ## Red.List.status ## LC :1219 ## VU : 176 ## EN : 168 ## NT : 114 ## CR : 66 ## (Other): 10 ## NA&#39;s : 247 You can see that there are 247 missing values for the Red List status. These are either species that have not yet been assessed, or maybe where there are mismatches in the species names between the two databases. We will ignore this problem today. Before plotting, I will also use filter remove species that are extinct (status = “EX” and “EW”). To do this I use the %in% argument to allow me to match a vector of variables. Becuase I want to NOT match them I negate the match using !. I then ensure that those levels are removed from the variable using droplevels. x &lt;- x %&gt;% filter(!Red.List.status %in% c(&quot;EX&quot;,&quot;EW&quot;)) %&gt;% droplevels() Let’s now plot the data to answer the question. plot(x$Red.List.status,x$logMaturity,ylab=&quot;Maturity&quot;) What can we see? If you focus on the median values, it looks like there is a weak positive relationship between this life history trait and threat status: animals with slower life histories tend to be more threatened. 4.1 Exercise - joining datasets In this exercise you will practice your skills using gather to convert a wide-format data sheet into a long-format tidy data set. You will then use left_join to combine two data sets for analysis to answer an interesting biological question. 4.1.1 The data Data has been collected on great tits (musvit) at SDU for several years. Your task today is to analyse these data to see if the egg laying date is associated with the temperature during the spring. The hypothesis is that warmer springs will lead to delayed egg laying which could have negative consequences to the population if their caterpillar food source doesn’t keep pace with the change. You are provided with two datasets: one on the birds and another on weather. You will need to process these, and merge them for analysis. 4.1.2 The bird data The first data set, eggDates.csv, is data from the SDU birds project. The data are arranged in columns where each column is a year and each row is a nest. The data in each column is the day of the year that the first egg in the nest was laid. These data do NOT fullfil the “tidy data” standard where each variable gets a column. In this case,a single variable (first egg date) gets many columns (one for each year), and column headers are data (the years). The data will need to be processed before you can analyse it. You will need to use gather to fix this issue so that you produce a version of the data with three columns - nestNumber, Year and dayNumber. 4.1.3 The weather data The second dataset, AarslevTemperature.csv, is a weather dataset from Aarslev near Odense. This dataset includes daily temperatures records for several years. You will need to summarise this data to obtain a small dataset that has the weather of interest - average temperature in the months of February to April for each year. You will then join these datasets together. Import the data and take a look at it with head or str. Use gather to reformat the data. This might take a bit of trial and error - don’t give up! Maybe this will help: The first argument in the gather command indicates what the columns in the main data represent (i.e. here the column represents “Year”). The second argument is the name you would give to the actual data (i.e. “day”, in this case). The final argument then tells the function which columns of the data are not to be rearranged (i.e. “boxNumber” in this case). You should end up with a dataset with three columns as described above. Ensure that year is coded as numeric variable using mutate. [Hint, you can use the command as.numeric, but first remove the “y” in the name using gsub]. Calculate the mean egg date per year using summarise (remember to group_by the year first). Take a look at the data. Import the weather data and take a look at it with head or str. Use filter subset to the months of interest (February-April) and then summarise the data to calculate the mean temperature in this period (remember to group_by year). Look at the data. You should end up with a dataset with two columns - year and meanSpringTemp. Join the two datasets together using left_join. You should now have a dataset with columns nestNumber, Year, dayNumber and meanAprilTemp plot a graph of meanAprilTemp on the x-axis and dayNumber on the y-axis. Now you should be able to answer the question we started with: is laying date associated with spring temperatures. https://esajournals.onlinelibrary.wiley.com/doi/10.1890/15-0846R.1↩︎ "],
["visualising-data-with-ggplot2.html", "Chapter 5 Visualising data with ggplot2 5.1 Introduction 5.2 Histograms 5.3 “Facets” - splitting data across panels 5.4 box plots 5.5 lines and points 5.6 scatter plot", " Chapter 5 Visualising data with ggplot2 5.1 Introduction In this worksheet you will be guided through using the ggplot package to make some pretty plots. You will need the tidyverse. library(tidyverse) We will use the SDU birds clutch size data that we produced at the end of the data wrangling practical for these examples. First you must remember to set your working directory, and then import the data. df &lt;- read.csv(&quot;CourseData/SDUClutchSize.csv&quot;) 5.2 Histograms The ggplot function expects two main arguments (1) the data and (2) the aesthetics. The aesthetics are the variables you want to plot, and associated characteristics like colours, groupings etc. The first argument is for the data, then the aesthetics are specified within the aes(...) argument. These usually include an argument for x which is normally the variable that appears on the horizontal axis, and (often) y which is usually the variable on the vertical axis. The details of this depend on the type of plot you are making. After setting up the plot the graphics are added as geometric layers or geoms. There are many of these available including geom_histogram, geom_line, geom_point etc. I will illustrate the construction of a simple plot by making a histogram of the clutch size of all the nests in the dataset. ggplot(df, aes(x = clutchSize)) This produces an empty plot because we have not yet specified what kind of plot we want. We want a histogram, so we can add this as follows. I have set binwidth to be 1 because we know we are dealing with counts between just 1 and 14. Try altering the binwidth. ggplot(df, aes(x = clutchSize)) + geom_histogram(binwidth = 1) We know that we have two species here and we would like to compare them. This is done within the aesthetic argument. The default is that the bars for different categories are stacked on top of eachother. This is good in some cases, but probably not here. ggplot(df, aes(x = clutchSize,fill = species)) + geom_histogram(binwidth = 1,position = &quot;dodge&quot;) You can immediately see that there are far fewer blue tit nests than great tit ones. But you can also see that the center of mass for blue tits is further to the right than great tits. To make it easier to compare distributions with very different counts, we can put density on the y-axis instead of the default count using the argument stat(density). ggplot(df, aes(x = clutchSize, fill = species, stat(density))) + geom_histogram(binwidth = 1,position = &quot;dodge&quot;) An alternative approach would be to overlay the two sets of bars (using position = \"identity\") and set the colours to be slightly transparent (using alpha = 0.7) so that you can see the overlapping region clearly. ggplot(df, aes(x = clutchSize, fill = species, stat(density))) + geom_histogram(binwidth = 1,position = &quot;identity&quot;,alpha=0.7) It is very clear from this plot that blue tits tend to have bigger clutch sizes than great tits. Is this difference statistically significant? We will look at testing this in a future class - for now we will be satisfied with our visualisation. 5.3 “Facets” - splitting data across panels You should recall that there were several years of data represnted here. ggplot has a very clever way of splitting up the plot to examine this. ggplot(df, aes(x = clutchSize, fill = species, stat(density))) + geom_histogram(binwidth = 1,position = &quot;identity&quot;,alpha=0.7) + facet_grid(.~Year) You could split the data up by species in a similar way, as yet another way of visualising the difference between species: ggplot(df, aes(x = clutchSize)) + geom_histogram(binwidth = 1) + facet_grid(species~.) You can change whether the separate graphs are presented in a rows or columns by changing the order of the argument: facet_grid(species~.) or facet_grid(.~species). Try it. 5.4 box plots Box plots are suitable for cases where one variable is categorical with 2+ levels, and the other is continuous. Therefore, another way to look at these distributions is to use a box plot. In a box plot the box shows the quartiles (i.e. the 25% and 75% quantiles) within which 50% of the data are found. The horizontal line in the box is the median, Then the whiskers extend from the smallest to largest value unless they are further than 1.5 times the interquartile range (the length of the box) away from the edge of the box, in which case they are individually shown as outlier points. To plot them using ggplot you must use a geom_boxplot layer. The categorical variable is normally placed on the x-axis so is placed as x in the aes argument, while the continuous variable is on the y axis. ggplot(df, aes(x = species, y = clutchSize)) + geom_boxplot() Some researchers argue that it is a good idea to add the data as points to these plots as “full disclosure” of what the underlying data look like. These can be added with a geom_jitter layer (jitter is random noise added in this case to the horizontal axis). You should set width and alpha arguments to make it look nice. ggplot(df, aes(x = species, y = clutchSize)) + geom_boxplot() + geom_jitter(width = .2, alpha = 0.5, colour=&quot;black&quot;,fill=&quot;black&quot;) Try splitting the data into different years using facet_grid with the box plot. 5.5 lines and points Perhaps not surprisingly lines and points can be added with the geoms, geom_line and geom_point respectively. To illustrate this we will make a plot showing how clutch size changes among years. First we will use summarise to create a dataset with the mean clutch size. We’ll start simply, by looking at only great tits. GTclutch &lt;- df %&gt;% filter(species == &quot;GT&quot;) %&gt;% group_by(Year) %&gt;% summarise(meanClutchSize = mean(clutchSize)) Then you can plot this like this. ggplot(GTclutch, aes(x = Year, y = meanClutchSize)) + geom_line() I think this looks OK, but we should add both species. I’ll first need to produce a mean clutch size dataset that includes both species. meanClutch &lt;- df %&gt;% group_by(species,Year) %&gt;% summarise(meanClutchSize = mean(clutchSize)) Now I can do the plot again. The only difference to the command is that I need to tell R that I want to colour the lines by species (colour = species). ggplot(meanClutch, aes(x = Year, y = meanClutchSize, colour = species)) + geom_line() I can improve on this by (1) changing the y axis limits (using ylim) so that it goes through the full range of my data (0 - 14); (2) adding points (using a geom_point layer) where my actual data values are; (3) adding a nicely formatted axis label (using ylab); adding a title (ggtitle) ggplot(meanClutch, aes(x = Year, y = meanClutchSize, colour = species)) + geom_line() + geom_point() + ylim(0,14) + ylab(&quot;Mean clutch size&quot;) + ggtitle(&quot;Clutch size data from SDU Campus&quot;) 5.6 scatter plot Finally, lets make a scatter plot. The SDU bird data are not suitable for this type of plot so we’ll use the data from a few days ago on suburban bird diversity. df &lt;- read.csv(&quot;CourseData/suburbanBirds.csv&quot;) Take a look at the data to remind ourselves what it looks like head(df) ## Name Year HabitatIndex nIndividuals nSpecies ## 1 Alamotos 1946 10.0 48 12 ## 2 Ramona 1946 9.5 30 13 ## 3 Verona 1947 9.5 38 15 ## 4 Valle Vista 1950 9.5 42 11 ## 5 La Gonda 1955 11.0 44 13 ## 6 Belgian 1956 9.0 27 14 These data show the result of standardised bird surveys at housing developments of different ages in California. The surveys were carried out in 1975, and the data includes the Year and number of individual birds seen nIndividuals and number of species seen nSpecies. The question being addressed is “How does the age of the housing development affect the number of species?” To investigate this we should first add a new variable for Age to the data set. We can do this using the mutate function from dplyr. This function creates new variables, for example by manipulating existing ones. df &lt;- mutate(df,Age = 1975 - Year) When we have created this variable we can plot the data. For aesthetic reasons I also would like to set the limits on the y-axis to go extend to zero, and I would like to include proper labels on the axes. ggplot(df, aes(x = Age,y = nSpecies)) + geom_point() + ylim(0,15) + xlab(&quot;Age of development&quot;) + ylab(&quot;Bird species richness&quot;) This shows very clearly that older developments have more species, but it also appears to show that there is an asymptote around 13 species. Compare this plot to the one you made with base graphics in a previous class. "],
["distributions-and-summarising-data.html", "Chapter 6 Distributions and summarising data 6.1 Introduction 6.2 Distributions 6.3 Normal distribution 6.4 Compare normal distributions 6.5 The Poisson distribution 6.6 Comparing normal and Poisson distributions", " Chapter 6 Distributions and summarising data 6.1 Introduction This worksheet covers two broad topics. First it covers the concept of distributions; Secondly it covers summarising data. We’ll use some functions from the tidyverse so you should load those libraries before continuing. library(tidyverse) 6.2 Distributions A statistical distribution is a description of the relative number of times (frequency) possible outcomes will occur in many trials. They are important because (1) they are useful descriptors of data and (2) they form the basis for assumptions in some statistical approaches. For example, normal distribution is often assumed. The normal distribution is symmetrical (centered on the mean) and 68% of observations fall within 1 standard deviation, and 95% of observations fall within 2 s.d.. We will use R to simulate some distributions, and explore these to get a feel for them. R has functions for generating random numbers from different kinds of distributions. For example, the function rnorm will generate numbers from a normal distribution and rpois will generate numbers from a Poisson distribution. 6.3 Normal distribution The rnorm function has three arguments. The first argument is simply the number of values you want to generate. Then, the second and third arguments specify the the mean and standard deviation values of the distribution (i.e. where the distribution is centered and how spread out it is). The following command will produce 6 numbers from a distribution with a mean value of 5 and a standard deviation of 2. rnorm(6,5,2) ## [1] 3.880114 4.448199 3.655503 10.359694 7.693923 ## [6] 2.005800 Try changing the values of the arguments to alter the number of values you generate, and to alter the mean and standard deviation. Let’s use this to generate a larger data frame, and then place markers for the various measures of “spread” onto a plot. Note that here I put a set of parentheses around the plot code to both display the result AND save the plot as an R object called p1 rn &lt;- data.frame(d1 = rnorm(500,5,2)) summary(rn) #Take a look ## d1 ## Min. :-0.7199 ## 1st Qu.: 3.8196 ## Median : 5.0766 ## Mean : 5.0622 ## 3rd Qu.: 6.5164 ## Max. :11.1718 #Plot the data (p1 &lt;- ggplot(rn,aes(x=d1)) + geom_histogram() ) We can calculate the mean and standard deviation using summarise (along with other estimates of “spread”). The mean and standard deviation values will be close (but not identical) to the values you set when you generated the distribution. Note that here I put a set of parentheses around the code to both display the result AND save the result in an object called sv (sv &lt;- rn %&gt;% summarise(meanEst = mean(d1), sdEst = sd(d1), varEst = var(d1), semEst = sd(d1)/sqrt(n())) ) ## meanEst sdEst varEst semEst ## 1 5.06219 1.905303 3.63018 0.08520775 Let’s use the function geom_vline to add some markers to the plot from above to show these values… (p2 &lt;- p1 + geom_vline(xintercept = sv$meanEst, size=2) + #mean geom_vline(xintercept = sv$meanEst+sv$sdEst, size=1) + #upperSD geom_vline(xintercept = sv$meanEst-sv$sdEst, size=1) #lowerSD ) We can compare these with the true values (the values we set when we generated the data), by adding them to the plot in a different colour (mean=5, sd=2). (p3 &lt;- p2 + geom_vline(xintercept = 5, size=2, colour=&quot;red&quot;) + #mean geom_vline(xintercept = 5+2, size=1,colour=&quot;red&quot;) + #upperSD geom_vline(xintercept = 5-2, size=1,colour=&quot;red&quot;) #lowerSD ) Try repeating these plots with data that has different sample sizes. For example, use sample sizes of 5000, 250, 100, 50, 10. What do you notice? You should notice that for smaller sample sizes, the true distribution is not captured very well. When you calculate the mean and standard deviation, you are actually fitting a simple model: the mean and sd are parameters of the model, which assumes that the data follow a normal distribution. Try adding lines for the standard error of the mean to one of your histograms. 6.4 Compare normal distributions Because normal distributions all have the same shape, it can be hard to grasp the effect of changing the distribution’s parameters viewing them in isolation. In this section you will write some code to compare two normal distributions. This approach can be useful when considering whether a proposed experiment will successfully detect a difference between treatment groups. We’ll look at this topic, known as “power analysis”, in greater detail in a later class. For now we will simply use ggplot to get a better feel for the normal distribution. Let’s use rnorm to generate a larger data frame with two sets of numbers from different distributions: (d1: mean = 5, sd = 2; d2: mean = 8, sd = 1). rn &lt;- data.frame(d1 = rnorm(500,5,2),d2 = rnorm(500,8,1)) summary(rn) ## d1 d2 ## Min. :-2.540 Min. : 4.991 ## 1st Qu.: 3.852 1st Qu.: 7.436 ## Median : 5.196 Median : 8.040 ## Mean : 5.106 Mean : 8.070 ## 3rd Qu.: 6.456 3rd Qu.: 8.716 ## Max. :10.670 Max. :11.120 The summaries (above) show that the mean and the width of the distributions vary, but we should always plot our data. So lets make a plot in ggplot. In the dataset I created I have the data arranged by columns side-by-side, but ggplot needs the values to be arranged in a single column, and the identifier of the sample ID in a second column. I can use the function gather to rearrange the data into the required format. rn &lt;- gather(rn,key = &quot;sampleID&quot;,value = &quot;value&quot;) #rearrange data #Plot histograms using &quot;identity&quot;, and make them transparent ggplot(rn,aes(x = value,fill=sampleID)) + geom_histogram(position = &quot;identity&quot;, alpha=0.5) Try changing the distributions and replotting them (you can change the number of samples, the mean values and the standard deviations). 6.5 The Poisson distribution The Poisson distribution is typically used when dealing with count data. The values must be whole numbers (integers) and they cannot be negative. The shape of the distributions varies with the “lambda” parameter. Small values of lambda give more skewed distributions. Let’s generate and plot some Poisson distributed data. rp &lt;- data.frame(d1 = rpois(500,2.4)) summary(rp) #Take a look ## d1 ## Min. :0.000 ## 1st Qu.:1.000 ## Median :2.000 ## Mean :2.358 ## 3rd Qu.:3.000 ## Max. :7.000 #Plot the data (p1 &lt;- ggplot(rp,aes(x=d1)) + geom_histogram(binwidth = 1) # we know the bins will be 1 ) Try changing the value of lambda and look at how the shape changes. Let’s calculate summary statistics of mean and standard deviation for this distribution (sv &lt;- rp %&gt;% summarise(meanEst = mean(d1), sdEst = sd(d1)) ) ## meanEst sdEst ## 1 2.358 1.500112 Now lets plot the mean and the 2 times the sd on the graph. Remember that for the normal distribution (above) that 95% of the data were within 2 times the sd. p1 + geom_vline(xintercept = sv$meanEst, size=2) + #mean geom_vline(xintercept = sv$meanEst+2*sv$sdEst, size=1) + #upper2SD geom_vline(xintercept = sv$meanEst-2*sv$sdEst, size=1) #lower2SD This looks like a TERRIBLE fit: The mean is not close to the most common value in the data set and the lower limit of the s.d. indicates we should expect some negative values - this is impossible for Poisson data. The reason for this is that mean and standard deviation, and therefore standard error, are intended for normally distributed data. When the data come from other distributions we must take another approach. So how should we summarise this data? One approach is to report the median as a measure of “central tendency” instead of the mean, and to report “quantiles” of the data along with the range (i.e. minimum and maximum). Quantiles are simply the cut points that divide the data into parts. For example, the 25% quantile is the point where (if the data were arranged in order) one quarter of the values would fall below; the 50% quantile would mark the middle of the data (= the median); the 75% quantile would be the point when three-quarters of the data are below. You can calculate those things using dplyr’s summarise. However, you can also simply use the base R summary command. (sv &lt;- rp %&gt;% summarise(minVal = min(d1), q25 = quantile(d1,0.25), med = median(d1), q75 = quantile(d1,0.75), maxVal = max(d1)) ) ## minVal q25 med q75 maxVal ## 1 0 1 2 3 7 #base R summary is just as good. summary(rp$d1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.000 2.000 2.358 3.000 7.000 6.6 Comparing normal and Poisson distributions To get a better feel for how these two distributions differ, lets use the same approach we used above to plot two distributions together. rn &lt;- data.frame(normDist = rnorm(500,2,2),poisDist = rpois(500,2.4)) summary(rn) ## normDist poisDist ## Min. :-4.0357 Min. :0.000 ## 1st Qu.: 0.7508 1st Qu.:1.000 ## Median : 2.2188 Median :2.000 ## Mean : 2.1804 Mean :2.348 ## 3rd Qu.: 3.7197 3rd Qu.:3.000 ## Max. : 7.7649 Max. :9.000 rn &lt;- gather(rn,key = &quot;sampleID&quot;,value = &quot;value&quot;) #rearrange data #Plot histograms using &quot;identity&quot;, and make them transparent ggplot(rn,aes(x = value,fill=sampleID)) + geom_histogram(position = &quot;identity&quot;, alpha=0.5,binwidth = 1) Try changing the arguments in the rnorm and rpois commands to change the distributions. Finally, let’s take another view of these data and look at them using box plots. Box plots are a handy alternative to histograms and many people prefer them. ggplot(rn,aes(x=sampleID, y = value,fill=sampleID)) + geom_boxplot() You should see the main features of both distributions are captured pretty well. The normal distribution is approximately symetrical and the Poisson distribution is skewed (one whisker longer then the other) and cannot be &lt;0. Which graph to you prefer? (there’s no right answer!) "],
["customising-your-plots.html", "Chapter 7 Customising your plots 7.1 Introduction 7.2 A basic plot 7.3 Axis limits 7.4 Transforming the axis (log scale) 7.5 Changing the axis tick marks 7.6 Axis labels 7.7 Colours 7.8 Themes 7.9 Moving the legend 7.10 Combining multiple plots 7.11 Saving your plot 7.12 Final word on plots", " Chapter 7 Customising your plots 7.1 Introduction The purpose of this worksheet it to learn how to customise plots made with ggplot to improve “readability”, or just for aesthetic reasons. We will cover the following: Modifying axes (log transform, different tick marks/ranges etc.). Colour schemes . “Themes” - built-in sets of styles. Multiple sub-plots in a plot. Saving your plots. As usual we need to install tidyverse. library(tidyverse) For these examples I will use the dataset on animal life history, Anage. x &lt;- read.csv(&quot;CourseData/anage_data.csv&quot;) You can remind yourself what this data looks like using commands like summary, str and names. I will process the data a bit to make it easier to work with. One of the commands might be new to you - rename. This is simply a way of renaming columns, in this case to make them more “user friendly” (e.g. I want to rename the column “Metabolic.rate..W.” to “BMR” (for basal metabolic rate)). I will also use mutate to (1) convert the Mass from grams to kilograms and (2) to make a new variable called “BMRperKg” which standardises metabolic rate by expressing it as rate per kilogram. anage &lt;- x %&gt;% mutate(Species = paste(Genus,Species)) %&gt;% rename(Longevity = &quot;Maximum.longevity..yrs.&quot;, Mass = &quot;Body.mass..g.&quot; , BMR = &quot;Metabolic.rate..W.&quot;) %&gt;% select(Class, Order, Species,Mass,Longevity,BMR) %&gt;% filter(Class %in% c(&quot;Aves&quot;,&quot;Amphibia&quot;,&quot;Mammalia&quot;,&quot;Reptilia&quot;)) %&gt;% droplevels() %&gt;% #this removes unused &quot;factor levels&quot; e.g. &quot;Insecta&quot; mutate(Mass = Mass/1000, BMRperKg = BMR/Mass) summary(anage) ## Class Order Species ## Length:3231 Length:3231 Length:3231 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## Mass Longevity BMR ## Min. : 0.001 Min. : 0.40 Min. : 0.0001 ## 1st Qu.: 0.026 1st Qu.: 10.20 1st Qu.: 0.2655 ## Median : 0.131 Median : 16.20 Median : 0.7050 ## Mean : 13.188 Mean : 19.37 Mean : 11.8309 ## 3rd Qu.: 1.111 3rd Qu.: 24.45 3rd Qu.: 3.1370 ## Max. :3672.000 Max. :211.00 Max. :2336.5000 ## NA&#39;s :2604 NA&#39;s :432 NA&#39;s :2604 ## BMRperKg ## Min. : 0.0454 ## 1st Qu.: 2.2191 ## Median : 4.5745 ## Mean : 7.0439 ## 3rd Qu.: 9.8686 ## Max. :45.7692 ## NA&#39;s :2604 png(&quot;basePlot.png&quot;) plot(log(anage$Mass),log(anage$BMR)) dev.off() ## RStudioGD ## 2 7.2 A basic plot Now lets start with a basic plot. You will see a warning about removing rows with missing values. This is just a warning to let you know that there are missing (NA) values in the data you are plotting. (p1 &lt;- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + geom_point(alpha=0.3)) #use alpha argument to make points transparent 7.3 Axis limits These points are really spread out. One option to deal with this might be to set the range over which the axes are allowed to go using xlim and ylim. p1 + xlim(0,500) + ylim(0,300) 7.4 Transforming the axis (log scale) In this particular case though use of a log scale would be best because even after focusing on a smaller part of the range of values you can see that the points are still concentrated at smaller values. In a moment, you will also see that log-transforming the data makes the cloud of points pleasingly linear. You can set a log scale by using the commands scale_x_continuous(trans = \"log\") and scale_y_continuous(trans = \"log\"). (p2 &lt;- p1 + scale_x_continuous(trans = &quot;log&quot;) + scale_y_continuous(trans = &quot;log&quot;)) 7.5 Changing the axis tick marks This looks nice. But the numbers on the axis are not very nice. Using summary(anage$BMR) tells us that the range of data is from 0.0001 to 2336.5. We could place tick marks anywhere on this axis, but let’s try 0.0001, 0.001,0.1, 1,10, 100, 1000. (p2 &lt;- p1 + scale_x_continuous(trans = &quot;log&quot;) + scale_y_continuous(trans = &quot;log&quot;, breaks = c(0.0001,0.001,0.01,0.1,1,10,100,1000))) Using summary(anage$Mass) tells us that the range of data is from 0.001 to 3672. We could place tick marks anywhere on this axis, but let’s try 0.001,0.1, 1,10, 100, 1000. (p2 &lt;- p1 + scale_x_continuous(trans = &quot;log&quot;, breaks = c(0.001,0.01,0.1,1,10,100,1000)) + scale_y_continuous(trans = &quot;log&quot;, breaks = c(0.0001,0.001,0.01,0.1,1,10,100,1000)) ) 7.6 Axis labels Now, let’s think about the axis labels. The labels in the plots so far have no units indicated, and might not be easy to interpret for the reader. Let’s add units, and also spell out more fully what “BMR” and “Mass” means (the axes is basal metabolic rate in Watts and adult body mass in kg). (p3 &lt;- p2 + xlab(&quot;Adult body mass (kg)&quot;) + ylab(&quot;Basal metabolic rate (W)&quot;) ) 7.7 Colours What about those colours? The ggplot package uses some default colours that are OK, but sometimes you will want to make a change. You can “manually” adjust colours using the scale_colour_manual function. You can either name individual colours (e.g. “red”,“green”,“orange”,“black”)4, or you can find their so-called “hex-codes” from a site like http://colorbrewer2.org/ or https://htmlcolorcodes.com/color-picker/. You can add a two digit number after the hex code to set the “opaqueness” of the colour. For example “#FF000075” is red, with 75% opacity. With colour names… p3 + scale_colour_manual(values = c(&quot;red&quot;,&quot;green&quot;,&quot;orange&quot;,&quot;black&quot;)) And with some hex codes… p3 + scale_colour_manual(values = c(&quot;#33FF6475&quot;,&quot;#3368FF75&quot;,&quot;#FF33CE75&quot;,&quot;#FFCA3375&quot;)) Another alternative is to use some of ggplot’s built in “palettes” of colour combinations. For example, there are several palettes called “viridis”. p3 + scale_colour_viridis_d(option = &quot;D&quot;) Try using other option arguments A, B, C and E. Try also adding an argument for transparency alpha = 0.5. Here’s a couple more palettes. There’s one for shades of grey… p3 + scale_colour_grey() There’s another one for various colour schemes, called “colour brewer”. Try using “RdGy”, “RdYlBu” and “Spectral” see ?scale_colour_brewer for more options. p3 + scale_colour_brewer(palette = &quot;BrBG&quot;) 7.8 Themes Finally, ggplot includes the option to set a theme for the plots. “Themes”\" make adjustments to the “look” of the plot. It is possible to write your own themes, but I recommend to use some ready-made ones. You can implement them by adding them as you would any other addition to the ggplot command (e.g. + theme_light(). There are several themes included with ggplot. Try my favourite, theme_minimal(). Then try theme_classic() and theme_dark(). (p4 &lt;- p3 + theme_minimal() ) For more theme fun, you can install packages that include more themes. The best one is called ggthemes (remember that you only need to install the package once). Try theme_economist(), theme_tufte() and (ugh!) theme_excel(). You can see what other themes there in this package at https://jrnold.github.io/ggthemes/reference/index.html (some of them are really ugly in my opinion!). install.packages(&quot;ggthemes&quot;) library(ggthemes) p3 + theme_economist() This package also includes some useful colour scales, including some for colour blind people. p3 + scale_color_colorblind() 7.9 Moving the legend By default, the legend is placed on the right. You can move it around by adding a theme argument to your plot commands. It can also be placed on the “top”, “bottom”, or “left”. You can also remove the legend altogether by using legend.position = \"none\". You might also want to remove the legend title using the theme argument legend.title = element_blank(). p3 + theme(legend.position = &quot;bottom&quot;) 7.10 Combining multiple plots It is often useful to combine two or more plots into a single figure. For example, many journals have strict limits on the number of plots so it is useful to combine plots into “Figure 1A and B” etc. There are several R packages that can do this and my favourite is called patchwork. install.packages(&quot;patchwork&quot;) #only need to do this once library(patchwork) I will illustrate it by first making another plot, this time showing the relationship between body mass and standardised BMR (BMR per kg). Because I am combining the plots into a smaller space I have decided to remove the figure legend (I could put it in the figure caption instead). #PlotA (this is what you have already created above) plotA &lt;- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + geom_point(alpha=0.5) + scale_x_continuous(trans = &quot;log&quot;,breaks =c(0.001,0.01,0.1,1,10,100,1000)) + scale_y_continuous(trans = &quot;log&quot;,breaks =c(0.0001,0.001,0.01,0.1,1,10,100,1000)) + xlab(&quot;Adult body mass (kg)&quot;) + ylab(&quot;Basal metabolic rate (W)&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) (plotB &lt;- ggplot(anage,aes(x = Mass, y = BMRperKg, colour = Class)) + geom_point(alpha=0.5) + scale_x_continuous(trans = &quot;log&quot;,breaks =c(0.001,0.01,0.1,1,10,100,1000)) + scale_y_continuous(trans = &quot;log&quot;,breaks =c(0.0001,0.001,0.01,0.1,1,10)) + xlab(&quot;Adult body mass (kg)&quot;) + ylab(&quot;Basal metabolic rate (W/kg)&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) ) #This one is wrapped in brackets so that R shows it Now I can combine these using the very simple syntax like this: plotA + plotB I can add titles using the ggtitle command like this. plotA + ggtitle(&quot;A&quot;) + plotB + ggtitle(&quot;B&quot;) You caould place the sub-plots on top of eachother like this. (plotA + ggtitle(&quot;A&quot;)) / (plotB + ggtitle(&quot;B&quot;)) 7.11 Saving your plot You should, I think, already know about using the “Export” button in RStudio to save out your plot. This is useful and easy, but you should know that you can also save the plots using a typed command (ggsave) in your script. This command is handy because it allows you to automatically set the size, and file name of your plot. The default setting for ggsave is that it will save the last plot that was printed to your computer screen to a filename that you specify. Therefore easiest way to use the command is to simply place the ggsave command immediately after your ggplot command. You should set the width and height of the plot and the units (the default is inches). It usually takes a few attempts and a bit of trial-and-error to choose the dimensions so that the plot looks nice. ggsave(&quot;MySavedPlot1.png&quot;, width = 18, height=10, units = &quot;cm&quot;) The command can save to various file types including png, jpeg, pdf (see the ggplot help file for more). R knows what file file type is chosen by checking the file extension in the file name (e.g. .png). I advise to use png. 7.12 Final word on plots We have covered a lot of ground here. There is a lot to learn, but don’t feel like you have to remember all of these commands (I don’t). Mostly it is simply a case of remembering that it is possible to do these things, and knowing where to look up the commands. Obvious starting points are these worksheets, and the text book (including the online version!). You can also usually find help by Googling “ggplot” followed by what you are trying to do (e.g. “ggplot change axis ticks”). One of my frequently used web sites is this one http://www.sthda.com/english/ which has an extensive section on ggplot (http://www.sthda.com/english/wiki/ggplot2-essentials). Even though we have covered a lot of ground we have still only gotten a taster of what ggplot is capable of. I encourage you to learn more. A useful resource for learning is the online R graph gallery\" at https://www.r-graph-gallery.com/, which shows you how to make and modify many types of plot. See https://www.r-graph-gallery.com/42-colors-names.html↩︎ "],
["randomisation-tests.html", "Chapter 8 Randomisation Tests 8.1 Introduction 8.2 The example 8.3 Setting up 8.4 Calculate the observed difference 8.5 Null distribution 8.6 Testing significance 8.7 Testing the hypothesis 8.8 Writing it up 8.9 Paired Randomisation Tests 8.10 Exercises - Randomisation tests", " Chapter 8 Randomisation Tests 8.1 Introduction Simple experiments testing for a difference in mean values between two groups usually have the null hypothesis that there is no difference. The alternative hypothesis varies. Sometimes it is simply that the two groups are different (and that the difference could be wither positive or negative). In other cases the alternetive hypothesis is that the mean of Group A is less then the mean of Group B (or that it is greater). Randomisation tests are an intuitive, but computationally intensive way of testing these hypotheses. They have a long history and were first proposed by R.A. Fisher in the 1930s. However they only became convenient when computers became sufficiently fast to do the calculations. Carrying out a test in R requires that you put your dplyr skills to the test. Here you will be guided through an example. 8.2 The example A new drug has been developed that is supposed to reduce cholesterol levels in men. An experiment has been carried out where 12 human test subjects have been assigned randomly to two groups: “Control” and “Drug”. The pharmaceutical company is hoping that the “Drug” group will have lower cholesterol than the “Control” group. The aim here is to do a randomisation test to check that. 8.3 Setting up First you need to load the tidyverse libraries required (we’ll be making use of dplyr, magrittr and ggplot which are all included in the tidyverse set of packages. library(tidyverse) Next, import the data, called cholesterol. Note that your path will be different from mine, and that you should set your working directory accordingly. Let’s first take a look at the data by plotting it. I will first plot a boxplot first, and add the jittered points for clarity. ggplot(ch,aes(x=Treatment,y=Cholesterol)) + geom_boxplot()+ geom_jitter(colour=&quot;grey&quot;,width=.1) It looks like there might be a difference between the groups. Now let’s consider our test statistic and our hypotheses. Our test statistic is the difference in mean cholesterol levels between the two groups: mean of control group minus the mean of the drug group. The null hypothesis is that there is no difference between these two groups (i.e. the difference should be close to 0) The alternative hypothesis is that the mean of the drug group should be less than the mean of the drug group. (i.e. mean of control group minus the mean of the drug group should be negative). 8.4 Calculate the observed difference There are a few ways of doing this. In base-R you can use the function tapply (“table apply”), followed by diff (“difference”). tapply(ch$Cholesterol,ch$Treatment,mean) ## Control Drug ## 205.6667 185.5000 diff(tapply(ch$Cholesterol,ch$Treatment,mean)) ## Drug ## -20.16667 Because we are focussing on learning dplyr, you can also calculate the means like like this: ch %&gt;% # ch is the cholesterol data group_by(Treatment) %&gt;% # group the data by treatment summarise(mean = mean(Cholesterol)) # calculate means ## # A tibble: 2 x 2 ## Treatment mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Control 206. ## 2 Drug 186. Here the pipes (%&gt;%) are passing the result of each function on as imput to the next. You can use further commands, pull to get the mean vector from the summary table, and then use diff to calculate the difference between the groups, before passing that to a value called “observedDiff”. observedDiff &lt;- ch %&gt;% group_by(Treatment) %&gt;% # group the data by treatment summarise(mean = mean(Cholesterol)) %&gt;% # calculate means pull(mean) %&gt;% # extract the mean vector diff() This is a complicated set of commands. To make sure that you understand it, try running it bit-by-bit to see what is going on. 8.5 Null distribution Now we ask, what would the world look like if our null hypothesis was true. To do this we can dissassociate the treatment group variable from the measured cholesterol values. We do this using by using the mutate function to replace the Treatment variable with a shuffled version of itself with the sample function. Let’s try that one time: ch %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% #shuffle the Treatment data group_by(Treatment) %&gt;% summarise(mean = mean(Cholesterol)) %&gt;% pull(mean) %&gt;% diff() ## [1] -10.16667 In this instance, the difference with the shuffled Treatment values is 0.833, which is rather different from our observed difference of -20.1666667. Doing this one time is not much help though - we need to repeat this many times. I suggest that you do it 1000 times here, but some statisticians would suggest 5000 or even 10000 replicates. We can do this easily in R using the function replicate which simply a kind of wrapper that tells R to repeat a command n times and then pass the result to a vector. Let’s try it first 10 times to see how it works: replicate(10, ch %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% group_by(Treatment) %&gt;% summarise(mean = mean(Cholesterol)) %&gt;% pull(mean) %&gt;% diff() ) ## [1] -8.500000 13.500000 -8.166667 -2.833333 -14.500000 ## [6] 5.833333 15.500000 6.500000 -1.166667 -1.833333 You can see that the replicate command simply does the sampling-recalculation of the mean 10 times. In the commands below I create 1000 replicates of the shuffled differences. I want to put them in a dataframe to make it easy to plot. Therefore, I first create a data.frame called shuffledData. This data frame initially has a variable called rep which consists of the numbers 1-1000. I then use mutate to add the 1000 shuffled differences. shuffledData &lt;- data.frame(rep = 1:1000) %&gt;% mutate(shuffledDiffs = replicate(1000, ch %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% group_by(Treatment) %&gt;% summarise(mean = mean(Cholesterol)) %&gt;% pull(mean) %&gt;% diff() )) 8.6 Testing significance Before formally testing the hypothesis it is useful to visualise what we have created in a histogram. I can use ggplot to do this, to create a plot called p1. Note that by putting the command in brackets R will both create the plot object, and print it to the screen. Note that because the shuffling of the data is random process your graph will look slightly different to mine. (p1&lt;-ggplot(shuffledData,aes(x=shuffledDiffs)) + geom_histogram() + theme_minimal()+ xlab(&quot;Drug mean - Control mean&quot;)) You can now add your observed difference (calculated above) to this plot like this: p1 + geom_vline(xintercept = observedDiff) 8.7 Testing the hypothesis Recall that the alternative hypothesis is that the observed difference (control mean-drug mean) will be less than 0. You can see that there are few of the null distribution sample that are as extreme as the observed difference. To calculate a p-value we can simply count these values and express them as a proportion. Note that because the shuffling of the data is random process your result will probably be slightly different to mine. table(shuffledData$shuffledDiffs&lt;=observedDiff) ## ## FALSE TRUE ## 958 42 So that is 42 of the shuffled values that are equal to or less than the observed difference. The p-value is then simply 42/1000 = 0.042. Therefore we can say that the drug appears to be effective at reducing cholesterol. 8.8 Writing it up We can report our findings something like this: \"To test whether effect of the drug at reducing cholesterol level is statistically significant I did a 1000 replicate randomisation test with the null hypothesis being that there is no difference between the group means and the alternative hypothesis that the mean for the drug treatment is lower than the control treatment. I compared the observed difference to this null distribution to calculate a p-value in a one-sided test. The observed mean values of the control and treatment groups 205.667 and 185.500 respectively and the difference between them is therefore -20.167 (drug mean - control mean). Only 25 of the 1000 null distribution replicates were as low or lower than my observed difference value. I conclude that the observed difference between the means of the two treatment groups is statistically significant (p = 0.025)\" 8.9 Paired Randomisation Tests The paired randomisation test is simply a one-sample randomisation test where the distribution is tested against a value of 0 (no difference between the two). I will illustrate this with an example from Everitt (1994) who looked at using cognitive behaviour therapy as a treatment for anorexia. He collected data on weights of people before and after therapy. These data are in the file anorexiaCBT.csv library(tidyverse) #Remember to set your working directory first an &lt;- read.csv(&quot;CourseData/anorexiaCBT.csv&quot;) head(an) ## Subject Week01 Week08 ## 1 1 80.5 82.2 ## 2 2 84.9 85.6 ## 3 3 81.5 81.4 ## 4 4 82.6 81.9 ## 5 5 79.9 76.4 ## 6 6 88.7 103.6 These data are arrange in a so-called “wide” format. To make plotting and analysis data need to be rearranged into a tidy “long” format so that each observation is on a row. We can do this using the gather function: an &lt;- an %&gt;% gather(&quot;time&quot;,&quot;weight&quot;,-Subject) head(an) ## Subject time weight ## 1 1 Week01 80.5 ## 2 2 Week01 84.9 ## 3 3 Week01 81.5 ## 4 4 Week01 82.6 ## 5 5 Week01 79.9 ## 6 6 Week01 88.7 8.9.1 Plot the data We should ALWAYS plot the data. So here goes. (p1&lt;-ggplot(an,aes(x=weight,fill=time)) + geom_histogram(position = &quot;identity&quot;,alpha=.7) ) Another useful way to plot this data is to use an “interaction plot”. In these plots the matched pairs (grouped by Subject) are joined together with lines. You can plot one like this: (p2&lt;-ggplot(an,aes(x=time,y=weight,group=Subject)) + geom_point() + geom_line(size=1, alpha=0.5) ) #ggsave(&quot;/Users/jones/Dropbox/_SDU_Teaching/BB852 New Stats Course/PlotsintPlot.png&quot;,p2,width=6, height=8, dpi = 300, units=&quot;cm&quot;) What we are interested in is whether there has been a change in weight of the subjects after CBT. The null hypothesis is that there is zero change in weight. The alternative hypothesis is that weight has increased. The starting point for the analysis is to calculate the observed change in weight. an &lt;- an %&gt;% group_by(Subject) %&gt;% summarise(change = diff(weight)) You have created a dataset that looks like this: head(an) ## # A tibble: 6 x 2 ## Subject change ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.7 ## 2 2 0.700 ## 3 3 -0.100 ## 4 4 -0.700 ## 5 5 -3.5 ## 6 6 14.9 And you can calculate the observed change like this: obsChange &lt;- mean(an$change) obsChange ## [1] 3.006897 8.9.2 The paired randomisation test The logic of this test is that if the experimental treatment has no effect on weight, then the Before weight is just as likely to be larger than the After weight as it is to be smaller. In other words, if the null hypothesis is true, a permutation within any pair of scores is as likely as the reverse. Therefore to carry out this test, we can permute the SIGN of the change in weight (i.e. we randomly flip values from positive to negative and vice versa). We can do this by multiplying by 1 or -1, randomly. head(an) ## # A tibble: 6 x 2 ## Subject change ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.7 ## 2 2 0.700 ## 3 3 -0.100 ## 4 4 -0.700 ## 5 5 -3.5 ## 6 6 14.9 anShuffled &lt;- an %&gt;% mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %&gt;% mutate(shuffledChange = change * sign) Let’s take a look at this new shuffled dataset: head(anShuffled) ## # A tibble: 6 x 4 ## Subject change sign shuffledChange ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.7 1 1.7 ## 2 2 0.700 -1 -0.700 ## 3 3 -0.100 -1 0.100 ## 4 4 -0.700 -1 0.700 ## 5 5 -3.5 -1 3.5 ## 6 6 14.9 -1 -14.9 We need to calculate the mean of this shuffled vector. We can do this by pull to get the vector, and then mean. an %&gt;% mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %&gt;% mutate(shuffledChange = change * sign) %&gt;% pull(shuffledChange) %&gt;% mean() ## [1] -0.462069 Now we will build a null distribution of changes in weight by repeating this 1000 times. We can do this using the replicate function to “wrap” around the function, passing the result into a data frame. We can then compare this null distribution to the observed change. nullDist = data.frame(change = replicate(1000,an %&gt;% mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %&gt;% mutate(shuffledChange = change * sign) %&gt;% pull(shuffledChange) %&gt;% mean())) 8.9.3 Plot the null distribution (nullDistPlot &lt;- ggplot(nullDist,aes(x=change)) + geom_histogram()) We can add the observed change as a line to this: nullDistPlot + geom_vline(xintercept = obsChange) 8.9.4 The formal hypothesis test The formal test of significance then is to ask how many of the null distribution replicates are as extreme as the observed change. table(nullDist$change&gt;=obsChange) ## ## FALSE TRUE ## 975 25 So we can see that 25 of 1000 replicates were greater than or equal to the observed change. This translates to a p-value of 0.025. We can therefore say that the observed change in weight after CBT was signficantly greater than what we would expect from chance. 8.10 Exercises - Randomisation tests 8.10.1 Introduction A hercules beetle is a large rainforest species from South America. Researchers suspect that sexual selection has been operating on the species so that the males are significantly larger than the females. You are given data5 on width measurements in cm of a small sample of 20 individuals of each sex. Can you use your skills to report whether males are signficantly larger than females. The data are called herculesBeetle.csv and can be found via the Dropbox link on Blackboard. 8.10.2 Your task Follow the following prompts to get to your answer: What is your null hypothesis? What is your alternative hypothesis? Import the data. Calculate the mean for each sex (either using tapply or using dplyr tools) Plot the data as a histogram. Add vertical lines to the plot to indicate the mean values. Now calculate the difference between the mean values using dplyr tools, or tapply. Use sample to randomise the sex column of the data, and recalculate the difference between the mean. Use replicate to repeat this 10 times (to ensure that you code works). When your code is working, use replicate again, but this time with 1000 replicates and pass the results into a data frame. Use ggplot to plot the null distribution you have just created, and add the observed difference. Obtain the p-value for the hypothesis test described above. (1) how many of the observed differences are greater than or equal to the shuffled differences in the null distribution. (2) what is this expressed as a proportion of the number of replicates. Summarise your result as in a report. Describe the method, followed by the result and conclusion. This example from: https://uoftcoders.github.io/rcourse/lec09-Randomization-tests.html↩︎ "],
["comparing-two-means-with-a-t-test.html", "Chapter 9 Comparing two means with a t-test 9.1 Some theory. 9.2 One sample t-test 9.3 Doing it “by hand” - where does the t-statistic come from? 9.4 Paired t-test 9.5 A paired t-test is a one-sample test. 9.6 Two sample t-test 9.7 t-tests are linear models 9.8 Exercises - Two-sample t-test", " Chapter 9 Comparing two means with a t-test We will cover the following: One-sample t-test Paired t-test Two-sample t-test (“Welch t-test”) 9.1 Some theory. In this theory section I focus on the one-sample t-test, but the concepts apply to the other types of t-test. The one-sample t-test is used to compare the mean of a sample to some fixed value. For example, we might want to compare pollution levels (e.g. in mg/m3) in a sample to some acceptable threshold value to help us decide whether we need to take action to prevent or clean up pollution. One of the assumptions of t-tests (and many other tests/models) is that the distribution of values in the sample of data can be described by a normal distribution. If this assumption is true, you can use these data to estimate the parameters of this sample’s normal distribution: the mean and standard error of the mean. The mean gives an estimate of location, and the standard error of the mean (which is calculated as \\(s/ \\sqrt{n}\\), where s = standard deviation and n = sample size) gives an estimate of precision of this estimate (i.e. how certain is it that the mean value is really where you think it is?) The t-test then works by comparing your estimated distribution with some fixed value. Sometimes you are asking “is my mean different from the value?”, other times you are asking “is my mean less than/greater than the value?”. This depends on the hypothesis. The default that R-uses is that it tests whether the mean of your distribution is different to the fixed value, but in many cases you should really be framing a directional hypothesis. It is helpful to visualise this, so some examples of the pollution threshold test are shown in the figure below (Figure ). The curves illustrate the estimated normal distributions that describe our estimate of the mean pollution level from some data (e.g. each curve might represent samples from different locations). We are interested in whether the mean values (the vertical dashed lines) are significantly greater than the threshold of 100mg/m2 (solid vertical black line) (this gives us a directional hypothesis). Formally we do this by establishing two hypotheses a null hypothesis and an alternative hypothesis. In this case, the null hypothesis is that the mean of the sample measurements is not significantly different from the threshold value we define. The alternative hypothesis is that the sample mean is significantly greater than this threshold value. The degree of confidence that we can have that the mean pollution values are different from the threshold value depend on (A) the position of the distribution relative to the threshold value and (B) on the spread of the distribution (the standard deviation/error). Based on Figure , which of these four different samples shows a mean value significantly greater than 100? (you should be looking at the amount of the normal distribution curve that is overlapping the threshold value.) This should look familiar – it is the same concept as we used in the class on randomisation tests. If you find it confusing, please go back and review the randomisation test materials! Another useful way to think about t-tests is that it is a way of distinguishing between signal and noise: the signal is the mean value of the thing you are measuring, and the noise is the uncertainty in that estimate. This uncertainty could be due to measurement error and/or natural variation. In fact, the t-value that the t-test relies on is a ratio between the signal (difference between mean (\\(\\bar{x}\\)) and threshold (\\(\\mu_{0}\\))) and noise (variability, standard error of the mean (\\(s/ \\sqrt{n}\\))): \\[t = \\frac{\\bar{x}-\\mu_{0}} {s/ \\sqrt{n}}\\] The larger the signal is compared to the noise, the higher the t-value will be. e.g. a t-value of 2 means that the signal was 2 times the variability in the data. A t-value of zero, or close to zero, means that the signal is “drowned out” by the noise. Therefore, high t-values give you more confidence that the difference is true. To know if the t-value means that the difference is significant, the t-value is compared to a known theoretical distribution (the t-distribution). The area under the curve of the distribution is 1, but its shape depends on the degrees of freedom (i.e. sample size - 1). The plot below (Figure ) shows three t-distributions of different degrees of freedom (d.f.). What R is doing when it figures out the p-value is calculating the area under the curve beyond the positive/negative values of the t-statistic. If t is small, then this value is large (p-value). If t is large then the area (and the p-value) is small. In the olden-days (&gt;15 years ago) you would have looked these values up in printed tables, but now R does that for us. 9.2 One sample t-test Enough theory. Here’s how you would apply such a test in R. Firstly, lets load some data. Because this is a very small example, you can simply cut and paste the data in rather than loading it from a CSV file. pollution &lt;- data.frame(mgm3 = c(105, 196, 226, 81, 156, 201, 142, 149, 191, 192, 178, 185, 231, 76, 207, 138, 146, 175, 114, 155)) First I plot the data (Figure ). One reason for doing this is to check that the data look approximately normally distributed. These data are slightly left-skewed but they are close enough. ggplot(pollution,aes(x=mgm3))+ geom_histogram(bins=8) + geom_vline(xintercept = 100) Figure 9.1: Histogram of the pollution data. Now we can run a t-test in R like this. The command is simple - the first two arguments are the data (x) and the the fixed value you are comparing the data to. The final argument defines the alternative hypothesis. This can take values of “two.sided”, “less” or “greater” (the default is two.sided). In this example, the alternative hypothesis is that the mean of our sample is greater than the threshold of 100. t.test(x = pollution$mgm3, mu = 100, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: pollution$mgm3 ## t = 6.2824, df = 19, p-value = 2.478e-06 ## alternative hypothesis: true mean is greater than 100 ## 95 percent confidence interval: ## 145.0803 Inf ## sample estimates: ## mean of x ## 162.2 The output of the model tells us (1) what type of t-test is being fitted (“One Sample t-test”). Then it gives some values for the t-statistic, the degrees of freedom and the p-value. The model output also tells us that the alternative hypothesis “true mean is greater than 100”. Because the p-value is very small (p&lt;0.05) we can reject the null hypothesis and accept the alternative hypothesis. Finally, the output gives you the confidence interval (the area where we strongly believe the true mean to lie) and the estimate of the mean. We could report these results like this: \"the mean value of the sample was 162.2 mg/m3, which is significantly greater than the acceptable threshold of 100 mg/m3 (t-test: t = 6.2824, df = 19, p-value = 2.478e-06). 9.3 Doing it “by hand” - where does the t-statistic come from? At this point, to ensure that you understand where the t-statistic comes from we will calculate the t-statistic using the equation from above. The purpose of this is to illustrate that this is not brain surgery - it all hinges on a straightforward comparison between signal (the difference between mean and threshold in this case) and noise (the variation, or standard error of the mean). To do this we first need to know the mean value and the threshold (the signal: \\(\\bar{x} - \\mu_{0}\\)). We can then divide that by the standard error of the mean (the noise: \\(s/ \\sqrt{n}\\)) Here goes… I first create a vector (x) of the values to save typing. Then I show how to calculate mean and standard error, before dividing the “signal” by the “noise”. #First create a vector of the values x &lt;- pollution$mgm3 #mean mean(x) ## [1] 162.2 #standard error of the mean sd(x)/sqrt(length(x)) ## [1] 9.900718 #Putting it all together (mean(x) - 100) / (sd(x) / sqrt(length(x))) ## [1] 6.282373 This matches exactly with the t-statistic above! One can obtain a p-value from a given t-statistic and degrees of freedom like this for a t-test like the one fitted above (the d.f. is the sample size minus one for a one-sample t-test): 1 - pt(6.282373, 19) ## [1] 2.477945e-06 Again, this matches the value from the t.test function above. 9.4 Paired t-test It’s actually quite hard to find examples of one-sample t-tests in biology. In most cases, the one-sample t-tests are really paired t-tests, which are a special case of the one sample test where rather than using the actual values measured, we use the difference between them instead (Figure ). Here’s a simple example. Anthropologists studied tool use in women from the indigenous Machinguenga of Peru. They estimated the amount of cassava obtained in kg/hour using either a wooden tool (a broken bow) or a metal machete. The study focused on 5 women who were randomly assigned to groups to use the wooden tool then the machete (or vice versa). The anthropologists hypothesised that using different tools led to different harvesting efficiencies. The null hypothesis is that there is no difference between the two groups and that a woman was equally efficient at foraging using either tool. The alternative hypothesis was that there is a difference between the two tools. (NOTE - this could also be formulated as a directional hypothesis e.g. with the expectation that machete is more efficient than the bow.) First let’s import and look at the data. Make sure you understand it. A plot will be fairly useless to tell if the data are normally distributed, so we will simply have to assume that they are. In fact, t-tests are famously robust to non-normality. toolUse &lt;- read.csv(&quot;CourseData/toolUse.csv&quot;) toolUse ## subjectID tool amount ## 1 1 machete 119 ## 2 1 bow 39 ## 3 2 machete 216 ## 4 2 bow 114 ## 5 3 machete 240 ## 6 3 bow 150 ## 7 4 machete 129 ## 8 4 bow 51 ## 9 5 machete 137 ## 10 5 bow 60 We should now plot our data (Figure ). A nice way of doing this for paired data is to plot points with lines joining the pairs. This way, the slope of the lines is a striking visual indication of the effect. ggplot(toolUse,aes(x=tool,y=amount,group=subjectID)) + geom_line()+ geom_point() + xlab(&quot;Tool used&quot;) + ylab(&quot;Casava harvested (kg/hr)&quot;) Figure 9.2: An interaction plot for the tool use data We can also look at the mean values and standard deviations: toolUse %&gt;% group_by(tool) %&gt;% summarise(meanAmount = mean(amount),sdAmount = sd(amount)) ## # A tibble: 2 x 3 ## tool meanAmount sdAmount ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bow 82.8 47.3 ## 2 machete 168. 55.6 Now lets do a paired t-test to compare the means using the two tools. There are several ways to do a paired t-test. The first is to use a model formula in the command. The formula takes the form measurements ~ group. You must also specify the name of the data.frame and that the data are paired (paired = TRUE). IMPORTANT: it is very important that the pairs are grouped together in the data frame so that the pairs match up when you filter the data to each group. It is therefore advisable to use arrange to sort the data by first the pairing variable (in this case, subjectID), and then the explanatory variable (the variable that defines the group - in this case, tool. toolUse &lt;- toolUse %&gt;% arrange(subjectID, tool) t.test(amount ~ tool, data = toolUse, paired = TRUE) ## ## Paired t-test ## ## data: amount by tool ## t = -17.98, df = 4, p-value = 5.625e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -98.58738 -72.21262 ## sample estimates: ## mean of the differences ## -85.4 An alternative way is to give the two samples separately in the t.test command. To do this you will need to create two vectors containing the data from the two groups like this, using the dplyr command pull to extract the variable along with filter to subset the data: A &lt;- toolUse %&gt;% filter(tool == &quot;machete&quot;) %&gt;% pull(amount) B &lt;- toolUse %&gt;% filter(tool == &quot;bow&quot;) %&gt;% pull(amount) t.test(A, B, data = toolUse, paired = TRUE) ## ## Paired t-test ## ## data: A and B ## t = 17.98, df = 4, p-value = 5.625e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 72.21262 98.58738 ## sample estimates: ## mean of the differences ## 85.4 You would report these results something like this: “Women harvested cassava more efficiently with a machete (168.2 kg/hr) than with a wooden tool (82.8kg/hr). The difference of 85.4 kg/hr (95% CI 72.2-98.6 kg) was statistically significant (paired t-test: t = 17.98, df = 4, p-value = 5.625e-05).” NOTE: you could add the argument alternative = \"less\" or greater to these t-tests to turn them into directional one-tailed hypotheses. However, you should also be aware that the p-value for a one-tailed t-test is always half that of the two-tailed test. Therefore, you could also simply half the p-value when you report it rather than adding the “alternative” argument. 9.5 A paired t-test is a one-sample test. A paired t-test is the same as a one-sample t-test really. Here’s proof. First we need to calculate the difference between the two measures difference &lt;- A - B Then we can fit the one-sample t-test from above, with the mu set as 0 (because the null hypothesis is that there is no difference between the groups). Compare this result with the paired t-test above. t.test(x = difference, mu = 0) ## ## One Sample t-test ## ## data: difference ## t = 17.98, df = 4, p-value = 5.625e-05 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 72.21262 98.58738 ## sample estimates: ## mean of x ## 85.4 9.6 Two sample t-test The two sample t-test is used for comparing the means of two samples [no shit!? :)] You can visualise this by picturing your two distributions (Figure ) and thinking about their overlap. If they overlap a lot the difference between means will not be significant. If they don’t overlap very much then the difference between means will be significant. The underlying mathematical machinery for the two-sample t-test is similar to the one-sample and paired t-tests. Again, the important value is the t-statistic, which can be thought of as a measure of signal:noise ratio (see above). It is harder to detect a signal (the true difference between means) if there is a lot of noise (the variability, or spread of the distributions), or if the signal is small (the difference between means is small). Figure 9.3: A two-sample t-test. The mathematics involved with calculating the t-statistic is very similar to the one-sample t-test, except the numerator in the fraction is the difference between two means rather than between a mean and a fixed value. \\[t = \\frac{\\bar{x_1}-\\bar{x_2}} {s/ \\sqrt{n}}\\] So far so good… let’s push on and use R to do some statistics. In this example, we can revisit the class data and ask the question, Is the reaction time of males different than that of females? The null hypothesis for this question is that there is no difference in mean reaction times between the two groups. The alternative hypothesis is that there is a difference in the mean reaction time between the two groups. Import the data in the usual way, and subset it to the right year. x &lt;- read.csv(&quot;CourseData/classData.csv&quot;) Then look at the data. Here I do this using a box plot with jittered points (a nice way of plotting data with small sample sizes) (Figure ). From the Figure it looks like males have a faster reaction time than females, but there is a lot of variation. We need to apply the t-test in a similar way to above. ggplot(x,aes(x=Gender,y=Reaction)) + geom_boxplot() + geom_jitter(col=&quot;grey60&quot;,width=0.2) t.test(Reaction ~ Gender, data = x,var.equal=F) ## ## Welch Two Sample t-test ## ## data: Reaction by Gender ## t = 1.6602, df = 28.63, p-value = 0.1078 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.009270184 0.088978588 ## sample estimates: ## mean in group Female mean in group Male ## 0.3460471 0.3061929 This output first tells us that we are using something called a “Welch Two Sample t-test”. This is a form of the two-sample t-test that relaxes the assumption that the variance in the two groups is the same. This is a good thing. Although it is possible to fit a t-test with equal variances, I recommend that you stick with the default Welch’s test and not make this limiting assumption. Then we are told the t-statistic (1.660), the degrees of freedom (28.630) and the p-value (0.108). We must therefore accept the null hypothesis: there is no significant difference between the two groups. Males are not faster than females. We could write report this something like this: “Although females had a slightly slower reaction time than males (0.346 seconds compared to 0.306 seconds), this difference was not statistically significant (Welch t-test: t= 1.660, d.f.= 28.630), p=0.108).” Note: With a t-test that did assume equal variances in the two groups, the d.f. is calculated as the sample size - 2 (the number of groups). You can do this by adding the argument “var.equal = TRUE” to the t-test command. With the Welch test, the appropriate degrees of freedom are estimated by looking at the sample sizes and variances in the two groups. The details of this are beyond the scope of this course. 9.7 t-tests are linear models It is also possible to formulate t-tests as linear models (using the lm function). To do this with the paired t-test you would specify a model that estimates an intercept. In R you can do this by writing the formula as x ~ 1. So, for the tool use example you can write the code like this: mod1 &lt;- lm(difference ~ 1) summary(mod1) ## ## Call: ## lm(formula = difference ~ 1) ## ## Residuals: ## 1 2 3 4 5 ## -5.4 16.6 4.6 -7.4 -8.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.40 4.75 17.98 5.62e-05 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.62 on 4 degrees of freedom If you look at the summary will notice that the estimate of the intercept (the average difference between the two pairs), the degrees of freedom and the t-value and the p-value are all the same as the value reported when using t.test. In fact, all of the t-tests, and ANOVA (below) are kinds linear models and can be also fitted with lm. Here is the paired t-test investigating gender differences in reaction time. You can see that the test statistics and coefficients match those obtained from t.test. mod &lt;- lm(Reaction ~ Gender, data = x) anova(mod) ## Analysis of Variance Table ## ## Response: Reaction ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Gender 1 0.012194 0.0121945 2.5921 0.1182 ## Residuals 29 0.136429 0.0047044 summary(mod) ## ## Call: ## lm(formula = Reaction ~ Gender, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.08705 -0.04712 -0.01479 0.03755 0.22775 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.34605 0.01664 20.80 &lt;2e-16 *** ## GenderMale -0.03985 0.02475 -1.61 0.118 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06859 on 29 degrees of freedom ## Multiple R-squared: 0.08205, Adjusted R-squared: 0.0504 ## F-statistic: 2.592 on 1 and 29 DF, p-value: 0.1182 9.8 Exercises - Two-sample t-test Some people have suggested that there might be sex differences in fine motor skills in humans. Use the data collected on the class to address this topic using t-tests. The relevant data set is called classData.csv, and the columns of interest are Gender and Precision. Carry out a two-sample t-test. Plot the data (e.g. with a box plot, or histogram) Formulate null and alternative hypotheses. Use the t.test function to do the test. Write a sentence or two describing the results. 9.8.1 Paired t-test A study was carried out looking at the effect of cognitive behavioural therapy on weight of people with anorexia. Weight was measured in week 1 and again in week 8. Use a paired t-test to assess whether the treatment is effective. The data is called anorexiaCBT.csv The data are in “wide format”. You may wish to convert it to “long format” depending on how you use the data. You can do that with the gather function, which rearranges the data: anorexiaCBT_long &lt;- anorexiaCBT %&gt;% gather(&quot;week&quot;,&quot;weight&quot;,-Subject) Plot the data (e.g. with an interaction plot like Figure 5 in the t-tests PDF) Formulate a null and alternative hypothesis. Use t.test to conduct a paired t-test. Write a couple of sentences to report your result. 9.8.2 Optional extra. Try re-fitting some of these tests as randomisation tests (or analyse the randomisation test data using t.test). Do they give the same results? Try answering the question - “are people who prefer dogs taller than those who prefer cats?” using the courseData.csv. Can you think of any problems with this analysis? "],
["linear-models-with-a-single-categorical-explanatory-variable.html", "Chapter 10 Linear models with a single categorical explanatory variable 10.1 Introduction 10.2 One-way ANOVA 10.3 Fitting an ANOVA in R 10.4 Where are the differences? 10.5 Tukey’s Honestly Significant Difference (HSD) 10.6 ANOVA calculation “by hand” (optional)", " Chapter 10 Linear models with a single categorical explanatory variable 10.1 Introduction With the previous work on t-tests (and also with randomisation tests), you are now equipped to test for differences between two groups (or between one group and some fixed value). But what if there are more than two groups? The answer is to use a one-way analysis of variance (ANOVA). Conceptually, this works the same way as a t-test. 10.2 One-way ANOVA The one-way ANOVA is illustrated below with two cases (Figure ). In both cases there are three groups. These could represent treatment groups in an experiment (e.g. different fertiliser addition to plants). In figure A, the three groups are very close, and the means are not significantly different from each other. In figure B, there is one group that stands apart from the others. The ANOVA will tell us whether at least one of the groups is different from the others. After figuring out if at least one of the groups is significantly different from the others it is often enough to examine plots (or summary statistics) to see where the differences are (e.g. which group(s) are different from each other). In other cases it might be necessary to do follow up post-hoc multiple comparison tests. We will come to those later. 10.3 Fitting an ANOVA in R New coffee machines use “pods” to make espresso. These have become much more popular than the traditional “bar machines”. This data looks at the amount of “crema” or foam produced (a sign of quality!) using three methods: bar machines (BM), Hyper Espresso Pods (HIP) and Illy Espresso System (IES). Are any of these methods better than the others? Import the data and look at it. espresso &lt;- read.csv(&quot;CourseData/espresso.csv&quot;, stringsAsFactors = TRUE) head(espresso) ## foamIndx method ## 1 36.64 BM ## 2 39.65 BM ## 3 37.74 BM ## 4 35.96 BM ## 5 38.52 BM ## 6 21.02 BM (ggplot(espresso,aes(x=method,y=foamIndx)) + geom_boxplot()+ geom_jitter(width=0.2)) Figure 10.1: A box and whisker plot, with jittered points, for the espresso foam data. You can see that the categorical explanatory variable (“method”) defines the three treatment groups and has the three levels representing the different coffee types: BM, HIP and IES. Let’s first fit the ANOVA using R. One way ANOVAs are fitted using the lm function (lm stands for “linear model” - yes, an ANOVA is a type of linear model). foam_mod &lt;- lm(foamIndx ~ method, data = espresso) Before proceeding, we need to check the assumptions of the model. This can be done visually using the autoplot function in the ggfortify package. If you don’t have the package installed, install it now (install.packages(\"ggfortify\")). library(ggfortify) autoplot(foam_mod) Figure 10.2: Diagnostic plots for the ANOVA model. This looks great. The main thing to look at here in Figure is the “Q-Q” plot on the top right. We want those points to be approximately along the line. If that is the case, then it tells us that the model’s residuals are normally distributed (this is one of the assumptions of ANOVA). We may cover these diagnostic plots more thoroughly later. You can find more details on pages 112-113 of the Beckerman et al textbook, or at the following if you are interested: https://data.library.virginia.edu/diagnostic-plots/. Trust me, everything here looks great. Now let’s evaluate our ANOVA model. We do this using two functions: anova and summary (it sounds strange, but yes we do use a function called anova on our ANOVA model). First, the anova. This gives us the following summary: anova(foam_mod) ## Analysis of Variance Table ## ## Response: foamIndx ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## method 2 4065.2 2032.59 28.413 4.699e-07 *** ## Residuals 24 1716.9 71.54 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This gives some numbers (degrees of freedom, sum of squares, mean squares). These are the important values that go into calculating an F value (also called an F-statistic). We will not worry about these details now, except to say that large F-statistics mean that we are more certain that there is a difference between the groups (and that the p-value is smaller). In this case, the F-value is 28.413. As with the t-test, R compares this value to a theoretical distribution (a “table”), based on two degrees of freedom. The first one is number of groups minus one, i.e. 2.000 in this case. The second one is the overall sample size, minus the number of groups, i.e. 24.000, in this case. This results in a p-value of 0.0000004699 (very highly significant!). Based on this p-value we can reject the null hypothesis that there is no difference between groups. We might report this simple result like this: The foam index varied significantly among groups (ANOVA: F = 28.413, d.f. = 2 and 24, p = 0.000). 10.4 Where are the differences? This model output doesn’t tell us where those differences are, nor does it tell us what the estimated mean values of foaminess are for the three groups: what are the effects? We need to dig further into the model to get to these details. There are several ways to do this and we’ll look at one of them. We do this using the summary function. summary(foam_mod) ## ## Call: ## lm(formula = foamIndx ~ method, data = espresso) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.62 -6.60 0.41 5.73 16.49 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.400 2.819 11.492 3.04e-11 *** ## methodHIP 28.900 3.987 7.248 1.73e-07 *** ## methodIES 7.300 3.987 1.831 0.0796 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.458 on 24 degrees of freedom ## Multiple R-squared: 0.7031, Adjusted R-squared: 0.6783 ## F-statistic: 28.41 on 2 and 24 DF, p-value: 4.699e-07 To properly interpret this output you need to understand something called “treatment contrasts”. Essentially, contrasts define how model coefficients (the estimates made by the model) are presented in R outputs. They are a bit hard to wrap your head aroudn and I STRONGLY recommend that you always do this with reference to a plot of the actual data, and the mean values for your groups. To do this you can use group_by and summarise to calculate the means for your the levels of your explanatory variable. espresso %&gt;% group_by(method) %&gt;% summarise(gp_mean = mean(foamIndx)) ## # A tibble: 3 x 2 ## method gp_mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 BM 32.4 ## 2 HIP 61.3 ## 3 IES 39.7 Look at the coefficients of the model. Remember that you have three levels in your explanatory variable, but only two levels are shown in the summary. Which one is missing? The “missing” group is the first one alphabetically (i.e. BM). The estimate (of the mean) for this group is labelled “(Intercept)” (with a value of 32.4. This is like a baseline or reference value, and the estimates for the other groups (HIP and IES), are differences between this baseline value and the estimated mean for those groups. In other words, the second group (HIP) is 28.9 more than 32.4 (32.4 + 28.9 = 61.3). Similarly, the third group (IES) is 7.3 more than 32.4 (32.4 + 7.3 = 39.7). Compare these values with the ones you got above using summarise - they should be the same. This is illustrated below in Figure A. You can see that the coefficients of the model are the same as the lengths of the arrows that run from 0 (for the first level of method (BM), the Intercept) or from this reference value. It is often a good idea to sketch something like this on paper when you are trying to understand your model outputs! Likewise, the t-values and p-values, are evaluating differences between the focal group and this baseline. Thus in this case, the comparisons (the “contrasts”) are between the intercept (BM) and the second level (HIP), and the intercept (BM) and the third level (IES). There is no formal statistical comparison between HIP and IES. You can see that it is very important to understand the levels of your explanatory variable, and how these relate to the summary outputs of the model. It can be useful to use the function relevel to manipulate the explanatory variable to make sure that the output gives you the comparisons you are interested in. Another simple trick would be to always ensure that your reference group (e.g. “control”) comes first alphabetically and is therefore selected by R as the intercept (reference point). For example, we can relevel the method variable so that the levels are re-ordered as HIP, BM, then IES so that the comparisons are between zero-HIP, HIP-BM and HIP-IES. (make sure that you understand this before proceeding). #This is what the original data looks like: levels(espresso$method) ## [1] &quot;BM&quot; &quot;HIP&quot; &quot;IES&quot; #releveling changes this by changing the reference. espresso_2 &lt;- espresso %&gt;% mutate(method = relevel(method,ref = &quot;HIP&quot;)) levels(espresso_2$method) ## [1] &quot;HIP&quot; &quot;BM&quot; &quot;IES&quot; Now we can refit the model with this modified data set and see what difference that made: foam_mod2 &lt;- lm(foamIndx ~ method, data = espresso_2) anova(foam_mod2) ## Analysis of Variance Table ## ## Response: foamIndx ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## method 2 4065.2 2032.59 28.413 4.699e-07 *** ## Residuals 24 1716.9 71.54 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(foam_mod2) ## ## Call: ## lm(formula = foamIndx ~ method, data = espresso_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.62 -6.60 0.41 5.73 16.49 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 61.300 2.819 21.743 &lt; 2e-16 *** ## methodBM -28.900 3.987 -7.248 1.73e-07 *** ## methodIES -21.600 3.987 -5.417 1.45e-05 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.458 on 24 degrees of freedom ## Multiple R-squared: 0.7031, Adjusted R-squared: 0.6783 ## F-statistic: 28.41 on 2 and 24 DF, p-value: 4.699e-07 Now the coefficients in the model summary look different, but the model is actually the same. Compare the two graphs in Figure - can you see the differences/similarities? Figure 10.3: Comparison illustrating the difference between ANOVA models using (A) BM and (B) HIP as references in the espresso foam data set. So, from the first of the model outputs above you can say that BM is significantly different than HIP (t= 7.248, p &lt; 0.0001), but that BM is not significantly different from IES (t = 1.831, p = 0.0796). Then, from the second one you can see that HIP is significantly different from IES (t=-5.417, p &lt; 0.0001). You could write this into the main text, include the information in a figure caption (i.e. add it to Figure . e.g. The foam index varied significantly among groups (ANOVA: F = 28.413, d.f. = 2 and 24, p = 0.000). The pairwise comparisons in the ANOVA model showed that means of BM and HIP were significantly different (t= 7.248, p &lt; 0.0001), as were those of HIP and IES (t=-5.417, p &lt; 0.0001), but the BM-IES comparison showed no significant difference (t= 1.831, p= 0.0796). 10.5 Tukey’s Honestly Significant Difference (HSD) An alternative way to approach these comparisons between groups is to use something called a post-hoc multiple comparison test. The words “post-hoc” mean “after the event” – i.e. after the ANOVA in this case – while the “multiple comparison” refers to the (potentially many) pairwise comparisons that you would like to make with an ANOVA. One of the most widely used post-hoc tests is called Tukey’s Honestly Significant Difference (Tukey HSD) test. There is a convenient R package called agricolae that will do these for you. #You only need to do this once! install.packages(&quot;agricolae&quot;) When you have the package installed, you can load it (using library). Then you can run the Tukey HSD test using the function HSD.test. The first argument for the function is the name of the model, followed by the name of the variable you are comparing (in this case method) and finally console = TRUE tells the function to print the output to your computer screen. library(agricolae) HSD.test(foam_mod2, &quot;method&quot;, console=TRUE) ## ## Study: foam_mod2 ~ &quot;method&quot; ## ## HSD Test for foamIndx ## ## Mean Square Error: 71.5383 ## ## method, means ## ## foamIndx std r Min Max ## BM 32.4 7.300060 9 21.02 39.65 ## HIP 61.3 10.100604 9 46.68 73.19 ## IES 39.7 7.700768 9 32.68 56.19 ## ## Alpha: 0.05 ; DF Error: 24 ## Critical Value of Studentized Range: 3.531697 ## ## Minimun Significant Difference: 9.957069 ## ## Treatments with the same letter are not significantly different. ## ## foamIndx groups ## HIP 61.3 a ## IES 39.7 b ## BM 32.4 b The output is long-winded, and the main thing to look at is the part at the end. The key to understanding this is actually written in the output, “Treatments with the same letter are not significantly different”. You could include these in a figure or a table with text like, “Means followed by the same letter did not differ significantly (Tukey HSD test, p&gt;0.05)”. 10.6 ANOVA calculation “by hand” (optional) The ANOVA calculation involves calculating something called an F-value or F-statistic (the F stands for Fisher, who invented ANOVA). This is similar to the t-statistic in that it is a ratio between two quantities, in this case variances. In ANOVA, the F-statistic is calculated as the “treatment variance” divided by the “error variance”. What does that mean? Let’s consider the espresso data set again. In the Figure , you can see on the left (A) the black horizontal line which is the overall mean foam index. The vertical lines are the “errors” or departures from the overall mean value, colour coded by treatment (i.e. method). These can be quantified by summing up their square values (squaring ensures that the summed values are all positive). We call this quantity the Total Sum of Squares (SSTotal). If there is a lot of variation, the sum of squares will be large, if there is little variation the sum of squares will be small. On the right hand side (Figure B) we see the same data points. However, this time the horizontal lines represent the treatment-specific mean values, and the vertical lines illustrate the errors from these mean values. Again we can sum up these as sum of squares, which we call the Error Sum of Squares (SSError). The difference between those values is called the Treatment Sum of Squares (SSTreatment) and is the key to ANOVA - it represents the importance of the treatment: \\[SSTreatment = SSTotal - SSError\\]. If that doesn’t make sense yet, picture the case where the treatment-specific means are all very similar, and are therefore very close to the overall mean. Now the difference between the Total Sum of Squares and the Error Sum of Squares will be small. Sketch out a couple of examples with pen on paper if that helps. You should now see that you can investigate differences among means by looking at variation. ## [1] 44.46667 Figure 10.4: The relative size of the squared residual errors from the overall mean (SSTotal) (A) and from the treatment-specific means (SSError) (B) tell us about the importance of the treatment variable. The difference between the two values is the “treatment sum of squares”. In the following I will show how these calculations can be done “by hand” in R. The purpose of showing you this is to demonstrate exactly how the lm model that you fitted above works, and prove to yourself that it is not rocket science… you will never need to do this in real life, because you have the wonderful lm function. Here goes… First, calculate the total sum of squares: (SSTotal = sum((overallMean-espresso$foamIndx)^2)) ## [1] 5782.099 Now calculate the group-specific means: (groupMeans&lt;-espresso %&gt;% group_by(method) %&gt;% summarise(groupMean = mean(foamIndx)) %&gt;% pull(groupMean)) ## [1] 32.4 61.3 39.7 Now add those group-specific mean values to the dataset using left_join so that you can calculate the group-specific errors. espresso &lt;- left_join(espresso,espresso %&gt;% group_by(method) %&gt;% summarise(groupMean = mean(foamIndx))) %&gt;% mutate(groupSS = (foamIndx - groupMean)^2) head(espresso) ## foamIndx method groupMean groupSS ## 1 36.64 BM 32.4 17.9776 ## 2 39.65 BM 32.4 52.5625 ## 3 37.74 BM 32.4 28.5156 ## 4 35.96 BM 32.4 12.6736 ## 5 38.52 BM 32.4 37.4544 ## 6 21.02 BM 32.4 129.5044 Then, to calculate the errors: (SSError &lt;- espresso %&gt;% summarise(sum(groupSS)) %&gt;% pull()) ## [1] 1716.919 From there, you can calculate the Treatment Sum of Squares (SSTreatment &lt;- SSTotal - SSError) ## [1] 4065.18 So far, so good - but we can’t just look at the ratio of SSTreatment/SSError, because sum of square errors always increase with sample size. We can account for this by dividing We need to take account of sample size (degrees of freedom) by dividing these sum of squares by the degrees of freedom to give us variances. There are 3 treatment groups and 9 samples per group. Therefore there are 2 degrees of freedom for the treatment, and 8 degrees of freedom per each of the three treatments, giving a total of 8*3 = 24 error degrees of freedom. Now we need to correct for degrees of freedom, which will give us variances. (meanSSTreatment &lt;- SSTreatment/2) ## [1] 2032.59 (meanSSError &lt;- SSError/24) ## [1] 71.5383 The F-statistic is then the ratio of these values. (Fstat &lt;- meanSSTreatment/meanSSError) ## [1] 28.41261 We can “look up” the p-value associated with this F-statistic using the pf function (pf stands for probability of f) like this: 1-pf(Fstat,df1=2,df2=24) ## [1] 4.698636e-07 As you can see, the method is a bit laborious and time consuming but it is conceptually fairly straightforward - it all hinges on the ratio of variation due to treatment effect vs. overall variation. Signal and noise. "],
["linear-models-with-a-several-categorical-explanatory-variables.html", "Chapter 11 Linear models with a several categorical explanatory variables 11.1 Background 11.2 Fitting a two-way ANOVA model 11.3 Summarising the model (anova) 11.4 Summarising the model (summary) 11.5 A 2-way ANOVA on a fish behaviour study", " Chapter 11 Linear models with a several categorical explanatory variables 11.1 Background In the one-way ANOVA we covered in the previous worksheet we were interested in understanding the effect of a single categorical explanatory variable with two or more levels on a continuous response variable. Although the explanatory variable must be categorical (i.e. with discrete levels), it could represent a continuous variable. For example, the explanatory variable could be a two-level soil nutrient level (high or low), even though nutrient level is a continuous variable and one could measure the actual quantitative value of nutrients in mg/g. The two-way ANOVA is an extension of one-way ANOVA that allows you to investigate the effect of two categorical variables. This can be useful in an experimental context. For example, one might have run an experiment investigating in the effect of two types of diet (lowProtein and highProtein), and genotype (gt1 and gt2), on adult size of a pest species. It is worth thinking about what potential outcomes there are for this experiment. There may be no effect of diet, and no effect of genotype. There may be an effect of one of these variables but not the other. The effect of the diet might be the same for the different genotypes, or it might be different. Some of these different possibile outcomes are illustrated in Figure . the titles indicate with Y (yes) or N (no) whether the figure shows a significant diet, genotype (gt) or interaction (int) effect. The dotted lines joining the estimates for the two genotypes are a kind of interaction plot: where they are parallel, there is no interaction. Figure 11.1: Some potential results of the experiment. There may be a significant effect (or not) of both of the main effects (diet and genotype) and there may be a significant interaction effect (or not). In the model we aim to quantify these effects, and ask if they are statistically significant (i.e. if the effect sizes are &gt;0). We divide the effects of the explanatory variables into two types: main effects and interaction effects. The main effects are the overall effect of the explanatory variables (genotype and diet in this case) while the interaction effect allows us to ask whether one main effect depends on another. In this case we are asking whether the effect of diet depends on genotype (and vice versa). Make sure that you understand this important concept. 11.2 Fitting a two-way ANOVA model Let’s use R to fit a two-way ANOVA model using data from the example I just described. As with one-way ANOVA, you can fit a two-way ANOVA model in R using lm. First, import the insectDiet.csv data and plot it, to produce a plot like in Figure . From looking at the graph in Figure you can see (a) genotype 1 tends to be larger than genotype 2; (b) insects raised on a high protein diet tend to be larger than those on a low protein diet; and (c) the effect of the diet (i.e. the difference in size between the insects raised on the different diets) is larger for genotype 1 than it is for genotype 2. But are these differences statistically meaningful? insectDiet &lt;- read.csv(&quot;CourseData/insectDiet.csv&quot;) ggplot(insectDiet,aes(x = genotype,y = lengthMM,fill=diet)) + geom_boxplot() + xlab(&quot;Genotype&quot;)+ ylab(&quot;Length (mm)&quot;) Figure 11.2: The effect of diet protein content and genotype on adult size of an insect species To address this question, we will fit a linear model (the two-way ANOVA) to estimate the effects of diet and genotype. The model formula is lengthMM ~ genotype + diet + genotype:diet. Let’s try to understand this. The genotype + diet part represents the main effects of these two variables, and the genotype:diet part represents the interaction effect between them. This formulacan be shortened to lengthMM ~ genotype * diet (i.e. this is exactly equivalent to the more complicated-looking formula), but I recommend to use the longer version because it is clearer. So we fit the model like this - putting the formula first, then telling R which data to use: mod_A &lt;- lm(lengthMM ~ genotype + diet + genotype:diet, data = insectDiet) Then we can look at diagnostic plots, as with ANOVA etc.: library(ggfortify) autoplot(mod_A) These all look OK. The slightly odd structure in the QQ-plot is caused by the fact that the length data are rounded to the nearest millimeter. There is no evidence of heteroscedasticity (left hand plots) now any major outliers. 11.3 Summarising the model (anova) Since we are satisfied with the diagnostic plots we can proceed by summarising the model using first anova and then summary. anova(mod_A) ## Analysis of Variance Table ## ## Response: lengthMM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 1 426.02 426.02 99.575 7.135e-13 *** ## diet 1 111.02 111.02 25.949 7.064e-06 *** ## genotype:diet 1 54.19 54.19 12.665 0.0009073 *** ## Residuals 44 188.25 4.28 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This summary Analysis of Variance Table is similar to the ones you have already seen for one-way ANOVA and linear regression. It just has some extra rows because you have extra explanatory variables. It shows you the degrees of freedom for the different terms in the model (all 1, because they have two levels), the sum of squares (Sum Sq) and mean sum of squares (Mean Sq) and the associated F value and p-value (Pr(&gt;F)). Those F values are all large, leading to highly-significant p-values. This means that all of those terms in the model explain a significant proportion of the variation in insect length. But as you know, this summary table doesn’t tell you the direction of the effects. The obvious way to understand your data is to simply look at the plot you have already produced. You could also make an interaction plot which is a simplified version of the plot of the raw data. To do this you first need to create a summary table using dplyr tools summarise and group_by to get the mean and standard errors of the mean: insectDiet_means &lt;- insectDiet %&gt;% group_by(genotype, diet) %&gt;% # &lt;- remember to group by *both* factors summarise(MeanLength = mean(lengthMM),SELength = sd(lengthMM)/sqrt(n())) insectDiet_means ## # A tibble: 4 x 4 ## # Groups: genotype [2] ## genotype diet MeanLength SELength ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gt1 highProtein 25.8 0.672 ## 2 gt1 lowProtein 20.7 0.527 ## 3 gt2 highProtein 17.8 0.698 ## 4 gt2 lowProtein 16.8 0.458 Then you can make a simple plot of this information by plotting points, and lines joining them: (A&lt;-ggplot(insectDiet_means, aes(x = genotype, y = MeanLength, colour = diet, group = diet)) + geom_point(size = 4) + geom_line()) You could add error bars to the points by adding a line defining the ymin and ymax values from the data summary like this: ggplot(insectDiet_means, aes(x = genotype, y = MeanLength, colour = diet, group = diet, ymin = MeanLength - SELength, ymax = MeanLength + SELength)) + geom_point(size = 4) + geom_line() + geom_errorbar(width = 0.1) But are these points statistically significantly different from each other? To answer that question we need to use a post-hoc test library(agricolae) HSD.test(mod_A, trt = c(&quot;diet&quot;, &quot;genotype&quot;), console = TRUE) ## ## Study: mod_A ~ c(&quot;diet&quot;, &quot;genotype&quot;) ## ## HSD Test for lengthMM ## ## Mean Square Error: 4.278409 ## ## diet:genotype, means ## ## lengthMM std r Min Max ## highProtein:gt1 25.83333 2.329000 12 23 31 ## highProtein:gt2 17.75000 2.416797 12 14 22 ## lowProtein:gt1 20.66667 1.825742 12 17 24 ## lowProtein:gt2 16.83333 1.585923 12 14 19 ## ## Alpha: 0.05 ; DF Error: 44 ## Critical Value of Studentized Range: 3.775958 ## ## Minimun Significant Difference: 2.254643 ## ## Treatments with the same letter are not significantly different. ## ## lengthMM groups ## highProtein:gt1 25.83333 a ## lowProtein:gt1 20.66667 b ## highProtein:gt2 17.75000 c ## lowProtein:gt2 16.83333 c The important part of this output is at the bottom where it tells us Treatments with the same letter are not significantly different.. You can see that the mean lengths between diets for genotype 1 are significantly different (they do not share a letter). However, there is no significant difference between diets for genotype 2 (they share the same letter, c). The two genotypes are also significantly different from each other. 11.4 Summarising the model (summary) This (above) is generally enough information for a complete write up of results. However, you can ask R to provide the model summary that includes the \\(R^2\\) values, coefficient estimates and standard errors using summary. summary(mod_A) ## ## Call: ## lm(formula = lengthMM ~ genotype + diet + genotype:diet, data = insectDiet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7500 -1.0417 0.1667 1.2500 5.1667 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 25.8333 0.5971 43.264 ## genotypegt2 -8.0833 0.8444 -9.572 ## dietlowProtein -5.1667 0.8444 -6.118 ## genotypegt2:dietlowProtein 4.2500 1.1942 3.559 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## genotypegt2 2.53e-12 *** ## dietlowProtein 2.26e-07 *** ## genotypegt2:dietlowProtein 0.000907 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.068 on 44 degrees of freedom ## Multiple R-squared: 0.7585, Adjusted R-squared: 0.742 ## F-statistic: 46.06 on 3 and 44 DF, p-value: 1.254e-13 The most useful thing shown here is the \\(R^2\\) value. Because we have several terms in the model we should use the Adjusted R-squared value of 0.742. This indicates that our model explains 74.2% of variation in insect length. The next bit is not 100% necessary most of the time… We already have a good idea of the mean values and standard errors for these data look because we calculated them above directly from the data. For completeness though I will now run through the coefficient estimates part of the summary table. The coefficient Estimates here are interpreted in a similar way to a one-way ANOVA. Again, it is important to know what the reference point is. When you understand this you can reconstruct the mean values for the various levels of the variables that are estimated by the model. You will see that the model estimates lead to precisely the same estimates as obtained from summarising the data. Here you can see that: The (Intercept) is 25.8333 and must refer to the point for genotype 1 on a high protein diet (look at the value of the intercept and compare to the graph/summary table, and/or the output from the Tukey test). The second coefficient (genotypegt2) is -8.0833 which is the difference between the reference (intercept) and the value for genotype 2 on a high protein diet: (25.8333 + (-8.0833) = 17.75). The third coefficient (dietlowProtein) is -5.1667 which is the difference between the reference point and for genotype 1 on a low protein diet: (25.8333 + (-5.1667) = 20.6666). The final coefficient dietlowProtein:genotypegt2 is 4.25 and is “interaction effect” of diet and genotype and represents the additional effect of genotype when it is on diet. In other words, in comparison to the reference point (genotype 1 &amp; high protein diet), the effect of a low protein diet is negative (-8.0833), as is the effect of being genotype 2 (-5.1667). However, having both a low protein diet and being genotype 2 leads to an additional positive effect (4.25) on length. The resulting estimate of mean length for genotype 2 on a low protein diet is 25.8333 + (-8.0833) + (-5.1667) + 4.25 = 16.833. This is a bit complicated so my advice is generally to refer to the figures and the outputs of the Tukey.HSD function to obtain the estimate in the different groups. The logic and methods of the two-way ANOVA can be extended to produce \\(n\\)-way ANOVA with \\(n\\) categorical variables. 11.5 A 2-way ANOVA on a fish behaviour study 11.5.1 Background Individual differences in animal personality and external appearance such as colouration patterns have both been extensively studied separately. A significant body of research has explored many of pertinent ecological and biological aspects that can be affected by them and their impact upon fitness. Currently little is known about how both factors interact and their effect on reproductive success. Researchers carried out a study looking at differences in personality and its interaction with colour phenotype in zebrafish (Danio rerio). They used two colour morphs, “homogenous” which has clearly defined lateral stripes, and “heterogenous” which has more variable and less clear patterns. They also assigned individuals to two personality types which they called “Proactive” (adventurous, risk taking) and “Reactive” (timid, less risk taking). They did this by recording how they explore a new environment The two variables of interest are: Colour pattern (homogenous and heterogenous) Personality (proactive and reactive) The research questions are: What is the relative influence of colour pattern and personality? Which is more important? How do the variables interact to determine fitness? e.g. do proactive individuals do better than reactive ones, and does this depend on colour pattern? Or some other pattern? 11.5.2 Your task Import the data set, fishPersonality.csv Plot the data (e.g. as a box plot) Fit an ANOVA model using lm. Look at diagnostic plots from this model (autoplot) Use anova to get an Analysis of Variance summary table, and interpret the results. Get the coefficient summary (summary) and interpret the output. Do post-hoc Tukey tests (e.g. using HSD.test from the agricolae package). Interpret the results. Sum up your findings with reference to the initial research questions. "],
["linear-models-with-a-single-continuous-explanatory-variable.html", "Chapter 12 Linear models with a single continuous explanatory variable 12.1 Background 12.2 Some theory 12.3 Evaluating a hypothesis with a linear regression model 12.4 Assumptions 12.5 An example using R 12.6 Exercises - Linear Regression", " Chapter 12 Linear models with a single continuous explanatory variable 12.1 Background Linear regression models, at their simplest, are a method of estimating the linear (straight line) relationships between two continuous variables. As an example, picture the relationship between two variables height and hand width (Figure ). In this figure there is a clear relationship between the two variables, and the straight line running through the cloud of data points is the fitted linear regression model. The aim of linear regression is to (1) determine if there is a meaningful statistical relationship between the explanatory variable(s) and the response variable, and (2) to quantify those relationships by estimating the characteristics of those relationships. These characteristics include the slope and intercepts of fitted models, and the amount of variation explained by variables in the model. Figure 12.1: A linear regression model fitted through data points. 12.2 Some theory To understand linear regression models it is important to know that the equation of a straight line is \\(y = ax+b\\). In this equation, \\(y\\) is the response variable and \\(x\\) is the explanatory variable, and \\(a\\) and \\(b\\) are the slope and intercept of the line with the vertical axis (y-axis). These (\\(a\\) and \\(b\\)) are called coefficients. These are illustrated in Figure . Figure 12.2: The equation of straight lines. When looking at data points on a graph, unless all of the data points are arranged perfectly along a straight line, there will be some distance between the points and the line. These distances, measured parallel to the vertical axis, are called residuals (you have encountered them before in this course). These residuals represent the variation left after fitting the line (a linear model) through the data. Because we want to fit a model that explains as much variation as possible, it is intuitive that we should wish to minimise this residual variation. One way of doing this is by minimising the sum of squares of the residuals (again, you have come across this concept a few times before). In other words, we add up the squares of each of the residuals. We square the values, rather than simply adding up the residuals themselves because we want to ensure that the positive and negative values don’t cancel each other out (a square of a negative number is positive). This method is called least squares regression and is illustrated in Figure : Which is the best fitting line? Figure 12.3: Residuals and least squares: which is the best fitting line? In fact, these residuals represent “error” caused by factors including measurement error, random variation, variation caused by unmeasured factors etc. This error term is given the label, \\(\\epsilon\\). Thus we can write the model equation as: \\[y = ax+b+\\epsilon\\] Sometimes, this equation is written with using the beta symbol (\\(\\beta\\)) for the coefficients, so that the slope is \\(\\beta_0\\) and the intercept is \\(\\beta_1\\) for example. \\[y = \\beta_0 x+\\beta_1+\\epsilon\\] The idea is that this equation, and its coefficients and error estimates, describe the relationship we are interested in (including the error or uncertainty). Together this information allows us to not only determine if there is a statistically significant relationship, but also what the nature of the relationship is, and the uncertainty in our estimates. 12.3 Evaluating a hypothesis with a linear regression model Usually, the most important hypothesis test involved with a linear regression model relates to the slope: is the slope coefficient significantly different from 0?, or should we accept the null hypothesis that the slope is no different from 0. Sometimes hypotheses like this are a bit boring, because we already know the answer before collecting and analysing the data. What we usually don’t know is the nature of the relationship (the slope, intercept, their errors, and amount of variation explained). Usually it is more interesting and meaningful to focus on those details. The following example, where we focus on the relationship between hand width and height, is one of these “boring” cases: we already know there is a relationship. Nevertheless, we’ll use this example because it helps us understand how this hypothesis test works. The aim of this section is to give you some intuition on how the hypothesis test works. We can address the slope hypothesis by calculating an F-value in a similar way to how we used them in ANOVA. Recall that F-values are ratios of variances. To understand how these work in the context of a linear regression we need to think clearly about the slope hypothesis: The null hypothesis is that the slope is not significantly different to 0 (that the data can be explained by random variation). The alternative hypothesis is that the slope is significantly different from 0. The first step in evaluating these hypotheses is to calculate what the total sum of squares6 is when the null hypothesis is true (Figure A) - this value is the total variation that the model is trying to explain. Then we fit our model using least squares and figure out what the residual sum of squares is from this model (Figure B). This is the amount of variation left after the model has explained some of the total variation - it is sometimes called residual error, or simply error. The difference between these two values is the explained sum of squares, which measures the amount of variation in \\(y\\) explained by variation in \\(x\\). The rationale for this is that the model is trying to explain total variation. After fitting the model there will always be some unexplained variation (“residual error”) left. If we can estimate total variation and unexplained variation, then the amount of variation explained can be calculated with a simple subtraction: \\[Total = Explained + Residual\\] … and, therefore … \\[Explained = Total - Residual\\] Before using these values we need to standardise them to control for sample size. This is necessary because sum of squares will always increase with sample size. We make this correction by dividing our sum of squares measures by the degrees of freedom. The d.f. for the explained sum of squares is 1, and the d.f. for the residual sum of squares is the number of observations minus 2. The result of these calculations is the mean explained sum of squares (mESS) and the mean residual sum of squares (mRSS). These “mean” quantities are variances, and the ratio between them gives us the F-value. Notice that this is very similar to the variance ratio used in the ANOVA. \\[F = \\frac{mESS}{mRSS}\\] If the explained variance (mESS) is large compared to the residual error variance (mRSS), then F will be large. The size of F tells us how likely or unlikely it is that the null hypothesis is true. When F is large, the probability that the slope is significantly different from 0 is high. To obtain the actual probabilities, the F-value must be compared to a theoretical distribution which depends on the two degrees of freedom (explained and residual d.f.). Once upon a time you would have looked this up in a printed table, but now R makes this very straightforward. Figure 12.4: (A) the total variation around the overall mean Height value (B) the residual error of the model. 12.4 Assumptions These models have similar assumptions to the other linear models7. These are (1) that the relationship between the variables is linear (hence the name); (2) that the data are continuous variables; (3) that the observations are randomly sampled; (4) that the errors in the model (the “residuals”) can be described by a normal distribution; and (5) and that the errors are “homoscedastic” (that they are constant through the range of the data). You can evaluate these things by looking at diagnostic plots after you have fitted the model. See page 112-113 in GSWR for a nice explanation. 12.5 An example using R Let’s now use R to fit a linear regression model to estimate the relationship between hand width and height. One application for such a model could be to predict height from a hand print, for example left at a crime scene. First, load the data: classData &lt;- read.csv(&quot;CourseData/classData.csv&quot;) We should then plot the data to make sure it looks OK. ggplot(classData,aes(x=HandWidth,y=Height)) + geom_point() This looks OK, and the relationship looks fairly linear. Now we can fit a model using the lm function (same as for ANOVA!).8 The response variable is always the one we would like to predict, in this case Height. The explanatory variable (sometimes called the predictor) is HandWidth. These are added to the model using a formula where they are separated with the ~ (“tilde”) symbol: Height ~ HandWidth. In the model expression, we also need to tell R where the data are using the data = argument. We can save the model as an R object by naming it e.g. mod_A &lt;-. mod_A &lt;- lm(Height ~ HandWidth, data = classData) Before proceeding further we should evaluate the model using a diagnostic plot. We can do this using the autoplot function in the ggfortify package (you may need to install and/or load the package). library(ggfortify) autoplot(mod_A) These diagnostic plots allow you to check that the assumptions of the model are not violated. On the left are two plots which (more or less) show the same thing. They show how the residuals (the errors in the model) vary with the predicted value (height). Looking at the plots allows a visual test for constant variance (homoscedasticity) along the range of the data. In an ideal case, there should be no pattern (e.g. humps) in these points. On the top right is the QQ-plot which shows how well the residuals match up to a theoretical normal distribution. In an ideal case, these points should line up on the diagonal line running across the plot. The bottom right plot shows “leverage” which is a measure of how much influence individual data points have on the model. Outliers will have large leverage and can mess up your model. Ideally, the points here should be in a cloud, with no points standing out from the others. Please read the pages 112-113 in the textbook GSWR for more on these. In this case, the model looks pretty good. Now that we are satisfied that the model doesn’t violate the assumptions we can dig into the model to see what it is telling us. To test the (slightly boring) slope hypothesis we use the anova function (again, this is the same as with the ANOVA). anova(mod_A) ## Analysis of Variance Table ## ## Response: Height ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HandWidth 1 1038.5 1038.46 28.397 1.017e-05 *** ## Residuals 29 1060.5 36.57 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 When you run this function, you get a summary table that looks exactly like the one you got with an ANOVA. There are degrees of freedom (Df), Sums of Squares (Sum Sq), Mean Sums of Squares (Mean Sq) and the F value and p-value (Pr(&gt;F)). The most important parts of this table are the F value (28.3968) and the p-value (0.00001017): as described above, large F values lead to small p-values. This tells us that it is VERY unlikely that the null hypothesis is true and we should accept the alternative hypothesis (that height is associated with hand width) We could report the results of this hypothesis test like this: There was a statistically significant association between hand width and height (F = 28.3968, d.f. = 1,29, p &lt; 0.001) Now we can dig deeper by asking for a summary of the model. summary(mod_A) ## ## Call: ## lm(formula = Height ~ HandWidth, data = classData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.498 -3.748 -0.188 5.502 8.812 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 133.9464 7.7862 17.203 &lt; 2e-16 *** ## HandWidth 4.6552 0.8736 5.329 1.02e-05 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.047 on 29 degrees of freedom ## Multiple R-squared: 0.4947, Adjusted R-squared: 0.4773 ## F-statistic: 28.4 on 1 and 29 DF, p-value: 1.017e-05 This summary has a lot of information. First we see Call which reminds us what the formula we have used to fit the model. Then there is some summary information about the residuals. Ideally these should be fairly balanced around 0 (i.e. the Min value should be negative but with the same magnitude as Max). If they are wildly different, then you might want to check the data or model. In this case they look OK. Then we get to the important part of the table - the Coefficients. This lists the coefficients of the model and shows first the Intercept and then the slope, which is given by the name of the explanatory variable (HandWidth here). For each coefficient we get the Estimate of its value, and the uncertainty in that estimate (the standard error (`Std. Error)). These estimates and errors are each followed by a t value and a p-value (Pr(&gt;|t|)). These values provide a test of whether the slope/intercept is different from zero. In this case they both are. The t-tests are indeed doing t-tests of these estimates, in the same way that a regular t-test works, so that the significance depends on the ratio between signal (the estimate) and the noise (the standard error). This is illustrated for the coefficient estimates for our model in Figure . Figure 12.5: Illustration of the coefficient estimates for our model. The peak of the distribution is at the coefficient estimate, and the spread of the distribution indicates the standard error of the mean for the estimate. The statistical significance of the coefficient is determined by the degree of overlap with 0. The summary then gives some information about the amount of residual variation left after the model has been fitted (this is the \\(\\epsilon\\) term in the equations at the start of this worksheet). Then we are told what the \\(R^2\\) value is 0.4947. The adjusted \\(R^2\\) is for use in multiple regression models, where there are many explanatory variables and should not be used for this simple regression model. So what does \\(R^2\\) actually mean? \\(R^2\\) is the square of the correlation coefficient \\(r\\) and is a measure of the amount of variation in the response variable (Height) that is explained by the model. If all the points were sitting on the regression line, the \\(R^2\\) value would be 1. This idea is illustrated in Figure . We could describe the model like this: There is a statistically significant association between hand width and height (F = 28.3968, d.f. = 1,29, p &lt; 0.001) The equation of the fitted model is: Height = 4.66(\\(\\pm\\) 0.87) \\(\\times\\) HandWidth + 133.95(\\(\\pm\\) 7.79). The model explains 49% of the variation in height (\\(R^2\\) = 0.495). … or maybe, The model, which explained 49% of the variation in height, showed that the slope of the relationship between hand width and height is 4.66 \\(\\pm\\) 0.87 which is significantly greater than 0 (t = 17.20, p &lt; 0.01) Figure 12.6: An illustration of different R-squared values. A plot is usually a good idea because it is easier for the reader to interpret than an equation, or coefficients. The ggplot2 package has a neat and simple function called geom_smooth which will add the fitted regression line to simple models like this. For linear regression models you simply need to tell it to use method = \"lm\". This will plot the fitted regression model, and will add, by default\" a shaded “ribbon” which represents the so called “95% confidence interval” for the fitted values. These are 2 time the standard error. ggplot(classData,aes(x = HandWidth,y = Height)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Question: If police find the 9.8cm wide hand print at a crime scene, what is your best guess of the height of the person involved? 12.6 Exercises - Linear Regression 12.6.1 Background Male crickets produce a “chirping” sound by rubbing the edges of their wings together: the male cricket rubs a sharp ridge on his wing against a series ridges on the other wing. In a 1948 study on striped ground cricket (Allonemobius fasciatus), the biologist George W. Pierce recorded the frequency of chirps (vibrations per second) in different temperature conditions. Crickets are ectotherms so their physiology and metabolism is influenced by temperature. We therefore believe that temperature might have an effect on their chirp frequency. 12.6.2 Your task The data file chirps.csv contains data from Pierce’s experiments. Your task is to analyse the data and find (1) whether there is a statistically significant relationship between temperature and chirp frequency and (2) what that relationship is. The data has two columns - chirps (the frequency in Hertz) and temperature (the temperature in Fahrenheit). You should express the relationship in Celsius. Import the data Use mutate to convert Fahrenheit to Celsius (Google it) Plot the data Fit a linear regression model with lm Look at diagnostic plots to evaluate the model Use anova to figure out if the effect of temperature is statistically significant. Use summary to obtain information about the coefficients and \\(R^2\\)-value. Summarise the model in words. Add model fit line to the plot. Can I use cricket chirp frequency as a kind of thermometer? sum of squares is simply a way to estimate variation.↩︎ t-tests, ANOVA and linear regression are all types of linear model, mathematically↩︎ R knows that this is a linear regression rather than an ANOVA because the explanatory variable is numeric rather than categorical - smart!.↩︎ "],
["linear-models-with-categorical-and-continuous-explanatory-variables.html", "Chapter 13 Linear models with categorical and continuous explanatory variables 13.1 Background 13.2 The height ~ hand width example. 13.3 Summarising with anova 13.4 The summary of coefficients (summary) 13.5 Simplifying the model", " Chapter 13 Linear models with categorical and continuous explanatory variables 13.1 Background In the previous worksheet we looked at linear models where there is a continuous response variable and two categorical explanatory variables (we call this type of linear model two-way ANOVA). In this worksheet we will look at linear models where the explanatory variables are both continuous and categorical. You can think of these as a kind of cross between ANOVA and linear regression. These type of models are often given the name “ANCOVA” or Analysis of Covariance. In a simple case, you might be interested in a model with a continuous response variable (e.g. height) and continous and a categorical explanatory variables (e.g. hand width and gender). The categoreical variable may have any number of levels, but the simplest case is with two (e.g. gender with male and female levels). Some of these different possible outcomes of this type of analysis are illustrated in Figure . We might see that neither of the two explanatory variables has a significant effect. We might see that one of them does but not the other one. We might see an interaction effect (where the effect of one variable (e.g. hand width) depends on the other (e.g gender). We might also see an interaction effect but no main effect. Figure 13.1: Some potential results of the experiment. There may be a significant effect (or not) of both of the main effects (diet and genotype) and there may be a significant interaction effect (or not). 13.2 The height ~ hand width example. In a previous class (linear regression) you explored the relationship between hand width and height. The aim there was (1) to determine if the relationship (i.e. the slope) was significantly differnet from 0. and (2) to make an estimate of what the equation of the relationship would be so you could make predictions of height from hand width. Here we will extend that example by asking whether there are differences between males and females. We’ll begin by plotting the data (Figure ). classData &lt;- read.csv(&quot;CourseData/classData.csv&quot;) (A&lt;-ggplot(classData,aes(x = HandWidth,y = Height,colour = Gender)) + geom_point() + geom_smooth(method =&quot;lm&quot;,se = FALSE)) #This shows the ANCOVA model Figure 13.2: ANCOVA on hand width vs. height data in males and females #before we have even fit it! You can see that our two continuous variables, Height (the response variable) and HandWidth (one of the explanatory variables) are associated: There is an overall positive relationship between HandWidth and Height You can also see that Gender (the categorical explanatory variable) is important: males tend to be taller than females for any given hand width. For example, a female with hand width of 9cm is ~172cm tall while a male would be about 180cm tall. This shows us that males have a higher intercept than females. There is also a slight difference in the slope of the relationship, with males having a slightly steeper slope than females. We already know that the overall relationship between hand width and height is significant (from the regression worksheet). These new observations leave us with the following additional questions: (1) are the intercepts for males and females significantly different? (2) are the slopes for males and females significantly different (or would a model with a single slope, but different intercepts be better)? Now we can fit our model using the lm function. The model formula is Height ~ HandWidth + Gender + HandWidth:Gender. The HandWidth and Gender are the so called main effects while HandWidth:Gender represents the interaction between them (i.e. it is used to address the question “does the effect of hand width differ between the sexes?”). R knows that is fitting an ANCOVA type model rather than a two-way ANOVA because it knows the type of variables that it is dealing with. You can see this if you ask R to tell you what the class of the variables are: class(classData$Gender) ## [1] &quot;character&quot; class(classData$HandWidth) ## [1] &quot;numeric&quot; mod_A &lt;- lm(Height ~ HandWidth + Gender + HandWidth:Gender, data = classData) The first step should, as before, be to check out the diagnostic plots. We should not read to much into these in this case, because we have a small sample size. Nevertheless, lets keep with good habits: library(ggfortify) autoplot(mod_A) These look good. No evidence of non-normality in the residuals, no heteroscedasticity and no weird outliers. 13.3 Summarising with anova Now we can get the anova table of our ANCOVA model (yes, I know that sounds strange.). anova(mod_A) ## Analysis of Variance Table ## ## Response: Height ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HandWidth 1 1038.46 1038.46 40.8621 7.576e-07 *** ## Gender 1 369.46 369.46 14.5378 0.0007246 *** ## HandWidth:Gender 1 4.89 4.89 0.1922 0.6645538 ## Residuals 27 686.17 25.41 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This type of sequential sum of squares Analysis of Variance table should be getting fairly familiar to you now, but let’s unpack what this means. There are four rows in the summary table - one for each of the terms in the model (HandWidth, Gender and HandWidth:Gender), and one for the Residuals (the unexplained variation that remains after fitting the model). The table includes degrees of freedom (Df), sum of squares (Sum Sq), mean sum of squares (Mean Sq) and the associated F and p-values (F value and Pr(&gt;F). You can interpret the mean sum of squares column in terms of the amount of variation in the response variable (Height) that is explained by the term: The table first tells us the amount of variation (in terms of Mean Sum of Squares) in Height that is captured by a model that includes a common slope for both genders (1038.46). Then it tells us that an additional bit of variation (369.46) is captured if we allow the intercepts to vary with gender. Then it tells us that a small additional amount of variation is explained by allowing the slope to vary between the genders (4.89). Finally, there is a bit of unexplained variation left over (Residuals) (25.41). So you can see that hand width explains most variation, followed by gender, followed by the interaction between them. You would report from this table something like this: Hand width and gender both explain a significant amount of the variation in height (ANCOVA - Handwidth: F = 40.862, d.f. = 1 and 27, p&lt;0.001; Gender: F = 14.538, d.f. = 1 and 27, p&lt;0.001). The interaction effect was not significant, which means that the slopes of the relationship between hand width and height are not significantly different (ANCOVA - F = 0.192, d.f. = 1 and 27, p = 0.665). It is of course useful to take the interpretation a bit further. You could do this with reference to the plot - e.g. Figure X shows the clear positive relationship between hand width and height and shows that the intercept for females is smaller than that for males. This which means that, for a given hand width, males tend to be taller. 13.4 The summary of coefficients (summary) To put some quantitative numbers on this description of the pattern we need to get the summary from R. summary(mod_A) ## ## Call: ## lm(formula = Height ~ HandWidth + Gender + HandWidth:Gender, ## data = classData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.480 -4.096 1.845 3.195 7.821 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 154.9285 10.3206 15.012 1.26e-14 ## HandWidth 1.7667 1.2707 1.390 0.176 ## GenderMale 1.3710 18.4712 0.074 0.941 ## HandWidth:GenderMale 0.8839 2.0160 0.438 0.665 ## ## (Intercept) *** ## HandWidth ## GenderMale ## HandWidth:GenderMale ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.041 on 27 degrees of freedom ## Multiple R-squared: 0.6731, Adjusted R-squared: 0.6368 ## F-statistic: 18.53 on 3 and 27 DF, p-value: 9.892e-07 This summary table gives the coefficients of the statistical model, their standard errors, and the t-test results of whether the estimate is greater than 0. This is the same as the summary tables given for ANOVA and linear regression. In the ANOVA summary tables, the estimates were given in relation to the reference level – the (Intercept) and these ANCOVA summary tables are no difference. Interpreting is best done with reference to the graph of the data and fitted model outputs (the graph above). The reference level (the (Intercept)) is the intercept for the line for the first level of the categorical variable (Females, in this case). Here the model estimates that the intercept for Females is at 154.9 (i.e. if you extended the line out to the left it would eventually cross the y-axis at this point). The next coefficient HandWidth is the slope of this Female line (1.7667). Then we have GenderMale: this coefficient is the difference in intercept between the Female and Male lines. This is followed by the intercept for the interaction term HandWidth:GenderMale: this is the difference between slopes for the two genders. We can therefore do some simple arithmetic to get the equations (i.e. slopes and intercepts) of the lines for both genders. For females this is easy (they are reference level, so you can just read the values directly from the table) - the intercept is 154.93 and the slope is 1.77. For males the intercept is 154.93 + 1.37 = 156.30. The slope is 1.77 + 0.88 = 2.65 We could add these equations to our reporting of the results. Figure X shows the clear positive relationship between hand width and height and shows that the intercept for females is smaller than that for males. This which means that, for a given hand width, males tend to be taller. The model fit for males is Height = 2.65\\(\\times\\)HandWidth + 156.30 and the fit for females is Height = 1.77\\(\\times\\)HandWidth + 154.93 You could check these by using geom_abline to add lines with those equations to the plot (just as a “sanity check”). A + geom_abline(intercept = 154.93, slope = 1.77) + geom_abline(intercept = 156.30, slope = 2.65) At the bottom of the summary output we are given the \\(R^2\\) values. Because this model has several terms (i.e. variables) in it we should use the adjusted \\(R^2\\) values. These have been corrected for the fact that the model has extra explanatory variables. So in this case, we could report that the model explains 64% of variation in Height (Adjusted \\(R^2\\) = 0.6368) - not bad! So, to describe this summary table more generally - the coefficients can be slopes, intercepts, differences between slopes, and differences between intercepts. They are slopes and intercepts for the first level of the categorical variable, and for the subsequent levels they are differences. Piecing these together can be hard to figure out without reference to the plot of the data and model fits - another good reason to plot your data! 13.5 Simplifying the model Our results above showed that the interaction between the gender and hand width was not significant. Think about what that means? It means that the effect of hand width on height (the slope) does not depend on gender. Therefore, one can argue that we don’t need to have a model that estimates both slopes - we could have a simpler model with one slope for both genders. In fact, creating models that are as simple as possible to explain the observations is a useful goal that is captured by the law of parsimony or “Occam’s razor”, which essentially states that simple explanations for a phenomenon are favorable to complex explanations. Let’s refit the model without this non-significant interaction: mod_B &lt;- lm(Height ~ HandWidth + Gender, data = classData) anova(mod_B) ## Analysis of Variance Table ## ## Response: Height ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HandWidth 1 1038.46 1038.46 42.076 4.992e-07 *** ## Gender 1 369.46 369.46 14.970 0.0005963 *** ## Residuals 28 691.05 24.68 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now all the terms in the model are significant. summary(mod_B) ## ## Call: ## lm(formula = Height ~ HandWidth + Gender, data = classData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.613 -3.797 1.446 3.203 8.210 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 152.0964 7.9323 19.174 &lt; 2e-16 *** ## HandWidth 2.1179 0.9722 2.179 0.037945 * ## GenderMale 9.3971 2.4288 3.869 0.000596 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.968 on 28 degrees of freedom ## Multiple R-squared: 0.6708, Adjusted R-squared: 0.6472 ## F-statistic: 28.52 on 2 and 28 DF, p-value: 1.758e-07 The coefficient summary now gives us a two intercept estimates (152.0964 for females and 152.0964 + 2.1179 = 154.2143 for males) and single estimate for a slope that applies to both genders (2.1179). Unfortunately, the handy geom_smooth function cannot handle this simpler model! We must take a slightly different, and sadly mode complicated approach: What we need to do is predict using the model what the height will be under different conditions. Think of this as “plugging values into an equation”. We want to predict heights across the range of hand widths (from 6.5cm to 11cm), and we need to do this for males and females. We do this by creating a “fake” dataset to predict from using the useful function expand.grid. This function takes inputs from columns of data and “expands” them to ensure that all possible combinations are included. predictData &lt;- expand.grid(HandWidth = c(6.5,11),Gender = c(&quot;Male&quot;,&quot;Female&quot;)) predictData ## HandWidth Gender ## 1 6.5 Male ## 2 11.0 Male ## 3 6.5 Female ## 4 11.0 Female Now we can use these values to predict what the heights will be for those particular combinations of values. The arguments for the predict function are the model name, then newdata = to give the function the data that you want to predict from. Here we can use the function to add the models predicted fitted value (fit) to the predictData object we just created. predictData$Height &lt;- predict(mod_B,newdata = predictData) predictData ## HandWidth Gender Height ## 1 6.5 Male 175.2598 ## 2 11.0 Male 184.7902 ## 3 6.5 Female 165.8626 ## 4 11.0 Female 175.3931 Now we can add lines for these predicted values to our plot. We do this using the geom_smooth function as before, but this time we use the arguments data = predictData to tell R to use the new data, and stat = \"identity\" and to ensure that we plot the data rather than fitting any model. You may wish to add an error ribbon to these lines. We will cover this in a later class (but see pages 159-164 in the GSWR textbook). ggplot(classData,aes(x = HandWidth,y = Height,colour = Gender)) + geom_point() + geom_smooth(data = predictData,stat=&quot;identity&quot;) We could report this in the usual way but first saying something like: “The interaction term was not significant (F = 0.1922, d.f. 1 and 27, p = 0.665) and I therefore simplified the model to remove this term. The resulting model with just HandWidth and Gender …” "],
["generalised-linear-models.html", "Chapter 14 Generalised linear models 14.1 Count data with Poisson errors.", " Chapter 14 Generalised linear models The models we have covered so far are ordinary linear models (including ANOVA, ANCOVA, ordinary linear regression etc.) that assume thatthe relationship between the explanatory variables and the response variable is linear, and that the systematic error in the model is constant (homoscedastic, i.e. the standard deviation of the data does not depend on the magnitude of the explanatory variable). In many cases this will not be the case. Non-linearity and heteroscedasticity tend to go hand-in-hand. Sometimes, for example, the data show an exponential growth type of pattern, and/or may be bounded in such a way that the errors cannot be homoscedastic. For example, counts of number of species on islands of different sizes have a lower bound at zero (you can’t have negative numbers of species!) and increase exponentially while the standard deviations are are small for small islands and large for large islands.; ratio data or percentage data such as proportion of individuals dying/surviving is bounded between 0 and 1 (0 - 100%). Transformation of the response variable could an option to linearise these data (although there would be problems with 0 values (because log(0) = -Infinity)), but a second problem is that the ordinary linear model assumes “homoscedasticity” - that the errors in the model are evenly distributed along the explanatory variables. This assumption will be violated in most cases. For example, with count data (e.g. number of species in relation to island size), the errors for very small islands will be smaller than those for large islands. In fact, even if we transform the response variable, for example by log transformation, the predictions of the model will allow errors that include negative counts. This is clearly a problem! Generalised linear models (GLMs) solve these problems by not only applying a transformation but also explicitly altering the error assumption. They do this using a link function to carry out the transformation and by choosing an error structure (sometimes referred to as family, or variance function). The choice of link and error structure can be a bit confusing, but there are so-called “canonical links” that are commonly associated with particular error strucutres. For example, a model for count data would usually have a log link and a Poisson error structure. The flexibility of the GLM approach means that one can fit GLM versions of all of the models you have already learned about until this points: ANOVA-like GLMs, ANCOVA-like GLMs, ordinary regression-like GLMs and so on. Here I will focus on this case, and in the next worksheet I will broaden the focus to illustrate uses of other error structures and links. 14.1 Count data with Poisson errors. The most common kind of count data where Poisson errors would be expected are frequencies of an event: we know how many times an event happened, but not how many times it did not happen (e.g. births, deaths, lightning strikes). In these cases: Linear model could lead to negative counts. Variance of response likely to increase with mean (it usually does with count data). Errors are non-normal. Zeros difficult to deal with by transformation (e.g. log(0) = -Inf). Other error families do not allow zero values. The standard (“canonical”) link used with the Poisson error family is the log link. The log link ensures that all fitted (i.e. predicted) values are positive, while the Poisson errors take account of the fact that the data are integer and the variance scales 1:1 with the mean (i.e. variance increases linearly and is equal to the mean). There are other potential link and error families that could be used with this kind of data, but we’ll stick with the standard ones here. Lets look at a couple of examples… 14.1.1 Example: Number of offspring in foxes. This example uses the fox.csv data set. This data set gives the number of offspring produced by a group of foxes, alongside the weight (in kg) of the mothers. Let’s import and plot the data. The first thing to notice is that, like all count data, the data are formed into horizontal rows of data reflecting the fact that the response data are integer values. There is clearly an increasing pattern, but how can we formally test for a statistical relationship. It is obvious that fitting an ordinary linear model though this data would not be the right approach: this would lead to the prediction of negative number of offspring for small foxes, and also, the variance appears to increase with weight/number of offpspring. Therefore this is a good candidate for a GLM. The data are bounded at 0, and are integer values, and for this reason the usual approach would be to fit a GLM with Poisson errors (and the standard log link). mod1 &lt;- glm(noffspring ~ weight, data = fox, family = poisson) After fitting the model it is a good idea to look at the model diagnostics, using autoplot from the ggfortify package. Now we can ask for the Analysis of Variance table for this model. This is exactly the same procedure as for the previous linear models (ANOVA, ANCOVA etc.) except for GLMs one must also specify that you would like to see the results of significance tests using hte test = \"F\" or test = \"Chi\". For Poisson and binomial GLMs the chi-squared test is most appropriate while for gaussian (normal), quasibinomial and quasipoisson models the F test is most appropriate. anova(mod1,test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: noffspring ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 99 166.84 ## weight 1 44.124 98 122.72 3.082e-11 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This summary table tells us that the single explanatory variable (weight) is fantastically important (p-value is very small indeed). We can then ask for the coefficient summary using summary. summary(mod1) ## ## Call: ## glm(formula = noffspring ~ weight, family = poisson, data = fox) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3891 -0.9719 -0.1183 0.5897 2.3426 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.74981 0.31107 -2.410 0.0159 * ## weight 0.63239 0.09502 6.655 2.83e-11 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 166.85 on 99 degrees of freedom ## Residual deviance: 122.72 on 98 degrees of freedom ## AIC: 405.56 ## ## Number of Fisher Scoring iterations: 5 The model coefficients and their standard errors are given on the scale of the linear predictor. They tell us that there is a significant association between the weight of the fox mother and the number of offspring she will produce: larger foxes produce more offspring. Because the coefficients are given on the scale of the linear predictor rather than on the real scale it is useful to plot predictions of the model to visualise the relationship. To do that we must (1) tell the model what to predict from i.e. we must provide a suitable sequence of numbers to predict from using seq, (2) use the predict function to predict values (fit) from the model. We use the argument type = \"response\" to tell the function that we want the preductions on the backtransformed (real) scale rather than on the scale of the linear predictor. We add the argument se.fit = TRUE to tell the function to give us the standard error estimates of the fit. The se.fit values are added or subtracted from the fit to obtain the plus/minus standard errors. We can multiply these by 1.96 to get the 95% confidence intervals of the fitted values. #Vector to predict from newData &lt;- data.frame(weight = seq(1.7,4.4,0.01)) #Predicted values (and SE) predVals &lt;- predict(mod1,newData,type=&quot;response&quot;,se.fit = TRUE) #Create new data for the predicted fit line newData &lt;- newData %&gt;% mutate(noffspring = predVals$fit) %&gt;% mutate(ymin = predVals$fit - 1.96*predVals$se.fit) %&gt;% mutate(ymax = predVals$fit + 1.96*predVals$se.fit) Take a look at this data to make sure it looks OK. head(newData) ## weight noffspring ymin ymax ## 1 1.70 1.384392 0.9649333 1.803850 ## 2 1.71 1.393174 0.9734837 1.812865 ## 3 1.72 1.402013 0.9821017 1.821924 ## 4 1.73 1.410907 0.9907879 1.831026 ## 5 1.74 1.419858 0.9995426 1.840173 ## 6 1.75 1.428865 1.0083663 1.849364 This looks OK. Now we can plot the data and add a the model fit line, and a “ribbon” representing the errors (the 95% confidence interval for the line). So we could summarise this something like this: Methods: I modelled the association between mother’s weight and number of pups produced using a generalised linear model with a log link and Poisson error structure. This is appropriate because the data are count data (number of pups) that are bounded at 0 with increasing variance with increased maternal weight. Results: The GLM showed that maternal weight was significantly associated with the number of pups produced (GLM: Null Deviance = 166.8, Residual Deviance = 122.7, d.f. = 1 and 98, p &lt;0.001). The slope of the relationship was 0.63 (on the log scale). The equation of the best fit line was log(nOffspring) = -0.75 + 0.63\\(\\times\\)MotherWeight (see Figure XXX) 14.1.2 Example: Cancer clusters This data show counts of prostate cancer and distance from a nuclear processing plant. Lets take a look at the data. Let’s first import the data and use summary to examine it by plotting it: First we can see that there are no negative count values. Again, you will notice that the data are formed into horizontal rows of integer response values. There are lots of zero values at all distances, but the biggest cluster (6 cases), is very close to the plant. But is there a relationship between the distance from the nuclear plant and the number of cancers? Let’s fit a Generalised Linear Model to find out. As before will assume that the error is Poisson (that they variance increases directly in proportion to the mean), and we will use the standard log link to ensure that we don’t predict negative values: mod1 &lt;- glm(Cancers ~ Distance, data = cancer, family = poisson) Next, plot the diagnostic plots. These look a bit dodgy, but we’ll stick with it for the moment. Next ask for the Analysis of Variance table. anova(mod1,test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: Cancers ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 93 149.48 ## Distance 1 2.8408 92 146.64 0.0919 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The ANOVA table tells us that there is no significant effect of the Distance variable. In other words a model that includes the Distance term does not explain significantly more variation than the NULL model that includes no terms and instead assumes that variation in cancer incidence is simply caused by random variation. We needn’t go further with this model, but go ahead and plot the model in any case (just for practice). summary(mod1) ## ## Call: ## glm(formula = Cancers ~ Distance, family = poisson, data = cancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5504 -1.3491 -1.1553 0.3877 3.1304 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.186865 0.188728 0.990 0.3221 ## Distance -0.006138 0.003667 -1.674 0.0941 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 149.48 on 93 degrees of freedom ## Residual deviance: 146.64 on 92 degrees of freedom ## AIC: 262.41 ## ## Number of Fisher Scoring iterations: 5 Use the approach from the fox example as guidance to make a plot with a fit line. "],
["extending-use-cases-of-glm.html", "Chapter 15 Extending use cases of GLM 15.1 Logistic regression with binary (0/1) data 15.2 Example: species presence/absence data 15.3 Binomial regression with proportion/ratio data", " Chapter 15 Extending use cases of GLM In the previous chapter we used the case of modelling count data, which is bounded at 0 and takes integer values, to understand how Generalised Linear Models work. In this chapter we extend our understanding by looking at some other type of data. We will look at three types of binomial data: data coded as 0/1 data on number of successes/failures percentage or proportion data 15.1 Logistic regression with binary (0/1) data Sometimes we are confronted with data where the response variable is simply a success or a failure, or a presence or absence. This kind of data is often best-analysed using a Generalised Linear Model with a binomial error structure (strictly speaking the errors are Bernoulli errors, but these are simply one kind of binomial error). This kind of analysis is often called logistic regression. Some examples dead/alive, occupied/empty, healthy/diseased etc. Again: Linear models could lead to unreasonable values (probabilities &gt;1 or &lt;0). Variance of response likely to be n-shaped (small near 0 and 1, large inbetween). Errors are non-normal. Can you think of any other examples of this kind of data? 15.2 Example: species presence/absence data Our example concerns the presence or absence on a set of islands of a particular species of bird. The response variable we are interested in is called “incidence” and a value of 1 means that the bird is present and breeding on the island, while a value of 0 means that the island has no breeding population there. The explanatory variable is the size of the island in \\(km^2\\) and isolation (the distance of the island from the mainland in km). Lets take a look at the data. island &lt;- read.csv(&quot;CourseData/isolation.csv&quot;) names(island) ## [1] &quot;incidence&quot; &quot;area&quot; &quot;isolation&quot; In this case there are two explanatory variables and we are therefore going to fit a multiple regression model with two variables using the GLM framework. model1 &lt;- glm(incidence ~ area+isolation, data = island, family = binomial) summary(model1) ## ## Call: ## glm(formula = incidence ~ area + isolation, family = binomial, ## data = island) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8189 -0.3089 0.0490 0.3635 2.1192 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.6417 2.9218 2.273 0.02302 * ## area 0.5807 0.2478 2.344 0.01909 * ## isolation -1.3719 0.4769 -2.877 0.00401 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 68.029 on 49 degrees of freedom ## Residual deviance: 28.402 on 47 degrees of freedom ## AIC: 34.402 ## ## Number of Fisher Scoring iterations: 6 Looking at coefficients and standard errors (which are on the logit scale) we can see that area has a significant positive effect on the probability of occupation (larger islands are more likely to be occupied) and isolation has a strong negative effect (isolated islands are much less likely to be occupied). To visualise this we can plot the model fits through a scatter plot of the data. It is best to do this separately for each variable so first we must make two separate models, one for each of the variables. modela &lt;- glm(incidence ~ area, data = island, family = binomial) modeli &lt;- glm(incidence ~ isolation, data = island, family = binomial) Then we can plot the two relationships in graphs side-by-side using the same approach as above. In these plots the fitted values (the modelled line) can be interpreted as a probability. par(mfrow=c(1,2)) plot(island$area,island$incidence) newData &lt;- data.frame(area = seq(0,9,0.01)) newData$fit &lt;- predict(modela,newData,type=&quot;response&quot;) lines(newData$area,newData$fit,col=&quot;red&quot;) plot(island$isolation,island$incidence) newData &lt;- data.frame(isolation = seq(0,10,0.01)) newData$fit &lt;- predict(modeli,newData,type=&quot;response&quot;) lines(newData$isolation,newData$fit,col=&quot;red&quot;) 15.3 Binomial regression with proportion/ratio data Another important class of binomial data is proportion or success/failure data. These data are characterised by the fact that we have information on how many times an event occurred and how many times it did not occur. These data include infection rates of diseases, sex ratios, percentage mortality etc. These data are bounded in a similar way to the data above, and are also best-analysed using binomial errors. Can you think of any other examples of this kind of data? 15.3.1 Example: sex ratios In the following example we will look at data from an experiment on sex ratios in an insect species. The experimenter wanted to know whether the population density at which the insect was held had a role in determining the sex ratio. Lets take a look at the data: sexRatio &lt;- read.csv(&quot;CourseData/sexratio.csv&quot;) str(sexRatio) ## &#39;data.frame&#39;: 8 obs. of 3 variables: ## $ density: int 1 4 10 22 55 121 210 444 ## $ females: int 1 3 7 18 22 41 52 79 ## $ males : int 0 1 3 4 33 80 158 365 You can see that the data are integer counts of males and females. In this case, the density is simply the sum of the females and males. Let’s plot the data as proportion males to see if we can see any pattern. First we can need to add a new column with that information: sexRatio$propMale &lt;- sexRatio$males/(sexRatio$males+sexRatio$females) Now we can plot the data. In this case I use the ylim argument to specify that the y-axis should go from 0 to 1 (since this is a proportion). plot(sexRatio$density,sexRatio$propMale,ylab = &quot;Proportion male&quot;,ylim=c(0,1)) Figure 15.1: Sex ratio as a function of density It looks like there is a relationship, and it is definitely non-linear so we are justified in using a GLM for the regression. This regression is different from the last binomial GLM because we are not dealing with a simple 0 or 1 response. Instead we have a number in two catefories (male and female). These are analysed by binding (using the cbind function) the two vectors of male and female counts into a single object that we will use as the response variable. y = cbind(sexRatio$males, sexRatio$females) Lets have a look: y ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 3 ## [3,] 3 7 ## [4,] 4 18 ## [5,] 33 22 ## [6,] 80 41 ## [7,] 158 52 ## [8,] 365 79 You can see that this is simply a matrix with two columns. Now lets fit the model and take a look at the summary output: model &lt;- glm(y ~ density, data = sexRatio,family=binomial) summary(model) ## ## Call: ## glm(formula = y ~ density, family = binomial, data = sexRatio) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4619 -1.2760 -0.9911 0.5742 1.8795 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.0807368 0.1550376 0.521 0.603 ## density 0.0035101 0.0005116 6.862 6.81e-12 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 71.159 on 7 degrees of freedom ## Residual deviance: 22.091 on 6 degrees of freedom ## AIC: 54.618 ## ## Number of Fisher Scoring iterations: 4 We can now plot the data and the model’s fitted values through it: plot(sexRatio$density,sexRatio$propMale,ylab = &quot;Proportion male&quot;,ylim=c(0,1)) newData &lt;- data.frame(density = seq(0,450,1)) newData$fit &lt;- predict(model,newData,type=&quot;response&quot;) lines(newData$density,newData$fit,col=&quot;red&quot;) Hmmm. This doesn’t look so great. The model doesn’t really capture the initial increase in male sex ratio. Let’s see if we can improve it by logging the density variable before fitting. model2 &lt;- glm(y ~ log(density), data = sexRatio,family=binomial) summary(model2) ## ## Call: ## glm(formula = y ~ log(density), family = binomial, data = sexRatio) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9697 -0.3411 0.1499 0.4019 1.0372 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.65927 0.48758 -5.454 4.92e-08 *** ## log(density) 0.69410 0.09056 7.665 1.80e-14 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 71.1593 on 7 degrees of freedom ## Residual deviance: 5.6739 on 6 degrees of freedom ## AIC: 38.201 ## ## Number of Fisher Scoring iterations: 4 There are two ways of easily comparing two GLM models that have the same response variable. Firstly you can look at the residual deviance. Resdual deviance is simply the amount of variation in the response variable that is NOT explained by the model: models with smaller residual deviance are better. You can also look at the AIC (Akaike Information Criterion). The AIC is a summary statistic that tries to capture how well the model fits the data while accounting for how complex the model is. It is too complicated to get into right now except to say that smaller AIC values are better. By both of these methods, the second model (model2) is better than model 1. Lets plot it to confirm that visually. plot(log(sexRatio$density),sexRatio$propMale,ylab = &quot;Proportion male&quot;,ylim=c(0,1)) newData &lt;- data.frame(density = seq(0,400,1)) newData$fit &lt;- predict(model2,newData,type=&quot;response&quot;) lines(log(newData$density),newData$fit,col=&quot;red&quot;) Figure 15.2: Sex ratio as a function of log density Yes, that looks much better. That just goes to show that one should visually inspect the models and consider other ways of fitting the data: there are often ways of improving the fit. In the next session we’ll get some more practice of choosing modelling approaches and interpreting outputs… "],
["power-and-study-design.html", "Chapter 16 Power and study design 16.1 Power analysis by simulation", " Chapter 16 Power and study design This chapter will cover aspects of statistical power and study design. It will first focus on answering questions like “what sample size should I use in my experiment?” and “with this sample size, what difference could I detect?” The chapter will then focus on study designs such as nested or blocked designs, and what this means in terms of analytical approach. 16.1 Power analysis by simulation As you learned in the chapters on t-tests and ANOVA, the detection of a significant difference between treatment groups (if there is one) depends on two things: (1) the actual difference between mean values for the groups (the “signal”) and (2) the amount of variation there is in the groups (the “noise”). When there is a lot of noise it is hard to detect the signal. In most cases we will already have some idea about what to expect when doing a study. Previous work on similar topics, or pilot studies, will have given us an idea of typical values for the response variable, and will give us a ballpark estimate of the amount of variation to expect. This information can be put to use to conduct a power analysis by simulation. The gist of this approach is to draw numbers from appropriate distributions to simulate the experiment before actually carrying out the experiment. For example, consider a planned study on bird song volume (amplitude) in relation to ambient noise. Previous work has shown that the amplitude of the particular species we’re interested in has a mean value of X with a standard distribution of Y. We could simulate an experiment, with a sample size of 20, by drawing from a normal distribution like this rnorm(20,mean = X, sd = Y) We could then figure out what difference we could detect using a t-test (or linear model) by repeating this sampling many times with different values of X using the replicate funtion to repeat the sampling procedure many times. I will illustrate this by using : rnorm(20,mean = 250, sd = 40) ## [1] 187.6944 225.9570 290.5048 218.0409 215.1744 257.6249 ## [7] 205.7226 281.6675 215.2931 253.4033 209.4445 272.1803 ## [13] 270.3865 277.1121 317.4581 255.9634 245.1332 264.9256 ## [19] 300.9877 222.9726 "],
["coming-soon-maybe.html", "Chapter 17 Coming soon (maybe)! 17.1 Multivariate statistics 17.2 Transforming variables 17.3 Non-linear regression 17.4 Nested or blocked study designs", " Chapter 17 Coming soon (maybe)! 17.1 Multivariate statistics This will cover Principal Component Analysis (PCA) which is a handy statistical tool to reduce the complexity of high-dimensional data (i.e. data sets with many columns). 17.2 Transforming variables It is often desirable to transform the response variable in order to linearise the relationship between it and the explanatory variables. Generalised Linear Models (GLMs) with non-normal error distributions do this, but sometimes it is useful to do this for ordinary linear regression. It is also useful to understand the different transformations that are part of a GLM. I will cover common transformations, and include “Tukey’s ladder” of transformations. 17.3 Non-linear regression Most of the statistics shown in this book focusses on modelling linear relationships between response variables (or their transformed values) and explanatory variables. Sometimes it is useful to explicitly model a known functional form and non-linear regression is a way of doing that. 17.4 Nested or blocked study designs Sometimes a study will have a design that has some inherent structure. This chapter covers how to account for this with linear models. "]
]

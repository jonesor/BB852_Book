[
["index.html", "BB852 - Data handling, visualisation and statistics Chapter 1 Preface 1.1 Data wrangling 1.2 Data visualisation 1.3 Statistics 1.4 Data sources 1.5 Acknowledgements 1.6 Schedule", " BB852 - Data handling, visualisation and statistics Owen R. Jones 2020-08-29 Chapter 1 Preface This book has been written to accompany the course, BB852 - Data Handling, Visualisation and Statistics. It is available as a website (https://jonesor.github.io/BB852_Book/) or as a PDF (click the link at the top of book’s website). I recommend to use the website where possible because the formatting is sometimes messy on the PDF, but the PDF is useful if you want a copy for offline use. Note: The book is a “work in progress” and will change during the course. The latest version can always be found at the website, or by downloading it again. Please let me know (jones@biology.sdu.dk) if you spot any errors, or have any suggestions for improvement. The book is divided into three parts: data wrangling, data visualisation and statistics. 1.1 Data wrangling The term data wrangling covers manipulation of data, for example collected from an experiment or observational study, from its raw form to a form that is ready for analysis, or summarised into tables. It includes reshaping, transforming, filtering and augmenting from other data. This book covers these processes in R mainly using the tools from the dplyr package. 1.2 Data visualisation Graphing data is a crucial analytical step that can both highlight problems with the data (e.g. errors and outliers) and can inform on appropriate statistical approaches to take. This book covers the use of ggplot2 to make high quality, publication-ready plots. 1.3 Statistics Statistics is a huge field and this book does not attempt to cover more than a small fraction of it. Instead it focusses on (ordinary) linear models and generalised linear models. In a nutshell, linear models model the effects of explanatory variables on a continuous response variable with a gaussian (normal) error distribution while generalised linear models (GLMs) offer a more flexible approach that allows the response variable to have non-normal error distributions. This flexibility allows the more-appropriate modelling of phenomena including integer counts (e.g. number of individuals, or species, or events), binary (0/1) data (e.g. survived/died) or binomial count data (e.g. counts of successess and failures). It is important to realise that most commonly-used statistical methods including t-tests, ANOVA, ANCOVA, n-way ANOVA, and of course linear and multiple regression are all special cases of linear models. My general approach with communicating these methods and ideas is to teach using examples. Therefore, the bulk of the text here consists of walk-throughs of manipulating, plotting and analysing real data. For the statistics section I focus on communicating the “gist” of the underlying mathematical machinery rather than the mathematical details. If you find yourself interested in these details then there are more specialist textbooks available. This book accompanies the course lectures. The general idea is that there will be a lecture, followed by computer work where you work through the examples in the relevant chapter of this book. At the end of most chapters there are also exercises to test your new skills. It is very important that you do these to gradually build up your skill level and confidence. 1.4 Data sources This book uses numerous data sets in examples, most of which are real datasets obtained from published works, or collected by me. The data sets can be found at the following link: https://www.dropbox.com/sh/z8iv9fl9l00bm0w/AAD1WjFwcrr1ERugpunp_YH-a?dl=0 1.5 Acknowledgements These materials are inspired by the excellent textbook, “Getting Started With R”1, which is the recommended textbook for BB852, and by materials for the Sheffield University course “AP 240 - Data Analysis And Statistics With R” ([https://dzchilds.github.io/stats-for-bio/]). 1.6 Schedule This is the schedule for the course. Please note that it is liable to change (possibly at short notice). If you find a mismatch between this schedule and the official one2, then it is the official one that is correct. The columns, GSWR and Course Book, refer to the relevant chapters in the recommended text book (“Getting Started With R”) and this course book, respectively. You should aim to read and work through these chapters as the course proceeds. PartDateSessionTypeTopicGSWRCourseBookIntroSep 2, Wed1LectureOverview and Philosophy2LectureHypotheses and questionsSep 4, Fri3LectureGetting acquainted with R1 &amp; 21 &amp; 24PracticalAn R refresher1 &amp; 21 &amp; 2Data handlingSep 9, Wed5LectureData wrangling336PracticalUsing dplyr33Sep 11, Fri7LectureData management33 &amp; 48PracticalData wrangling in R33 &amp; 4Sep 15, Tue9PracticalData wrangling in R33 &amp; 410PracticalData wrangling in R33 &amp; 4Data visualistionSep 18, Fri11LectureData visualisation with ggplot2.4512PracticalUsing ggplot45Sep 22, Tue13LectureSummary statistics, distributions and probability4614PracticalProbability and distributions in R46Sep 24, Thu15PracticalPimping your plots8716PracticalPimping your plots87StatisticsSep 29, Tue17LectureRandomisation tests818PracticalPractice randomisation tests8Oct 1, Thu19PracticalPractice randomisation tests820PracticalPractice randomisation tests8Oct 7, Wed21Lecturet-tests5922Practicalt-tests in R9Oct 8, Thu23LectureANOVA51024PracticalANOVA in R10Oct 26, Mon25LectureLinear Regression51126PracticalLinear Regression in R11Oct 29, Thu27LectureANCOVA and multiple regression61228PracticalMultiple regression in R12Nov 3, Tue29LectureEvaluating models1230PracticalTwo-way anova in R13Nov 6, Fri31LectureGLMs with count data71432PracticalPoisson GLMs in R14Nov 11, Wed33LectureGLMs with binomial data71534PracticalBinomial GLMs in R15Nov 13, Fri35LectureStatistical power1636PracticalPower analysis by simulation16Mini ProjectNov 17, Tue37PracticalProject work38PracticalProject workNov 19, Thu39PracticalProject work40PracticalProject workNov 25, Wed41PracticalProject work42PracticalProject workWrap-upNov 26, Thu43LectureCourse finale44LectureCourse finale Beckerman, Childs &amp; Petchey (2017) Getting Started With R. Oxford University Press (2nd edition)↩︎ https://mitsdu.sdu.dk/skema/activity/N110040101/e20↩︎ "],
["getting-acquainted-with-r.html", "Chapter 2 Getting acquainted with R 2.1 Getting started with R 2.2 Getting help 2.3 R as a fancy calculator 2.4 Objects in R 2.5 Missing values, infinity and “non-numbers” 2.6 Basic information about objects 2.7 Data frames 2.8 Organising your work 2.9 Inspecting the data 2.10 “Classes” in R 2.11 Tables and summary statistics 2.12 Plotting data 2.13 R Packages 2.14 Exercise: Californian bird diversity", " Chapter 2 Getting acquainted with R In this course we will be learning how manipulate, visualise and analyse data statistically using R. R is a programming language for data analysis and statistics. It is free and very widely used. One of its strengths is its very wide user base which means that there are hundreds of contributed packages for every conceivable type of analysis. The aim of these introductory sections is to give a basic introduction to the programming language as a tool for importing, manipulating, and exploring data. In later sections we will learn more about statistical analysis. Before proceeding you will need to ensure you have a recent version of R installed on your computer (the current version is 4.0.3). Do this: Check your R version, and/or install R on your own computer now. In this course we will not be using R on its own. Instead, we will be using it with RStudio. R and RStudio are not the same thing. It is possible to run R without RStudio, but RStudio will not work if R is not installed. So what is RStudio? RStudio, essentially, is a helpful piece of software that makes R easier to use. The three most useful features are: The R Console - this is where R runs inside RStudio. We can work directly with R by typing commands into this “console”. It is also where outputs (results) from R are printed to the screen. The Code Editor - this is where you can write R programs (called “scripts”)“, which are a set of commands/instructions in the R language saved to a text file. It is much easier to work with scripts using RStudio than with ordinary text editors like Notepad. For example, it colour codes the text to make it easier to read and it will”auto-complete\" some text to speed up your work. Useful “point-and-click” tools - RStudio can help with tasks like importing data, managing files, reading help files, and managing/installing packages. Doing these things is trickier in just R: RStudio just makes things easier! You should do your coding from within RStudio. You can download RStudio Desktop from https://rstudio.com/products/rstudio/download/. Select the correct version for your computer (Mac/Windows) and follow the usual instructions. Do this: Install RStudio Desktop on your computer. 2.1 Getting started with R In RStudio, create a new “R Script” file. Scripts are essentially programs that can be saved to allow you to return to your work in the future. They also make debugging of errors much easier. You can use the menu to do create a new R Script (File &gt; New File &gt; R Script), but there’s also a keyboard shortcut (Windows: Ctrl+Shift+N; Mac: Cmd+Shift+N). If you save (Windows: Ctrl+S; Mac: Cmd+S), you will be prompted for a file name. Make sure it has the suffix “.R” which denotes an R script file. Save the file in a folder with a memorable name (e.g. BB852_Work). When you double click on this file in future, it should automatically open in RStudio (if it doesn’t you should be able to right-click and select Open with...). In RStudio you can execute commands using the “run” icon at the top of the script window, or by selecting the text and typing the shortcut Ctrl+Enter (Windows) or Cmd+Enter (Mac). Another helpful feature of RStudio is that it will colour-code the syntax that you type, making it easier to read and debug. Note that the colours you see may be different from the ones shown in this handout. You can customise the look of RStudio using by clicking Tools → Options menu on Windows or RStudio → Preferences on a Mac. I will point out some of this in the lecture, or you can ask me to show you. Over the next few pages I will introduce the basics of the R programming language. Try typing them into the scripting window (top left) in RStudio and ensuring that you understand what the commands are doing. It is impossible to “break” R by typing the wrong command so I encourage you to experiment and explore the R language I introduce to you here as much as possible - it really is the best way to learn! The “look” of RStudio can be modified by changing the Preferences (RStudio → Preferences → Appearance). Also, there are some useful keyboard shortcuts that are worth learning, to run code, save files etc. without needing to point-and-click (Tools → Keyboard Shortcuts Help). 2.2 Getting help R features a wealth of commands, which are more properly termed functions. You will learn many of these over the next few weeks. Functions often feature a several options which are specified with arguments. For example, the function sum, has the argument ..., which is intended to be one or more vectors of numbers (see below), and the argument na.rm, which is a logical argument specifying whether or not missing values should be removed or not. Usually the arguments have default options which are used you choose not to specify them. In addition, you don’t necessarily need to fully-specify the argument if they are specified in the correct order. You can get help on R functions from within R/RStudio with the ? and help.search commands. ? requires that you know the function name while help.search will search all the available help files for a particular word or phrase. ?? is a synonym for help.search: ?rep help.search(&quot;bar plot&quot;) ??&quot;bar plot&quot; In RStudio, the help results will appear in the lower right hand area. 2.3 R as a fancy calculator R features the usual arithmetic operations for addition, subtraction, division, multiplication: 4+3 ## [1] 7 9-12 ## [1] -3 6/3 ## [1] 2 7*3 ## [1] 21 (2*7)+2-0.4 ## [1] 15.6 R also has commands for square root (sqrt), raising to powers (^), taking the absolute value (abs), and rounding (round), natural log (log), anti-log (exp), log to base-10 (log10): sqrt(945) ## [1] 30.74085 3^5 ## [1] 243 abs(-23.4) ## [1] 23.4 round(2.35425,digits=2) ## [1] 2.35 log(1.2) ## [1] 0.1823216 exp(1) ## [1] 2.718282 log10(6) ## [1] 0.7781513 Another thing you can do is evaluate TRUE/FALSE conditions: 3&lt;10 ## [1] TRUE 5&gt;7 ## [1] FALSE 5==5 ## [1] TRUE 6!=5 ## [1] TRUE 3 %in% c(1,2,3,4,5) ## [1] TRUE 6 %in% c(1,2,3,4,5) ## [1] FALSE 2.4 Objects in R R is an object oriented programming language. This means that it represents concepts as objects that have data fields describing the object. These objects can be manipulated by functions. Objects can include data, but also models. Don’t worry about these distinctions too much for now - all will become clear as you proceed! Objects are assigned names in R like this. The “&lt;-” command is pronounced “gets” so I would pronounce the following as “x gets four”: x &lt;- 4 To look at any object (function or data), just type its name. x ## [1] 4 The main data object types in R are: vectors, data frames, lists and matrices. We will focus on the first two of these during this course. A vector is simply a series of data (e.g. the sequence 1, 2, 3, 4, 5 is a vector, so is the non-numeric sequence Male, Female, Female, Male, Male ). Each item in a vector is called an element. Therefore, both of these examples contain 5 elements. There are several ways to create vectors in R. For example, you can make vectors of integers using the colon (:) function (e.g. 1:5), or vectors of any kind of variable using the c function. c stands for concatenate, which means to join (things) together in a chain or series. Other convenient functions for making vectors are seq, which builds a sequence of numbers according to some rules, and rep which builds a vector by repeating elements a specified number of times. Try the following: A &lt;- 1:5 B &lt;- c(1,3,6,1,7,9) C &lt;- seq(1,12,2) D &lt;- seq(1,5,0.1) E &lt;- rep(c(&quot;Male&quot;,&quot;Female&quot;),each = 3) G &lt;- rep(c(&quot;Male&quot;,&quot;Female&quot;),c(2,4)) Try modifying the commands to make sure you know what the commands are doing. #Manipulating objects Objects can be manipulated (just like in real life). In R, we use functions to manipulate objects. For example, we can use the basic arithmetic functions (*, +, /,-) on a vector: B ## [1] 1 3 6 1 7 9 B*3 ## [1] 3 9 18 3 21 27 B-2 ## [1] -1 1 4 -1 5 7 You can concatenate entire vectors together using the c function. E.g. concatenating the vectors A and B from above: c(A,B) ## [1] 1 2 3 4 5 1 3 6 1 7 9 Other manipulations are also done “element-by-element”. For example, here we multiply the first element of B by 1, the second by 2, the 3rd by 3 and so on…: B * c(1,2,3,4,5,6) ## [1] 1 6 18 4 35 54 If the length of the vectors match, we can also multiply (or add/subtract/divide etc.) multiple vectors: A / B ## [1] 1.0000000 0.6666667 0.5000000 4.0000000 0.7142857 0.1111111 2.5 Missing values, infinity and “non-numbers” By convention, missing values in R are coded by the value “NA”. The way that particular functions handle missing values varies: sometimes the NA values are stripped out of the data, other times the function may fail. For example, if we asked for the mean value of a vector of numbers with an NA value, it will fail: mean(c(1,3,6,1,7,9,NA)) ## [1] NA In this case you need to specify that any NA values should be removed before calculating the mean: mean(c(1,3,6,1,7,9,NA),na.rm=TRUE) ## [1] 4.5 Calculations can sometimes lead to answers that are plus, or minus, infinity. These values are represented in R by Inf or -Inf: 5/0 ## [1] Inf -4/0 ## [1] -Inf Other calculations lead to answers that are not numbers, and these are represented by NaN in R: 0/0 ## [1] NaN Inf-Inf ## [1] NaN 2.6 Basic information about objects You can obtain information about most objects using the summary function: summary(B) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 1.50 4.50 4.50 6.75 9.00 The functions max, min, range, and length are also useful: max(B) ## [1] 9 min(B) ## [1] 1 range(B) ## [1] 1 9 length(B) ## [1] 6 2.7 Data frames Data frames are the usual way of storing data in R. It is more-or-less the same as a worksheet in Excel. A data frame is usually made up of a number of vectors (of the same length) bound together in a single object. You can make a data frame by binding together vectors, or you can import them from outside R. This example shows the creation of a data frame in R, from 3 vectors: height &lt;- c(173, 145, 187, 155, 179, 133) sex &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;) age &lt;- c(17, 22, 32, 20, 27, 30) mydata &lt;- data.frame(height = height, age = age, sex = sex) mydata ## height age sex ## 1 173 17 Male ## 2 145 22 Female ## 3 187 32 Male ## 4 155 20 Female ## 5 179 27 Male ## 6 133 30 Female Data frames can be summarised using the summary function (or the str function, which gives you a different view of the same data): summary(mydata) ## height age sex ## Min. :133.0 Min. :17.00 Length:6 ## 1st Qu.:147.5 1st Qu.:20.50 Class :character ## Median :164.0 Median :24.50 Mode :character ## Mean :162.0 Mean :24.67 ## 3rd Qu.:177.5 3rd Qu.:29.25 ## Max. :187.0 Max. :32.00 str(mydata) ## &#39;data.frame&#39;: 6 obs. of 3 variables: ## $ height: num 173 145 187 155 179 133 ## $ age : num 17 22 32 20 27 30 ## $ sex : chr &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; ... Data frames can be subsetted using the square brackets [], or subset functions. With the square brackets, the first number specifies the row number, while the second number specifies the column number: mydata[1,] ## height age sex ## 1 173 17 Male mydata[,2] ## [1] 17 22 32 20 27 30 mydata[1,2] ## [1] 17 subset(mydata,sex == &quot;Female&quot;) ## height age sex ## 2 145 22 Female ## 4 155 20 Female ## 6 133 30 Female 2.8 Organising your work It would be incredibly tedious to enter real data into R by typing it in! Thankfully, R can import data from a several data formats, and it understands the file structure of your computer. Thus, you can use spreadsheet software (like Excel) to enter and store your data, and you can organise your project work in a sensible way in folders (sometimes called directories) on your computer. The most commonly used data format is comma separated value (CSV) so I will use that. You can also import from Excel, but the data must be formatted in a particular way to enable this (I’ll cover this in a later class). For this course, I suggest that you make a folder somewhere on your computer called “IntroToR”. We will use this as the working directory for the remainder of the session. In RStudio you can set the working directory by clicking through the menu items Session → Set Working Directory → Choose Directory. You can also using the setwd function to do this, if you know where your files are stored (the file path). File paths in Windows and Mac computers are expressed differently. Apple systems use the forward-slash (/) to separate folders whereas Windows can use the forward-slash (/) or double-backslash (\\). In windows you also need to define the drive (e.g. C:). So, to set the working directory in Apple OSX you would use something like this (obviously, you need to put your path!): setwd(&quot;/Users/orj/Desktop/IntroToR&quot;) While in Windows the equivalent command would be something like this (both of the following should work): setwd(&quot;C:\\\\Users\\\\orj\\\\Desktop\\\\IntroToR&quot;) setwd(&quot;C:/Users/orj/Desktop/IntroToR&quot;) Typing the path in can be annoying but there are ways to speed it up. In Windows you can copy paths from the Windows Explorer location/address bar, or you can hold down the Shift key as you right-click the file, and then choose Copy As Path. On a Mac you can copy file paths from Finder: Select your file/folder, Right click, Press the option key (on my keyboard this is the alt key) and click “Copy X as Pathname” I can check what the current working directory is using the getwd function: getwd() It is good practice to keep your files well-organised. I recommend that you create a folder in your working directory called CourseData (or similar). Store your data files in this folder. On the Blackboard site for BB852 I have put a link to a Dropbox folder containing data files for use in the course. In there you will find a file called “carnivora.csv”. Download this to your new CourseData folder. You can now import this file into R using the read.csv function. The specification of the argument header = TRUE signifies that the first row of our CSV file contains the column names. Note that your file path will be different to mine: carni &lt;- read.csv(&quot;CourseData/carnivora.csv&quot;, header = TRUE, stringsAsFactors = TRUE) The stringsAsFactors argument tells R to treat text-type data (technically known as “character strings”) as a special kind of data called factors. Essentially, factors are “categorical data” where the data can take a limited number of discrete values. For example, “treatmentA”, “treatmentB”, “treatmentC”. Although this may seem a little esoteric right now, it is important to ensure that your data is recognised by R in the correct way. In most cases, your text-type data will be factor data, so it is usually safe to set stringsAsFactors = TRUE. Tip: RStudio also has a point-and-click “Wizard” to help import data. Look for “Import Dataset” in the top-right pane. 2.9 Inspecting the data We can get some basic information on your imported data (e.g. the carni data frame) using the summary function, but also the dim and nrow/ncol functions: summary(carni) dim(carni) ## [1] 112 17 nrow(carni) ## [1] 112 ncol(carni) ## [1] 17 We can find the names of the columns of a data frame with the names function: names(carni) ## [1] &quot;Order&quot; &quot;SuperFamily&quot; &quot;Family&quot; &quot;Genus&quot; &quot;Species&quot; &quot;FW&quot; &quot;SW&quot; &quot;FB&quot; &quot;SB&quot; &quot;LS&quot; ## [11] &quot;GL&quot; &quot;BW&quot; &quot;WA&quot; &quot;AI&quot; &quot;LY&quot; &quot;AM&quot; &quot;IB&quot; The first few columns are to do with the taxonomic placement of the species (Order, SuperFamily, Family, Genus and Species). There then follow several columns of life history variables: FW = Female body weight (kg), SW = Average body weight of adult male and adult female (kg), FB = Female brain weight (g), SB = Average brain weight of adult male and adult female (g), LS = Litter size, GL = Gestation length (days), BW = Birth weight (g), WA = Weaning age (days), AI = Age of independence (days), LY = Longevity (months), AM = Age of sexual maturity (days), IB = Inter-birth interval (months). You can refer to the sub-parts of a data.frame (the columns) using the $ syntax: summary(carni$FW) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.050 1.245 3.400 18.099 10.363 320.000 2.10 “Classes” in R I have already mentioned the different object types in R (e.g. vectors and data frames). The object types are technically known as “classes”. You can find out what “class” an object is by using the class function: class(carni) ## [1] &quot;data.frame&quot; In this case, the data frame is, unsurprisingly, of class “data.frame”. However, the vectors that compose the data frame also have classes. There are several classes of vectors including “integer” (whole numbers), “numeric” (real numbers), “factor” (categorical variables) and “logical” (true/false values). I expect you have heard of the first two data types, but “factor” might be puzzling. Factors are defined as variables which can take on a limited number of different values. They are often referred to as categorical variables. For example, in the carnivore dataset, the taxonomic variables are factors. The different values that a factor can take are known as levels and you can check on the levels of a vector with the levels function. class(carni$Family) ## [1] &quot;factor&quot; levels(carni$Family) ## [1] &quot;Ailuridae&quot; &quot;Canidae&quot; &quot;Felidae&quot; &quot;Hyaenidae&quot; &quot;Mustelidae&quot; &quot;Procyonidae&quot; &quot;Ursidae&quot; &quot;Viverridae&quot; 2.11 Tables and summary statistics For vectors of class “factor” you can use the table function to give the counts for each level: table(carni$Family) ## ## Ailuridae Canidae Felidae Hyaenidae Mustelidae Procyonidae Ursidae Viverridae ## 1 18 19 4 30 4 4 32 You can use the function tapply (“table apply”), to get more complex summary information. For example, I could ask what the mean female weight (FW) is in each of the families using the argument mean: tapply(carni$FW, carni$Family, mean) ## Ailuridae Canidae Felidae Hyaenidae Mustelidae Procyonidae Ursidae Viverridae ## 120.000000 9.050000 31.432105 33.540000 3.989000 3.642500 198.250000 2.672813 2.12 Plotting data Basic plots can be made using the plot command. For example, let’s have a look at the relationship between log gestation length and log female body weight (see Figure , below): plot(log(carni$FW), log(carni$GL)) Figure 2.1: A simple scatter plot 2.13 R Packages R packages are collections of software that add capabilities to “base R”. In this course we use several packages including dplyr, which adds functionality for manipulating data, ggplot2 which helps us make pretty plots and magrittr which adds tools to allow more “elegant” programming. Packages need to be installed using install.packages command before they can be used. You only need to install them once. install.packages(&quot;dplyr&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;magrittr&quot;) To use the packages you need to load them with the library command, like this: library(dplyr) library(ggplot2) library(magrittr) We will be using these packages a lot, and you will need to remember to load them every session. It is therefore useful to add those library commands to the top of every script you write. 2.14 Exercise: Californian bird diversity In the 1950s-1970s there was rapid growth in the number of houses being built in California, with suburbs sprawling out into the new sites in the countryside. What effect would this have on local bird communities? Surveys on bird abundances were carried out in several locations near Oakland, California.3 The locations were of different ages, enabling us to investigate what changes might happen through time. Although there were no surveys before the developments, we can regard the bird abundance in the very youngest housing developments as the baseline pre-development condition. Think about what you might expect to happen to bird species diversity through time in a newly developing suburb. 2.14.1 The data The relevant data file is called suburbanBirds.csv. This file contains data on bird abundances surveyed in 1975. The columns of the data are Name (name of the suburb), Year (the year that the suburb was built), HabitatIndex (an index of habitat quality, related to tree height, garden maturity etc.), nIndividuals (number of individual birds seen in a standard survey) and nSpecies (number of species seen in a standard survey). Additional surveys found an average species richness of 3.5 in nearby undisturbed habitats of grassland savanna. 2.14.2 Try the following First import the data. Check that the columns look as they should (use summary or str functions). What is the mean, minimum, and maximum number of species seen? (there is more than one way to do this) How old are the youngest and oldest suburbs? (hint: the survey was carried out in 1975, do the math!) Plot the relationship between Year and nSpecies as a scatter plot using base-R graphics (using the plot function). The pattern might be easier to see if you could replace YearBuilt with suburb age. Create a new vector in your data frame for this variable (e.g. df$Age &lt;- 1975 - Year)). Re-plot your results. What do the data show? What might be the mechanisms for the patterns you see? Do they match your expectations? Export your plots and paste them into a Word Document. If you get this far, try plotting the other variables in the dataset. Vale, T. R., &amp; Vale, G. R. (1976). Suburban bird populations in west-central California. Journal of Biogeography, 157–165.↩︎ "],
["data-wrangling-with-dplyr.html", "Chapter 3 Data wrangling with dplyr 3.1 select 3.2 filter 3.3 arrange 3.4 summarise and group_by 3.5 Using pipes, saving data. 3.6 Exercise: Wrangling the Amniote Life History Database", " Chapter 3 Data wrangling with dplyr This chapter focuses on using the package dplyr, which is designed to make working with data in R easier. The package has several key “workhorse” functions, sometimes called verbs. These are: filter, select, mutate, arrange and summarise. I covered these in the lecture, and they are also discussed in the textbook. This chapter guides you through worked examples to illustrate their use. We will also be using pipes from the magrittr package. These are implemented using the command %&gt;%. library(&quot;dplyr&quot;) library(&quot;magrittr&quot;) To get to know dplyr and its functions we’ll use a data set collected from the university campus at University of Southern Denmark (SDU) The SDU bird project follows the fate of mainly great tits (musvit) and blue tits (blåmejse) in about 100 nest boxes in the woods around the main SDU campus. We will address two questions concerning clutch size (the number of eggs laid into the nest) - How does clutch size differ between blue tits and great tits? How does average clutch size vary among years? To answer these questions we need to calculate the average clutch size (number of eggs) for each nest in each year. The data are in a file called (sduBirds.csv) and are raw data collected while visiting the nests. The data will need to be processed to answer those questions. Let’s import the data and take a look at it. Make sure your data looks OK before moving on. You should first set up your working directory (e.g. a folder for the course, with a sub-folder for course data etc.), and set it (with setwd)). See the earlier material for how to do this, or ask for help. df &lt;- read.csv(&quot;CourseData/sduBirds.csv&quot;) str(df) ## &#39;data.frame&#39;: 9357 obs. of 15 variables: ## $ Timestamp : chr &quot;2013-05-14&quot; &quot;2013-05-03&quot; &quot;2013-06-25&quot; &quot;2013-06-18&quot; ... ## $ Year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ Day : int 134 123 176 169 112 112 116 183 143 107 ... ## $ boxNumber : int 1 1 1 1 1 1 1 1 1 1 ... ## $ species : chr &quot;BT&quot; &quot;BT&quot; &quot;BT&quot; &quot;BT&quot; ... ## $ stage : chr &quot;NL&quot; &quot;NL&quot; &quot;NE&quot; &quot;NE&quot; ... ## $ nEggs : int 12 8 0 0 0 0 0 0 NA 0 ... ## $ nLiveChicks: int 0 0 0 0 0 0 0 0 0 0 ... ## $ nDeadChicks: int 0 0 0 0 0 0 0 0 0 0 ... ## $ eggStatus : chr &quot;WA&quot; &quot;CO, CV&quot; NA NA ... ## $ chickStatus: chr NA NA NA NA ... ## $ adultStatus: chr &quot;FN&quot; &quot;FN&quot; NA NA ... ## $ finalStatus: chr NA NA &quot;NE&quot; &quot;NE&quot; ... ## $ Comments : chr NA &quot;MA&quot; NA NA ... ## $ observerID : chr &quot;AMK&quot; &quot;AMK&quot; &quot;AMK&quot; &quot;AMK&quot; ... 3.1 select From the str summary (above) you can see that there are many columns in the data set and we only need some of them. Let’s select only the columns that we need for our calculations to make things a bit easier to handle. We need the species, Year, Day, boxNumber and nEggs: df &lt;- select(df, species, Year, Day, boxNumber, nEggs) head(df) ## species Year Day boxNumber nEggs ## 1 BT 2013 134 1 12 ## 2 BT 2013 123 1 8 ## 3 BT 2013 176 1 0 ## 4 BT 2013 169 1 0 ## 5 BT 2013 112 1 0 ## 6 BT 2013 112 1 0 The output of head shows you the first few rows of the data set. You can see that each row represents a visit of a researcher to a particular nest. The researcher records the bird species if it is known (GT = Great tit, BT = Blue tit, NH = Nuthatch etc.), and then records the number of eggs, number of chicks, activity of the adults and so on. We need to convert this huge dataset into one which contains clutch size for each nest, for each year of the study. The information given by str (above) shows that there are data on species other than our target species. We are only interested in the great tits and blue tits so we can first filter the others out using the species variable. We can check what the make up of this part of the data is using the table function which will count up all of the entries. table(df$species) ## ## BT GT MT NH WR ## 992 4612 73 31 5 3.2 filter Now let’s filter this data and double check that this has worked: df &lt;- filter(df,species %in% c(&quot;GT&quot;,&quot;BT&quot;)) table(df$species) ## ## BT GT ## 992 4612 You will notice that all the levels of the variable are retained. This is not a problem, and can usually be ignored. You can also tidy this up using the droplevels function, which removes all unused factor levels. df &lt;- droplevels(df) table(df$species) ## ## BT GT ## 992 4612 3.3 arrange Recall that the data are records of visits to each nest a few times per week. To ensure that the data are in time order I can first arrange by first Year and then Day. To illustrate this we can make a temporary data set (called temp) to look at a particular nest in a particular year to get a record of the progress for that particular nest, and then plot it (this is an ugly plot and we will learn how to make beautiful ones soon): df &lt;- arrange(df,Year,Day) temp &lt;- filter(df,boxNumber == 1,Year == 2014) max(temp$nEggs) # get the max value ## [1] 12 plot(temp$Day,temp$nEggs,type=&quot;b&quot;) Eggs are usually laid one per day, and the clutch size is the maximum number of eggs reached for each nest box. In this case, the clutch size is 12 eggs. The rapid decline in number of eggs after this peak value shows when the eggs have hatched and the researcher finds chicks instead of eggs! 3.4 summarise and group_by The next part is the crucial part of our investigation. We need to get the maximum number of eggs seen at each nest. Of course we could repeatedly use filter, followed by max, for each nest-year combination but this would be incredibly tedious. Instead, we will use the dplyr function, summarise, to do this by asking for the maximum value of nEggs. To make this work we need to first use the group_by function tell R to group the data by the variables we are interested in. If we don’t do this we just get the overall maximum. We can ungroup the data using the ungroup function. Because there are missing data (NA values) we need to specify na.rm = TRUE in the argument. So first, let’s get the max per species, just to illustrate how this works: df &lt;- group_by(df,species) summarise(df,clutchSize = max(nEggs,na.rm= TRUE)) ## # A tibble: 2 x 2 ## species clutchSize ## &lt;chr&gt; &lt;int&gt; ## 1 BT 14 ## 2 GT 14 We can see how the data are grouped by asking for a summary: summary(df) ## species Year Day boxNumber nEggs ## Length:5604 Min. :2013 Min. : 60.0 Min. : 1.00 Min. : 0.000 ## Class :character 1st Qu.:2014 1st Qu.:116.0 1st Qu.: 29.75 1st Qu.: 0.000 ## Mode :character Median :2014 Median :133.0 Median : 57.00 Median : 0.000 ## Mean :2015 Mean :133.2 Mean : 55.83 Mean : 2.326 ## 3rd Qu.:2017 3rd Qu.:148.0 3rd Qu.: 85.00 3rd Qu.: 4.000 ## Max. :2019 Max. :212.0 Max. :101.00 Max. :14.000 ## NA&#39;s :613 We can ungroup the data again like this: df &lt;- ungroup(df) So both species lay the same maximum number of eggs, but maybe this is just caused by outliers for one of the species. We’ll need to dig deeper. How can we calculate the average? We cannot simply ask for the mean because the data run through time following the development in each nest. We need to calculate the maximum nEggs for each nest, and then calculate the average of those. We can do this in two steps. We first calculate the clutch size for each box for each species in each year: df &lt;- group_by(df, species, Year, boxNumber) df &lt;- summarise(df, clutchSize = max(nEggs,na.rm= TRUE)) Let’s first look at all the clutch size data: hist(df$clutchSize) You can see here that there are a lot of zero values. This is because nests were recorded even if they did not attempt to lay eggs. We should remove these from our data using filter again: df &lt;- filter(df, clutchSize &gt; 0) hist(df$clutchSize) That looks better. Now we can plot them again but this time split apart the species (again - this plot is ugly and we’ll learn to plot nicer ones soon). plot(as.factor(df$species),df$clutchSize) From these distributions it looks like the average clutch size is greater in the blue tit. We can use summarise to calculate the means. df &lt;- group_by(df,species) summarise(df, mean = mean(clutchSize),sd = sd(clutchSize)) ## # A tibble: 2 x 3 ## species mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BT 9.64 2.64 ## 2 GT 7.89 2.19 Lets now turn to the other question - how does the clutch size vary with year? df &lt;- group_by(df,species,Year) df2 &lt;- summarise(df, meanClutchSize = mean(clutchSize)) head(df2) ## # A tibble: 6 x 3 ## # Groups: species [1] ## species Year meanClutchSize ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BT 2013 8.33 ## 2 BT 2014 8.94 ## 3 BT 2016 8.86 ## 4 BT 2017 9.2 ## 5 BT 2018 10.1 ## 6 BT 2019 11 We can plot this by first making a plot for blue tits, and then adding the points for great tits. I have used the pch argument to use filled circles for the great tits: plot(df2$Year[df2$species == &quot;BT&quot;],df2$meanClutchSize[df2$species == &quot;BT&quot;],type=&quot;b&quot;, ylim=c(0,12),xlab=&quot;Year&quot;,ylab=&quot;Clutch Size&quot;) points(df2$Year[df2$species == &quot;GT&quot;],df2$meanClutchSize[df2$species == &quot;GT&quot;], pch = 16, type=&quot;b&quot;) So it looks like the clutch size varies a fair amount from year to year, but that generally blue tits have large clutch sizes than great tits. 3.5 Using pipes, saving data. I have walked you through a step-by-step data manipulation. During that process you made made (and replaced) new data sets at each step. In practice this can be done more smoothly using pipes (%&gt;%) to pass the result of one function into the next, and the next, and the next… You’ll get some more practice with this as we go on. Below I show how do do this to create a clutch size data set from the raw data (note that your file path will differ from mine): #Import and process data using pipes SDUClutchSize &lt;- read.csv(&quot;CourseData/sduBirds.csv&quot;) %&gt;% filter(species %in% c(&quot;GT&quot;,&quot;BT&quot;)) %&gt;% #include only GT and BT droplevels() %&gt;% #drop unwanted factor levels select(species, Year, Day, boxNumber, nEggs) %&gt;% #select columns needed group_by(species, Year, boxNumber) %&gt;% #group data summarise(clutchSize = max(nEggs,na.rm=TRUE)) %&gt;% #calculate clutch size (max eggs) filter(clutchSize &gt; 0) head(SDUClutchSize) ## # A tibble: 6 x 4 ## # Groups: species, Year [1] ## species Year boxNumber clutchSize ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BT 2013 1 12 ## 2 BT 2013 5 8 ## 3 BT 2013 27 10 ## 4 BT 2013 31 6 ## 5 BT 2013 35 10 ## 6 BT 2013 37 8 You can save this out using the write.csv function. You will need to set the argument row.names = FALSE to stop the data including row names. write.csv(x = SDUClutchSize,file = &quot;CourseData/SDUClutchSize.csv&quot;,row.names = FALSE) 3.6 Exercise: Wrangling the Amniote Life History Database In this exercise the aim is to use the “Amniote Life History Database”4 to investigate some questions about life history evolution. The questions are: (1) what are the records and typical life spans in different taxonomic classes? [what is the longest, shortest and median life span in birds, mammals and reptiles?] (2) is there a positive relationship between body mass and life span? [do big species live longer than small ones?]; (3) is there a trade-off between reproductive effort and life span? [do species that reproduce a lot have short lives, so there is a negative relationship between reproduction and life span?]; (4) is this trade-off universal across all Classes? [does the trade-off exist in birds, reptiles and amphibians?] The database is in a file called Amniote_Database_Aug_2015.csv in the course data folder. The missing values (which are normally coded as NA in R) are coded as “-999”. The easiest way to take care of this is to specify this when we import the data using the na.strings argument of the read.csv function. Thus we can import the data like this: amniote &lt;- read.csv(&quot;CourseData/Amniote_Database_Aug_2015.csv&quot;,na.strings = &quot;-999&quot;) Let’s make a start… When you have imported the data, use dim to check the dimensions of the whole data frame (you should see that there are 36 columns and 21322 rows). Use names to look at the names of all columns in the data in amniote. We are interested in longevity (lifespan) and body size and reproductive effort and how this might vary depending on the taxonomy (specifically, with Class). Use select to pick relevant columns of the dataset and discard the others. Call the new data frame x. The relevant columns are the taxonomic variables (class, species) and longevity_y, litter_or_clutch_size_n, litters_or_clutches_per_y, and adult_body_mass_g. Take a look at the first few entries in the species column. You will see that it is only the epithet, the second part of the Genus_species name, that is given. Use mutate and paste to convert the species column to a Genus_species by pasting the data in genus and species together. To see how this works, try out the following command, paste(1:3, 4:6). After you have created the new column, remove the genus column (using select and -genus). What is the longest living species in the record? Use arrange to sort the data from longest to shortest longevity (longevity_y), and then look at the top of the file using head to find out. (hint: you will need to use reverse sort (-)). Cut and paste the species name into Google to find out more! Do the same thing but this time find the shortest lived species. Use summarise and group_by to make a table summarising min, median and max life spans (longevity_y) for the three taxonomic classes in the database. Remember that you need to tell R to remove the NA values using a rm.na = TRUE argument. Body size is thought to be associated with life span. Let’s treat that as a hypothesis and test it graphically. Sketch what would the graph would look like if the hypothesis were true, and if it was false. Plot adult_body_mass_g vs. longevity_y (using base R graphics). You should notice that this looks a bit messy. Use mutate to create a new log-transformed variables, logMass and logLongevity. Use these to make a “log-log” plot. You should see that makes the relationship more linear, and easier to “read”. Is there a trade-off between reproductive effort and life span? Think about this as a hypothesis - sketch what would the graph would look like if that were true, and if it was false. Now use the data to test that hypothesis: Use mutate to create a variable called logOffspring which is the logarithm of number of litters/clutches per year multiplied by the number of babies in each litter/clutch . Then plot logOffspring vs. logLongevity. To answer the final question (differences between taxonomic classes) you could now use filter to subset to particular classes and repeat the plot to see whether the relationships holds universally. Remember that if you struggle you can check back to previous work where you have used dplyr commands to manipulate data in a similar way. If you get truly stuck, ask for help from instructors or fellow students. https://esajournals.onlinelibrary.wiley.com/doi/10.1890/15-0846R.1↩︎ "],
["combining-data-sets.html", "Chapter 4 Combining data sets 4.1 Using join 4.2 Exercise: Temperature effects on egg laying dates", " Chapter 4 Combining data sets [INSERT PREAMBLE ABOUT DATA MANAGEMENT] [ADD SECTION ON GATHER] 4.1 Using join By following this example you will learn how to combine two data sets to create a new one of combined data to answer a conservation-related question: “Does threat status vary with species’ generation times?” This question is crucial to conservation biologists because it helps us to generalise our ideas about what drives extinction risks. In other words, if we can say “species with slow life histories tend to be more threatened” then this gives useful information that can help with planning. For example, imagine we have some species that have not yet been assessed (we don’t know if they are threatened or not). Should we focus attention on the one with a short generation time, or long generation time? To answer the question we will need to import two large data sets, tidy them up a bit and then combine them for analysis. Let’s start with the “Amniote Life History Database,”5 which is a good source of life history data. We have encountered this database before. Recall that the missing values (which are normally coded as NA in R) are coded as “-999”. The easiest way to take care of this is to specify this when we import the data using the na.strings argument of the read.csv function. Thus we can import the data like this: amniote &lt;- read.csv(&quot;CourseData/Amniote_Database_Aug_2015.csv&quot;,na.strings = &quot;-999&quot;) We can filter on the taxonomic class to subset to only mammals. Then, to address our question, we want data on generation time for mammals. Generation time is often measured as the average age at which females reproduce so we can get close to that with female_maturity_d. We will first select these columns, along with genus and species. We can combine these two taxonomic variables using mutate and paste to get our Latin binomial species name. We have previously learned that log transforming such variables is a good thing to do, so we can use mutate again to do this transformation. Finally, we can use na.omit to get rid of entries with missing values (which we cannot use). This is not essential, but keeps things more manageable. mammal &lt;- amniote %&gt;% filter(class == &quot;Mammalia&quot;) %&gt;% #get the mammals only select(genus, species, female_maturity_d) %&gt;% #get useful columns mutate(species = paste(genus, species)) %&gt;% select(-genus) %&gt;% mutate(logMaturity = log(female_maturity_d)) %&gt;% na.omit() Let’s take a quick look at what we have: head(mammal) ## species female_maturity_d logMaturity ## 20 Echinops telfairi 278.42000 5.629131 ## 22 Hemicentetes nigriceps 48.57000 3.883006 ## 23 Hemicentetes semispinosus 46.19892 3.832956 ## 27 Microgale dobsoni 669.59200 6.506669 ## 40 Microgale talazaci 639.00000 6.459904 ## 47 Setifer setosus 198.00000 5.288267 Looks good. Now let’s import the IUCN Red List data. redlist &lt;- read.csv(&quot;CourseData/MammalRedList.csv&quot;) Let’s take a look at that. names(redlist) ## [1] &quot;Species.ID&quot; &quot;Kingdom&quot; &quot;Phylum&quot; &quot;Class&quot; &quot;Order&quot; ## [6] &quot;Family&quot; &quot;Genus&quot; &quot;Species&quot; &quot;Authority&quot; &quot;Infraspecific.rank&quot; ## [11] &quot;Infraspecific.name&quot; &quot;Infraspecific.authority&quot; &quot;Stock.subpopulation&quot; &quot;Synonyms&quot; &quot;Common.names..Eng.&quot; ## [16] &quot;Common.names..Fre.&quot; &quot;Common.names..Spa.&quot; &quot;Red.List.status&quot; &quot;Red.List.criteria&quot; &quot;Red.List.criteria.version&quot; ## [21] &quot;Year.assessed&quot; &quot;Population.trend&quot; &quot;Petitioned&quot; unique(redlist$Red.List.status) ## [1] &quot;DD&quot; &quot;LC&quot; &quot;CR&quot; &quot;NT&quot; &quot;EN&quot; &quot;VU&quot; &quot;EX&quot; &quot;EW&quot; There’s a lot of information there but what we really need is simply the Latin binomial (for which we need genus and species) and the threat status Red.List.status. R treats categorical variables (factor variables) as alphabetical, but in this case the red list status has a meaning going from low threat (Least Concern - LC) to Critically Endangered (CR) and even Extinct in the Wild (EX) at the other end of the spectrum. We can define this ordering using mutate with the factor function. redlist &lt;- redlist %&gt;% mutate(species = paste(Genus, Species))%&gt;% select(species, Red.List.status) %&gt;% mutate(Red.List.status = factor(Red.List.status, levels = c(&quot;LC&quot;,&quot;NT&quot;,&quot;VU&quot;,&quot;EN&quot;,&quot;CR&quot;,&quot;EW&quot;,&quot;EX&quot;))) head(redlist) ## species Red.List.status ## 1 Abditomys latidens &lt;NA&gt; ## 2 Abeomelomys sevia LC ## 3 Abrawayaomys ruschii LC ## 4 Abrocoma bennettii LC ## 5 Abrocoma boliviensis CR ## 6 Abrocoma budini &lt;NA&gt; Now we can combine this with the life history data from above using left_join. x &lt;- left_join(mammal,redlist,by = &quot;species&quot;) Let’s take a look at what we have now: head(x) ## species female_maturity_d logMaturity Red.List.status ## 1 Echinops telfairi 278.42000 5.629131 LC ## 2 Hemicentetes nigriceps 48.57000 3.883006 LC ## 3 Hemicentetes semispinosus 46.19892 3.832956 LC ## 4 Microgale dobsoni 669.59200 6.506669 LC ## 5 Microgale talazaci 639.00000 6.459904 LC ## 6 Setifer setosus 198.00000 5.288267 LC summary(x) ## species female_maturity_d logMaturity Red.List.status ## Length:2000 Min. : 23.81 Min. :3.170 LC :1219 ## Class :character 1st Qu.: 121.53 1st Qu.:4.800 VU : 176 ## Mode :character Median : 344.12 Median :5.841 EN : 168 ## Mean : 574.92 Mean :5.745 NT : 114 ## 3rd Qu.: 696.38 3rd Qu.:6.546 CR : 66 ## Max. :6391.56 Max. :8.763 (Other): 10 ## NA&#39;s : 247 You can see that there are 247 missing values for the Red List status. These are either species that have not yet been assessed, or maybe where there are mismatches in the species names between the two databases. We will ignore this problem today. Before plotting, I will also use filter remove species that are extinct (status = “EX” and “EW”). To do this I use the %in% argument to allow me to match a vector of variables. Because I want to NOT match them I negate the match using !. I then ensure that those levels are removed from the variable using droplevels. x &lt;- x %&gt;% filter(!Red.List.status %in% c(&quot;EX&quot;,&quot;EW&quot;)) %&gt;% droplevels() Let’s now plot the data to answer the question. plot(x$Red.List.status,x$logMaturity,ylab=&quot;Maturity&quot;) What can we see? If you focus on the median values, it looks like there is a weak positive relationship between this life history trait and threat status: animals with slower life histories tend to be more threatened. 4.2 Exercise: Temperature effects on egg laying dates Data have been collected on great tits (musvit) at SDU for several years. Your task today is to analyse these data to answer the question: is egg laying date associated with spring temperature? The idea here is that warmer springs will lead to delayed egg laying which could have negative consequences to the population if their caterpillar food source doesn’t keep pace with the change. You are provided with two data sets: one on the birds and another on weather. You will need to process these using tools in the dplyr package, and combine them (using left_join) for analysis. The first data set, eggDates.csv, is data from the SDU birds project. The data are arranged in columns where each column is a year and each row is a nest. The data in each column is the day of the year that the first egg in the nest was laid. These data do NOT fulfill the “tidy data” standard where each variable gets a column. In this case,a single variable (first egg date) gets many columns (one for each year), and column headers are data (the years). The data will need to be processed before you can analyse it. You will need to use gather to fix this issue so that you produce a version of the data with three columns - nestNumber, Year and dayNumber. The second dataset, AarslevTemperature.csv, is a weather dataset from Årslev near Odense. This dataset includes daily temperatures records for several years. You will need to summarise this data to obtain a small dataset that has the weather of interest - average temperature in the months of February to April for each year. To answer the question, you will need to join these data sets together. Import the data and take a look at it with head or str. Use gather to reformat the data. This might take a bit of trial and error - don’t give up! Maybe this will help: The first argument in the gather command indicates what the columns in the main data represent (i.e. here the column represents “Year”). The second argument is the name you would give to the actual data (i.e. “day”, in this case). The final argument then tells the function which columns of the data are not to be rearranged (i.e. “boxNumber” in this case). You should end up with a dataset with three columns as described above. Ensure that year is coded as numeric variable using mutate. [Hint, you can use the command as.numeric, but first remove the “y” in the name using gsub]. Calculate the mean egg date per year using summarise (remember to group_by the year first). Take a look at the data. Import the weather data and take a look at it with head or str. Use filter subset to the months of interest (February-April) and then summarise the data to calculate the mean temperature in this period (remember to group_by year). Look at the data. You should end up with a dataset with two columns - year and meanSpringTemp. Join the two datasets together using left_join. You should now have a dataset with columns nestNumber, Year, dayNumber and meanAprilTemp plot a graph of meanAprilTemp on the x-axis and dayNumber on the y-axis. Now you should be able to answer the question we started with: is laying date associated with spring temperatures. https://esajournals.onlinelibrary.wiley.com/doi/10.1890/15-0846R.1↩︎ "],
["visualising-data-with-ggplot.html", "Chapter 5 Visualising data with ggplot 5.1 Histograms 5.2 “Facets” - splitting data across panels 5.3 Box plots 5.4 Lines and points 5.5 Scatter plots", " Chapter 5 Visualising data with ggplot In this chapter you will be guided through using the ggplot2 package to make some pretty plots. You will therefore need the ggplot2 package to make this work. Remember, you can load packages like this: library(ggplot2) We will use the SDU birds clutch size data that we produced at the end of the “[Data wrangling with dplyr]” chapter for these examples. Remember to set your working directory, and start a new script. I am assuming that you have saved your data in a folder called “CourseData” inside your working directory. df &lt;- read.csv(&quot;CourseData/SDUClutchSize.csv&quot;) 5.1 Histograms The ggplot function expects two main arguments (1) the data and (2) the aesthetics. The aesthetics are the variables you want to plot, and associated characteristics like colours, groupings etc. The first argument is for the data, then the aesthetics are specified within the aes(...) argument. These usually include an argument for x which is normally the variable that appears on the horizontal axis, and (often) y which is usually the variable on the vertical axis. The details of this depend on the type of plot you are making. After setting up the plot the graphics are added as geometric layers or geoms. There are many of these available including geom_histogram, geom_line, geom_point etc. I will illustrate the construction of a simple plot by making a histogram of the clutch size of all the nests in the dataset. ggplot(df, aes(x = clutchSize)) This produces an empty plot because we have not yet specified what kind of plot we want. We want a histogram, so we can add this as follows. I have set binwidth to be 1 because we know we are dealing with counts between just 1 and 14. Try altering the binwidth. ggplot(df, aes(x = clutchSize)) + geom_histogram(binwidth = 1) We know that we have two species here and we would like to compare them. This is done within the aesthetic argument. The default is that the bars for different categories are stacked on top of each other. This is good in some cases, but probably not here. ggplot(df, aes(x = clutchSize,fill = species)) + geom_histogram(binwidth = 1,position = &quot;dodge&quot;) You can immediately see that there are far fewer blue tit nests than great tit ones. But you can also see that the center of mass for blue tits is further to the right than great tits. To make it easier to compare distributions with very different counts, we can put density on the y-axis instead of the default count using the argument stat(density). ggplot(df, aes(x = clutchSize, fill = species, stat(density))) + geom_histogram(binwidth = 1,position = &quot;dodge&quot;) An alternative approach would be to overlay the two sets of bars (using position = \"identity\") and set the colours to be slightly transparent (using alpha = 0.7) so that you can see the overlapping region clearly. ggplot(df, aes(x = clutchSize, fill = species, stat(density))) + geom_histogram(binwidth = 1,position = &quot;identity&quot;,alpha=0.7) It is very clear from this plot that blue tits tend to have bigger clutch sizes than great tits. Is this difference statistically significant? We will look at testing this in a future class - for now we will be satisfied with our visualisation. 5.2 “Facets” - splitting data across panels You should recall that there were several years of data represented here. ggplot has a very clever way of splitting up the plot to examine this. ggplot(df, aes(x = clutchSize, fill = species, stat(density))) + geom_histogram(binwidth = 1,position = &quot;identity&quot;,alpha=0.7) + facet_grid(.~Year) You could split the data up by species in a similar way, as yet another way of visualising the difference between species: ggplot(df, aes(x = clutchSize)) + geom_histogram(binwidth = 1) + facet_grid(species~.) You can change whether the separate graphs are presented in a rows or columns by changing the order of the argument: facet_grid(species~.) or facet_grid(.~species). Try it. 5.3 Box plots Box plots are suitable for cases where one variable is categorical with 2+ levels, and the other is continuous. Therefore, another way to look at these distributions is to use a box plot. In a box plot the box shows the quartiles (i.e. the 25% and 75% quantiles) within which 50% of the data are found. The horizontal line in the box is the median, Then the whiskers extend from the smallest to largest value unless they are further than 1.5 times the interquartile range (the length of the box) away from the edge of the box, in which case they are individually shown as outlier points. To plot them using ggplot you must use a geom_boxplot layer. The categorical variable is normally placed on the x-axis so is placed as x in the aes argument, while the continuous variable is on the y axis. ggplot(df, aes(x = species, y = clutchSize)) + geom_boxplot() Some researchers argue that it is a good idea to add the data as points to these plots as “full disclosure” of what the underlying data look like. These can be added with a geom_jitter layer (jitter is random noise added in this case to the horizontal axis). You should set width and alpha arguments to make it look nice. ggplot(df, aes(x = species, y = clutchSize)) + geom_boxplot() + geom_jitter(width = .2, alpha = 0.5, colour=&quot;black&quot;,fill=&quot;black&quot;) Try splitting the data into different years using facet_grid with the box plot. 5.4 Lines and points Perhaps not surprisingly lines and points can be added with the geoms, geom_line and geom_point respectively. To illustrate this we will make a plot showing how clutch size changes among years. First we will use summarise to create a dataset with the mean clutch size. We’ll start simply, by looking at only great tits. GTclutch &lt;- df %&gt;% filter(species == &quot;GT&quot;) %&gt;% group_by(Year) %&gt;% summarise(meanClutchSize = mean(clutchSize)) Then you can plot this like this. ggplot(GTclutch, aes(x = Year, y = meanClutchSize)) + geom_line() I think this looks OK, but we should add both species. I’ll first need to produce a mean clutch size dataset that includes both species. meanClutch &lt;- df %&gt;% group_by(species,Year) %&gt;% summarise(meanClutchSize = mean(clutchSize)) Now I can do the plot again. The only difference to the command is that I need to tell R that I want to colour the lines by species (colour = species). ggplot(meanClutch, aes(x = Year, y = meanClutchSize, colour = species)) + geom_line() I can improve on this by (1) changing the y axis limits (using ylim) so that it goes through the full range of my data (0 - 14); (2) adding points (using a geom_point layer) where my actual data values are; (3) adding a nicely formatted axis label (using ylab); adding a title (ggtitle) ggplot(meanClutch, aes(x = Year, y = meanClutchSize, colour = species)) + geom_line() + geom_point() + ylim(0,14) + ylab(&quot;Mean clutch size&quot;) + ggtitle(&quot;Clutch size data from SDU Campus&quot;) 5.5 Scatter plots Finally, lets make a scatter plot. The SDU bird data are not suitable for this type of plot so we’ll use the data from a few days ago on suburban bird diversity. df &lt;- read.csv(&quot;CourseData/suburbanBirds.csv&quot;) Take a look at the data to remind ourselves what it looks like head(df) ## Name Year HabitatIndex nIndividuals nSpecies ## 1 Alamotos 1946 10.0 48 12 ## 2 Ramona 1946 9.5 30 13 ## 3 Verona 1947 9.5 38 15 ## 4 Valle Vista 1950 9.5 42 11 ## 5 La Gonda 1955 11.0 44 13 ## 6 Belgian 1956 9.0 27 14 These data show the result of standardised bird surveys at housing developments of different ages in California. The surveys were carried out in 1975, and the data includes the Year and number of individual birds seen nIndividuals and number of species seen nSpecies. The question being addressed is “How does the age of the housing development affect the number of species?” To investigate this we should first add a new variable for Age to the data set. We can do this using the mutate function from dplyr. This function creates new variables, for example by manipulating existing ones. df &lt;- mutate(df,Age = 1975 - Year) When we have created this variable we can plot the data. For aesthetic reasons I also would like to set the limits on the y-axis to go extend to zero, and I would like to include proper labels on the axes. ggplot(df, aes(x = Age,y = nSpecies)) + geom_point() + ylim(0,15) + xlab(&quot;Age of development&quot;) + ylab(&quot;Bird species richness&quot;) This shows very clearly that older developments have more species, but it also appears to show that there is an asymptote around 13 species. Compare this plot to the one you made with base graphics in a previous class. "],
["distributions-and-summarising-data.html", "Chapter 6 Distributions and summarising data 6.1 Distributions 6.2 Normal distribution 6.3 Comparing normal distributions 6.4 Poisson distribution 6.5 Comparing normal and Poisson distributions", " Chapter 6 Distributions and summarising data This chapter covers two broad topics: the concept of statistical distributions and summarising data. 6.1 Distributions A statistical distribution is a description of the relative number of times (the frequency) possible outcomes will occur if repeated samples were to be taken. They are important because (1) they are useful descriptors of data and (2) they form the basis for assumptions in some statistical approaches. For example, statistical analyses often assume a normal distribution. The normal distribution is symmetrical (centered on the mean) and 68% of observations fall within 1 standard deviation (s.d.), and 95% of observations fall within 2 s.d.. We will use R to simulate some distributions, and explore these to get a feel for them. R has functions for generating random numbers from different kinds of distributions. For example, the function rnorm will generate numbers from a normal distribution and rpois will generate numbers from a Poisson distribution. 6.2 Normal distribution The rnorm function has three arguments. The first argument is simply the number of values you want to generate. Then, the second and third arguments specify the the mean and standard deviation values of the distribution (i.e. where the distribution is centered and how spread out it is). The following command will produce 6 numbers from a distribution with a mean value of 5 and a standard deviation of 2. rnorm(6,5,2) ## [1] 6.546567 5.868989 7.427407 2.799911 6.036900 4.607270 Try changing the values of the arguments to alter the number of values you generate, and to alter the mean and standard deviation. Let’s use this to generate a larger data frame, and then place markers for the various measures of “spread” onto a plot. Note that here I put a set of parentheses around the plot code to both display the result AND save the plot as an R object called p1 rn &lt;- data.frame(d1 = rnorm(500,5,2)) summary(rn) #Take a look ## d1 ## Min. :-0.04319 ## 1st Qu.: 3.80221 ## Median : 4.98292 ## Mean : 5.05574 ## 3rd Qu.: 6.29106 ## Max. :10.73554 #Plot the data (p1 &lt;- ggplot(rn,aes(x=d1)) + geom_histogram() ) We can calculate the mean and standard deviation using summarise (along with other estimates of “spread”). The mean and standard deviation values will be close (but not identical) to the values you set when you generated the distribution. Note that here I put a set of parentheses around the code to both display the result AND save the result in an object called sv (sv &lt;- rn %&gt;% summarise(meanEst = mean(d1), sdEst = sd(d1), varEst = var(d1), semEst = sd(d1)/sqrt(n())) ) ## meanEst sdEst varEst semEst ## 1 5.055737 2.000436 4.001743 0.0894622 Let’s use the function geom_vline to add some markers to the plot from above to show these values… (p2 &lt;- p1 + geom_vline(xintercept = sv$meanEst, size=2) + #mean geom_vline(xintercept = sv$meanEst+sv$sdEst, size=1) + #upperSD geom_vline(xintercept = sv$meanEst-sv$sdEst, size=1) #lowerSD ) We can compare these with the true values (the values we set when we generated the data), by adding them to the plot in a different colour (mean=5, sd=2). (p3 &lt;- p2 + geom_vline(xintercept = 5, size=2, colour=&quot;red&quot;) + #mean geom_vline(xintercept = 5+2, size=1,colour=&quot;red&quot;) + #upperSD geom_vline(xintercept = 5-2, size=1,colour=&quot;red&quot;) #lowerSD ) Try repeating these plots with data that has different sample sizes. For example, use sample sizes of 5000, 250, 100, 50, 10. What do you notice? You should notice that for smaller sample sizes, the true distribution is not captured very well. When you calculate the mean and standard deviation, you are actually fitting a simple model: the mean and standard deviation are parameters of the model, which assumes that the data follow a normal distribution. Try adding lines for the standard error of the mean to one of your histograms. 6.3 Comparing normal distributions Because normal distributions all have the same shape, it can be hard to grasp the effect of changing the distribution’s parameters viewing them in isolation. In this section you will write some code to compare two normal distributions. This approach can be useful when considering whether a proposed experiment will successfully detect a difference between treatment groups. We’ll look at this topic, known as “power analysis”, in greater detail in a later class. For now we will simply use ggplot to get a better feel for the normal distribution. Let’s use rnorm to generate a larger data frame with two sets of numbers from different distributions: (d1: mean = 5, sd = 2; d2: mean = 8, sd = 1). rn &lt;- data.frame(d1 = rnorm(500,5,2),d2 = rnorm(500,8,1)) summary(rn) ## d1 d2 ## Min. :-1.703 Min. : 5.375 ## 1st Qu.: 3.724 1st Qu.: 7.289 ## Median : 4.960 Median : 7.969 ## Mean : 5.008 Mean : 7.957 ## 3rd Qu.: 6.230 3rd Qu.: 8.643 ## Max. :11.436 Max. :10.773 The summaries (above) show that the mean and the width of the distributions vary, but we should always plot our data. So lets make a plot in ggplot. In the dataset I created I have the data arranged by columns side-by-side, but ggplot needs the values to be arranged in a single column, and the identifier of the sample ID in a second column. I can use the function gather to rearrange the data into the required format. rn &lt;- gather(rn,key = &quot;sampleID&quot;,value = &quot;value&quot;) #rearrange data #Plot histograms using &quot;identity&quot;, and make them transparent ggplot(rn,aes(x = value,fill=sampleID)) + geom_histogram(position = &quot;identity&quot;, alpha=0.5) Try changing the distributions and re-plotting them (you can change the number of samples, the mean values and the standard deviations). 6.4 Poisson distribution The Poisson distribution is typically used when dealing with count data. The values must be whole numbers (integers) and they cannot be negative. The shape of the distributions varies with the “lambda” parameter. Small values of lambda give more skewed distributions. Let’s generate and plot some Poisson distributed data. rp &lt;- data.frame(d1 = rpois(500,2.4)) summary(rp) #Take a look ## d1 ## Min. :0.00 ## 1st Qu.:1.00 ## Median :2.00 ## Mean :2.41 ## 3rd Qu.:3.00 ## Max. :7.00 #Plot the data (p1 &lt;- ggplot(rp,aes(x=d1)) + geom_histogram(binwidth = 1) # we know the bins will be 1 ) Try changing the value of lambda and look at how the shape changes. Let’s calculate summary statistics of mean and standard deviation for this distribution (sv &lt;- rp %&gt;% summarise(meanEst = mean(d1), sdEst = sd(d1)) ) ## meanEst sdEst ## 1 2.41 1.570616 Now lets plot the mean and the 2 times the standard deviation on the graph. Remember that for the normal distribution (above) that 95% of the data were within 2 times the standard deviation. p1 + geom_vline(xintercept = sv$meanEst, size=2) + #mean geom_vline(xintercept = sv$meanEst+2*sv$sdEst, size=1) + #upper2SD geom_vline(xintercept = sv$meanEst-2*sv$sdEst, size=1) #lower2SD This looks like a TERRIBLE fit: The mean is not close to the most common value in the data set and the lower limit of the standard deviation indicates we should expect some negative values - this is impossible for Poisson data. The reason for this is that mean and standard deviation, and therefore standard error, are intended for normally distributed data. When the data come from other distributions we must take another approach. So how should we summarise this data? One approach is to report the median as a measure of “central tendency” instead of the mean, and to report “quantiles” of the data along with the range (i.e. minimum and maximum). Quantiles are simply the cut points that divide the data into parts. For example, the 25% quantile is the point where (if the data were arranged in order) one quarter of the values would fall below; the 50% quantile would mark the middle of the data (= the median); the 75% quantile would be the point when three-quarters of the data are below. You can calculate those things using dplyr’s summarise. However, you can also simply use the base R summary command. (sv &lt;- rp %&gt;% summarise(minVal = min(d1), q25 = quantile(d1,0.25), med = median(d1), q75 = quantile(d1,0.75), maxVal = max(d1)) ) ## minVal q25 med q75 maxVal ## 1 0 1 2 3 7 #base R summary is just as good. summary(rp$d1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 1.00 2.00 2.41 3.00 7.00 6.5 Comparing normal and Poisson distributions To get a better feel for how these two distributions differ, lets use the same approach we used above to plot two distributions together. rn &lt;- data.frame(normDist = rnorm(500,2,2),poisDist = rpois(500,2.4)) summary(rn) ## normDist poisDist ## Min. :-3.7862 Min. :0.000 ## 1st Qu.: 0.5507 1st Qu.:1.000 ## Median : 1.7944 Median :2.000 ## Mean : 1.9137 Mean :2.346 ## 3rd Qu.: 3.3323 3rd Qu.:3.000 ## Max. : 7.7925 Max. :8.000 rn &lt;- gather(rn,key = &quot;sampleID&quot;,value = &quot;value&quot;) #rearrange data #Plot histograms using &quot;identity&quot;, and make them transparent ggplot(rn,aes(x = value,fill=sampleID)) + geom_histogram(position = &quot;identity&quot;, alpha=0.5,binwidth = 1) Try changing the arguments in the rnorm and rpois commands to change the distributions. Finally, let’s take another view of these data and look at them using box plots. Box plots are a handy alternative to histograms and many people prefer them. ggplot(rn,aes(x=sampleID, y = value,fill=sampleID)) + geom_boxplot() You should see the main features of both distributions are captured pretty well. The normal distribution is approximately symmetrical and the Poisson distribution is skewed (one whisker longer then the other) and cannot be &lt;0. Which graph to you prefer? (there’s no right answer!) "],
["customising-your-plots.html", "Chapter 7 Customising your plots 7.1 A basic plot 7.2 Axis limits 7.3 Transforming the axis (log scale) 7.4 Changing the axis tick marks 7.5 Axis labels 7.6 Colours 7.7 Themes 7.8 Moving the legend 7.9 Combining multiple plots 7.10 Saving your plot 7.11 Final word on plots", " Chapter 7 Customising your plots In this chapter you will learn, by following examples, how to customise plots made with ggplot to improve “readability”, or just for aesthetic reasons. We will cover the following: Modifying axes (log transform, different tick marks/ranges etc.). Colour schemes. Themes - built-in sets of styles. Multiple sub-plots in a plot. Saving your plots. For these examples I will use the dataset on animal life history, Anage. x &lt;- read.csv(&quot;CourseData/anage_data.csv&quot;) You can remind yourself what this data looks like using commands like summary, str and names. I will process the data a bit to make it easier to work with. One of the commands might be new to you - rename. This is simply a way of renaming columns, in this case to make them more “user friendly” (e.g. I want to rename the column “Metabolic.rate..W.” to “BMR” (for basal metabolic rate)). I will also use mutate to (1) convert the Mass from grams to kilograms and (2) to make a new variable called “BMRperKg” which standardises metabolic rate by expressing it as rate per kilogram. anage &lt;- x %&gt;% mutate(Species = paste(Genus,Species)) %&gt;% rename(Longevity = &quot;Maximum.longevity..yrs.&quot;, Mass = &quot;Body.mass..g.&quot; , BMR = &quot;Metabolic.rate..W.&quot;) %&gt;% select(Class, Order, Species,Mass,Longevity,BMR) %&gt;% filter(Class %in% c(&quot;Aves&quot;,&quot;Amphibia&quot;,&quot;Mammalia&quot;,&quot;Reptilia&quot;)) %&gt;% droplevels() %&gt;% #this removes unused &quot;factor levels&quot; e.g. &quot;Insecta&quot; mutate(Mass = Mass/1000, BMRperKg = BMR/Mass) summary(anage) ## Class Order Species Mass Longevity BMR BMRperKg ## Length:3231 Length:3231 Length:3231 Min. : 0.001 Min. : 0.40 Min. : 0.0001 Min. : 0.0454 ## Class :character Class :character Class :character 1st Qu.: 0.026 1st Qu.: 10.20 1st Qu.: 0.2655 1st Qu.: 2.2191 ## Mode :character Mode :character Mode :character Median : 0.131 Median : 16.20 Median : 0.7050 Median : 4.5745 ## Mean : 13.188 Mean : 19.37 Mean : 11.8309 Mean : 7.0439 ## 3rd Qu.: 1.111 3rd Qu.: 24.45 3rd Qu.: 3.1370 3rd Qu.: 9.8686 ## Max. :3672.000 Max. :211.00 Max. :2336.5000 Max. :45.7692 ## NA&#39;s :2604 NA&#39;s :432 NA&#39;s :2604 NA&#39;s :2604 7.1 A basic plot Now lets start with a basic plot. You will see a warning about removing rows with missing values. This is just a warning to let you know that there are missing (NA) values in the data you are plotting. (p1 &lt;- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + geom_point(alpha=0.3)) #use alpha argument to make points transparent 7.2 Axis limits These points are really spread out. One option to deal with this might be to set the range over which the axes are allowed to go using xlim and ylim. p1 + xlim(0,500) + ylim(0,300) 7.3 Transforming the axis (log scale) In this particular case though use of a log scale would be best because even after focusing on a smaller part of the range of values you can see that the points are still concentrated at smaller values. In a moment, you will also see that log-transforming the data makes the cloud of points pleasingly linear. You can set a log scale by using the commands scale_x_continuous(trans = \"log\") and scale_y_continuous(trans = \"log\"). (p2 &lt;- p1 + scale_x_continuous(trans = &quot;log&quot;) + scale_y_continuous(trans = &quot;log&quot;)) 7.4 Changing the axis tick marks This looks nice. But the numbers on the axis are not very nice. Using summary(anage$BMR) tells us that the range of data is from 0.0001 to 2336.5. We could place tick marks anywhere on this axis, but let’s try 0.0001, 0.001,0.1, 1,10, 100, 1000. (p2 &lt;- p1 + scale_x_continuous(trans = &quot;log&quot;) + scale_y_continuous(trans = &quot;log&quot;, breaks = c(0.0001,0.001,0.01,0.1,1,10,100,1000))) Using summary(anage$Mass) tells us that the range of data is from 0.001 to 3672. We could place tick marks anywhere on this axis, but let’s try 0.001,0.1, 1,10, 100, 1000. (p2 &lt;- p1 + scale_x_continuous(trans = &quot;log&quot;, breaks = c(0.001,0.01,0.1,1,10,100,1000)) + scale_y_continuous(trans = &quot;log&quot;, breaks = c(0.0001,0.001,0.01,0.1,1,10,100,1000)) ) 7.5 Axis labels Now, let’s think about the axis labels. The labels in the plots so far have no units indicated, and might not be easy to interpret for the reader. Let’s add units, and also spell out more fully what “BMR” and “Mass” means (the axes is basal metabolic rate in Watts and adult body mass in kg). (p3 &lt;- p2 + xlab(&quot;Adult body mass (kg)&quot;) + ylab(&quot;Basal metabolic rate (W)&quot;) ) 7.6 Colours What about those colours? The ggplot package uses some default colours that are OK, but sometimes you will want to make a change. You can “manually” adjust colours using the scale_colour_manual function. You can either name individual colours (e.g. “red”,“green”,“orange”,“black”)6, or you can find their so-called “hex-codes” from a site like http://colorbrewer2.org/ or https://htmlcolorcodes.com/color-picker/. You can add a two digit number after the hex code to set the “opaqueness” of the colour. For example “#FF000075” is red, with 75% opacity. With colour names… p3 + scale_colour_manual(values = c(&quot;red&quot;,&quot;green&quot;,&quot;orange&quot;,&quot;black&quot;)) And with some hex codes… p3 + scale_colour_manual(values = c(&quot;#33FF6475&quot;,&quot;#3368FF75&quot;,&quot;#FF33CE75&quot;,&quot;#FFCA3375&quot;)) Another alternative is to use some of ggplot’s built in “palettes” of colour combinations. For example, there are several palettes called “viridis”. p3 + scale_colour_viridis_d(option = &quot;D&quot;) Try using other option arguments A, B, C and E. Try also adding an argument for transparency alpha = 0.5. Here’s a couple more palettes. There’s one for shades of grey… p3 + scale_colour_grey() There’s another one for various colour schemes, called “colour brewer”. Try using “RdGy”, “RdYlBu” and “Spectral” see ?scale_colour_brewer for more options. p3 + scale_colour_brewer(palette = &quot;BrBG&quot;) 7.7 Themes Finally, ggplot includes the option to set a theme for the plots. “Themes”\" make adjustments to the “look” of the plot. It is possible to write your own themes, but I recommend to use some ready-made ones. You can implement them by adding them as you would any other addition to the ggplot command (e.g. + theme_light(). There are several themes included with ggplot. Try my favourite, theme_minimal(). Then try theme_classic() and theme_dark(). (p4 &lt;- p3 + theme_minimal() ) For more theme fun, you can install packages that include more themes. The best one is called ggthemes (remember that you only need to install the package once). Try theme_economist(), theme_tufte() and (ugh!) theme_excel(). You can see what other themes there in this package at https://jrnold.github.io/ggthemes/reference/index.html (some of them are really ugly in my opinion!). install.packages(&quot;ggthemes&quot;) library(ggthemes) p3 + theme_economist() This package also includes some useful colour scales, including some for colour blind people. p3 + scale_color_colorblind() 7.8 Moving the legend By default, the legend is placed on the right. You can move it around by adding a theme argument to your plot commands. It can also be placed on the “top”, “bottom”, or “left”. You can also remove the legend altogether by using legend.position = \"none\". You might also want to remove the legend title using the theme argument legend.title = element_blank(). p3 + theme(legend.position = &quot;bottom&quot;) 7.9 Combining multiple plots It is often useful to combine two or more plots into a single figure. For example, many journals have strict limits on the number of plots so it is useful to combine plots into “Figure 1A and B” etc. There are several R packages that can do this and my favourite is called patchwork. install.packages(&quot;patchwork&quot;) #only need to do this once library(patchwork) I will illustrate it by first making another plot, this time showing the relationship between body mass and standardised BMR (BMR per kg). Because I am combining the plots into a smaller space I have decided to remove the figure legend (I could put it in the figure caption instead). #PlotA (this is what you have already created above) plotA &lt;- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + geom_point(alpha=0.5) + scale_x_continuous(trans = &quot;log&quot;,breaks =c(0.001,0.01,0.1,1,10,100,1000)) + scale_y_continuous(trans = &quot;log&quot;,breaks =c(0.0001,0.001,0.01,0.1,1,10,100,1000)) + xlab(&quot;Adult body mass (kg)&quot;) + ylab(&quot;Basal metabolic rate (W)&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) (plotB &lt;- ggplot(anage,aes(x = Mass, y = BMRperKg, colour = Class)) + geom_point(alpha=0.5) + scale_x_continuous(trans = &quot;log&quot;,breaks =c(0.001,0.01,0.1,1,10,100,1000)) + scale_y_continuous(trans = &quot;log&quot;,breaks =c(0.0001,0.001,0.01,0.1,1,10)) + xlab(&quot;Adult body mass (kg)&quot;) + ylab(&quot;Basal metabolic rate (W/kg)&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) ) #This one is wrapped in brackets so that R shows it Now I can combine these using the very simple syntax like this: plotA + plotB I can add titles using the ggtitle command like this. plotA + ggtitle(&quot;A&quot;) + plotB + ggtitle(&quot;B&quot;) You could place the sub-plots on top of each other like this. (plotA + ggtitle(&quot;A&quot;)) / (plotB + ggtitle(&quot;B&quot;)) 7.10 Saving your plot You should, I think, already know about using the “Export” button in RStudio to save out your plot. This is useful and easy, but you should know that you can also save the plots using a typed command (ggsave) in your script. This command is handy because it allows you to automatically set the size, and file name of your plot. The default setting for ggsave is that it will save the last plot that was printed to your computer screen to a file name that you specify. Therefore easiest way to use the command is to simply place the ggsave command immediately after your ggplot command. You should set the width and height of the plot and the units (the default is inches). It usually takes a few attempts and a bit of trial-and-error to choose the dimensions so that the plot looks nice. ggsave(&quot;MySavedPlot1.png&quot;, width = 18, height=10, units = &quot;cm&quot;) The command can save to various file types including png, jpeg, pdf (see the ggplot help file for more). R knows what file file type is chosen by checking the file extension in the file name (e.g. .png). I advise to use png. 7.11 Final word on plots We have covered a lot of ground here. There is a lot to learn, but don’t feel like you have to remember all of these commands (I don’t). Mostly it is simply a case of remembering that it is possible to do these things, and knowing where to look up the commands. Obvious starting points are this course book, and the text book (including the online version!). You can also usually find help by Googling “ggplot” followed by what you are trying to do (e.g. “ggplot change axis ticks”). One of my frequently used web sites is this one http://www.sthda.com/english/ which has an extensive section on ggplot (http://www.sthda.com/english/wiki/ggplot2-essentials). Even though we have covered a lot of ground we have still only gotten a taster of what ggplot is capable of. I encourage you to learn more. A useful resource for learning is the online R graph gallery\" at https://www.r-graph-gallery.com/, which shows you how to make and modify many types of plot. See https://www.r-graph-gallery.com/42-colors-names.html↩︎ "],
["randomisation-tests.html", "Chapter 8 Randomisation Tests 8.1 Randomisation test in R 8.2 Paired Randomisation Tests 8.3 Exercise: Sexual selection in Hercules beetles", " Chapter 8 Randomisation Tests Simple experiments testing for a difference in mean values between two groups usually have the null hypothesis that there is no difference. The alternative hypothesis varies. Sometimes it is simply that the two groups are different (and that the difference could be wither positive or negative). In other cases the alternative hypothesis is that the mean of Group A is less then the mean of Group B (or that it is greater). Randomisation tests are an intuitive, but computationally intensive way of testing these hypotheses. They have a long history and were first proposed by R.A. Fisher in the 1930s. However they only became convenient when computers became sufficiently fast to do the calculations. Carrying out a test in R requires that you put your dplyr skills to the test. Here you will be guided through an example. 8.1 Randomisation test in R A new drug has been developed that is supposed to reduce cholesterol levels in men. An experiment has been carried out where 12 human test subjects have been assigned randomly to two groups: “Control” and “Drug”. The pharmaceutical company is hoping that the “Drug” group will have lower cholesterol than the “Control” group. The aim here is to do a randomisation test to check that. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. Import the data, called cholesterol. Let’s first take a look at the data by plotting it. I will first plot a boxplot first, and add the jittered points for clarity. ggplot(ch,aes(x=Treatment,y=Cholesterol)) + geom_boxplot()+ geom_jitter(colour=&quot;grey&quot;,width=.1) It looks like there might be a difference between the groups. Now let’s consider our test statistic and our hypotheses. Our test statistic is the difference in mean cholesterol levels between the two groups: mean of control group minus the mean of the drug group. The null hypothesis is that there is no difference between these two groups (i.e. the difference should be close to 0) The alternative hypothesis is that the mean of the drug group should be less than the mean of the drug group. (i.e. mean of control group minus the mean of the drug group should be negative). 8.1.1 Calculate the observed difference There are a few ways of doing this. In base-R you can use the function tapply (“table apply”), followed by diff (“difference”). tapply(ch$Cholesterol,ch$Treatment,mean) ## Control Drug ## 205.6667 185.5000 diff(tapply(ch$Cholesterol,ch$Treatment,mean)) ## Drug ## -20.16667 Because we are focusing on learning dplyr, you can also calculate the means like like this: ch %&gt;% # ch is the cholesterol data group_by(Treatment) %&gt;% # group the data by treatment summarise(mean = mean(Cholesterol)) # calculate means ## # A tibble: 2 x 2 ## Treatment mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Control 206. ## 2 Drug 186. Here the pipes (%&gt;%) are passing the result of each function on as input to the next. You can use further commands, pull to get the mean vector from the summary table, and then use diff to calculate the difference between the groups, before passing that to a value called “observedDiff”. observedDiff &lt;- ch %&gt;% group_by(Treatment) %&gt;% # group the data by treatment summarise(mean = mean(Cholesterol)) %&gt;% # calculate means pull(mean) %&gt;% # extract the mean vector diff() This is a complicated set of commands. To make sure that you understand it, try running it bit-by-bit to see what is going on. 8.1.2 Null distribution Now we ask, what would the world look like if our null hypothesis was true. To do this we can disassociate the treatment group variable from the measured cholesterol values. We do this using by using the mutate function to replace the Treatment variable with a shuffled version of itself with the sample function. Let’s try that one time: ch %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% #shuffle the Treatment data group_by(Treatment) %&gt;% summarise(mean = mean(Cholesterol)) %&gt;% pull(mean) %&gt;% diff() ## [1] -10.16667 In this instance, the difference with the shuffled Treatment values is 0.833, which is rather different from our observed difference of -20.1666667. Doing this one time is not much help though - we need to repeat this many times. I suggest that you do it 1000 times here, but some statisticians would suggest 5000 or even 10000 replicates. We can do this easily in R using the function replicate which simply a kind of wrapper that tells R to repeat a command n times and then pass the result to a vector. Let’s try it first 10 times to see how it works: replicate(10, ch %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% group_by(Treatment) %&gt;% summarise(mean = mean(Cholesterol)) %&gt;% pull(mean) %&gt;% diff() ) ## [1] -8.500000 13.500000 -8.166667 -2.833333 -14.500000 5.833333 15.500000 6.500000 -1.166667 -1.833333 You can see that the replicate command simply does the sampling-recalculation of the mean 10 times. In the commands below I create 1000 replicates of the shuffled differences. I want to put them in a dataframe to make it easy to plot. Therefore, I first create a data.frame called shuffledData. This data frame initially has a variable called rep which consists of the numbers 1-1000. I then use mutate to add the 1000 shuffled differences. shuffledData &lt;- data.frame(rep = 1:1000) %&gt;% mutate(shuffledDiffs = replicate(1000, ch %&gt;% mutate(Treatment = sample(Treatment)) %&gt;% group_by(Treatment) %&gt;% summarise(mean = mean(Cholesterol)) %&gt;% pull(mean) %&gt;% diff() )) 8.1.3 Testing significance Before formally testing the hypothesis it is useful to visualise what we have created in a histogram. I can use ggplot to do this, to create a plot called p1. Note that by putting the command in brackets R will both create the plot object, and print it to the screen. Note that because the shuffling of the data is random process your graph will look slightly different to mine. (p1&lt;-ggplot(shuffledData,aes(x=shuffledDiffs)) + geom_histogram() + theme_minimal()+ xlab(&quot;Drug mean - Control mean&quot;)) You can now add your observed difference (calculated above) to this plot like this: p1 + geom_vline(xintercept = observedDiff) 8.1.4 Testing the hypothesis Recall that the alternative hypothesis is that the observed difference (control mean-drug mean) will be less than 0. You can see that there are few of the null distribution sample that are as extreme as the observed difference. To calculate a p-value we can simply count these values and express them as a proportion. Note that because the shuffling of the data is random process your result will probably be slightly different to mine. table(shuffledData$shuffledDiffs&lt;=observedDiff) ## ## FALSE TRUE ## 958 42 So that is 42 of the shuffled values that are equal to or less than the observed difference. The p-value is then simply 42/1000 = 0.042. Therefore we can say that the drug appears to be effective at reducing cholesterol. 8.1.5 Writing it up We can report our findings something like this: \"To test whether effect of the drug at reducing cholesterol level is statistically significant I did a 1000 replicate randomisation test with the null hypothesis being that there is no difference between the group means and the alternative hypothesis that the mean for the drug treatment is lower than the control treatment. I compared the observed difference to this null distribution to calculate a p-value in a one-sided test. The observed mean values of the control and treatment groups 205.667 and 185.500 respectively and the difference between them is therefore -20.167 (drug mean - control mean). Only 25 of the 1000 null distribution replicates were as low or lower than my observed difference value. I conclude that the observed difference between the means of the two treatment groups is statistically significant (p = 0.025)\" 8.2 Paired Randomisation Tests The paired randomisation test is a one-sample randomisation test where the distribution is tested against a value of 0 (i.e. where there is no difference between the two groups). Often, this distribution is the difference in measurements between two sets of measurements taken from the same individuals (or study sites) before and after some treatment has been applied. I will illustrate this with an example from Everitt (1994) who looked at using cognitive behaviour therapy as a treatment for anorexia. Everitt collected data on weights of people before and after therapy. These data are in the file anorexiaCBT.csv #Remember to set your working directory first an &lt;- read.csv(&quot;CourseData/anorexiaCBT.csv&quot;) head(an) ## Subject Week01 Week08 ## 1 1 80.5 82.2 ## 2 2 84.9 85.6 ## 3 3 81.5 81.4 ## 4 4 82.6 81.9 ## 5 5 79.9 76.4 ## 6 6 88.7 103.6 These data are arranged in a so-called “wide” format. To make plotting and analysis data need to be rearranged into a tidy “long” format so that each observation is on a row. We can do this using the gather function: an &lt;- an %&gt;% gather(&quot;time&quot;,&quot;weight&quot;,-Subject) head(an) ## Subject time weight ## 1 1 Week01 80.5 ## 2 2 Week01 84.9 ## 3 3 Week01 81.5 ## 4 4 Week01 82.6 ## 5 5 Week01 79.9 ## 6 6 Week01 88.7 We should always plot the data. So here goes. (p1&lt;-ggplot(an,aes(x=weight,fill=time)) + geom_histogram(position = &quot;identity&quot;,alpha=.7) ) Another useful way to plot this data is to use an interaction plot. In these plots the matched pairs (grouped by Subject) are joined together with lines. You can plot one like this: (p2&lt;-ggplot(an,aes(x=time,y=weight,group=Subject)) + geom_point() + geom_line(size=1, alpha=0.5) ) What we are interested in is whether there has been a change in weight of the subjects after CBT. The null hypothesis is that there is zero change in weight. The alternative hypothesis is that weight has increased. The starting point for the analysis is to calculate the observed change in weight. an &lt;- an %&gt;% group_by(Subject) %&gt;% summarise(change = diff(weight)) You have created a dataset that looks like this: head(an) ## # A tibble: 6 x 2 ## Subject change ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.7 ## 2 2 0.700 ## 3 3 -0.100 ## 4 4 -0.700 ## 5 5 -3.5 ## 6 6 14.9 And you can calculate the observed change like this: obsChange &lt;- mean(an$change) obsChange ## [1] 3.006897 8.2.1 The randomisation test The logic of this test is that if the experimental treatment has no effect on weight, then the Before weight is just as likely to be larger than the After weight as it is to be smaller. Therefore, to carry out this test, we can permute the SIGN of the change in weight (i.e. we randomly flip values from positive to negative and vice versa). We can do this by multiplying by 1 or -1, randomly. head(an) ## # A tibble: 6 x 2 ## Subject change ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.7 ## 2 2 0.700 ## 3 3 -0.100 ## 4 4 -0.700 ## 5 5 -3.5 ## 6 6 14.9 anShuffled &lt;- an %&gt;% mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %&gt;% mutate(shuffledChange = change * sign) Let’s take a look at this new shuffled dataset: head(anShuffled) ## # A tibble: 6 x 4 ## Subject change sign shuffledChange ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.7 1 1.7 ## 2 2 0.700 -1 -0.700 ## 3 3 -0.100 -1 0.100 ## 4 4 -0.700 -1 0.700 ## 5 5 -3.5 -1 3.5 ## 6 6 14.9 -1 -14.9 We need to calculate the mean of this shuffled vector. We can do this by pull to get the vector, and then mean. an %&gt;% mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %&gt;% mutate(shuffledChange = change * sign) %&gt;% pull(shuffledChange) %&gt;% mean() ## [1] -0.462069 Now we will build a null distribution of changes in weight by repeating this 1000 times. We can do this using the replicate function to “wrap” around the function, passing the result into a data frame. We can then compare this null distribution to the observed change. nullDist = data.frame(change = replicate(1000,an %&gt;% mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %&gt;% mutate(shuffledChange = change * sign) %&gt;% pull(shuffledChange) %&gt;% mean())) 8.2.2 Null distribution (nullDistPlot &lt;- ggplot(nullDist,aes(x=change)) + geom_histogram()) We can add the observed change as a line to this: nullDistPlot + geom_vline(xintercept = obsChange) 8.2.3 The formal hypothesis test The formal test of significance then works by asking how many of the null distribution replicates are as extreme as the observed change. table(nullDist$change&gt;=obsChange) ## ## FALSE TRUE ## 975 25 So we can see that 25 of 1000 replicates were greater than or equal to the observed change. This translates to a p-value of 0.025. We can therefore say that the observed change in weight after CBT was significantly greater than what we would expect from chance. 8.3 Exercise: Sexual selection in Hercules beetles A Hercules beetle is a large rainforest species from South America. Researchers suspect that sexual selection has been operating on the species so that the males are significantly larger than the females. You are given data7 on width measurements in cm of a small sample of 20 individuals of each sex. Can you use your skills to report whether males are significantly larger than females. The data are called herculesBeetle.csv and can be found via the Dropbox link on Blackboard. Follow the following prompts to get to your answer: What is your null hypothesis? What is your alternative hypothesis? Import the data. Calculate the mean for each sex (either using tapply or using dplyr tools) Plot the data as a histogram. Add vertical lines to the plot to indicate the mean values. Now calculate the difference between the mean values using dplyr tools, or tapply. Use sample to randomise the sex column of the data, and recalculate the difference between the mean. Use replicate to repeat this 10 times (to ensure that you code works). When your code is working, use replicate again, but this time with 1000 replicates and pass the results into a data frame. Use ggplot to plot the null distribution you have just created, and add the observed difference. Obtain the p-value for the hypothesis test described above. (1) how many of the observed differences are greater than or equal to the shuffled differences in the null distribution. (2) what is this expressed as a proportion of the number of replicates. Summarise your result as in a report. Describe the method, followed by the result and conclusion. This example is from: https://uoftcoders.github.io/rcourse/lec09-Randomization-tests.html↩︎ "],
["comparing-two-means-with-a-t-test.html", "Chapter 9 Comparing two means with a t-test 9.1 Some theory 9.2 One sample t-test 9.3 Doing it “by hand” - where does the t-statistic come from? 9.4 Paired t-test 9.5 A paired t-test is a one-sample test. 9.6 Two sample t-test 9.7 t-tests are linear models 9.8 Exercise: Sex differences in fine motor skills 9.9 Exercise: Therapy for anorexia 9.10 Exercise: Compare t-tests with randomisation tests", " Chapter 9 Comparing two means with a t-test We will cover the following: One-sample t-test Paired t-test Two-sample t-test (“Welch t-test”) 9.1 Some theory In this theory section I focus on the one-sample t-test, but the concepts apply to the other types of t-test. The one-sample t-test is used to compare the mean of a sample to some fixed value. For example, we might want to compare pollution levels (e.g. in mg/m3) in a sample to some acceptable threshold value to help us decide whether we need to take action to prevent or clean up pollution. One of the assumptions of t-tests (and many other tests/models) is that the distribution of values in the sample of data can be described by a normal distribution. If this assumption is true, you can use these data to estimate the parameters of this sample’s normal distribution: the mean and standard error of the mean. The mean gives an estimate of location, and the standard error of the mean (which is calculated as \\(s/ \\sqrt{n}\\), where s = standard deviation and n = sample size) gives an estimate of precision of this estimate (i.e. how certain is it that the mean value is really where you think it is?) The t-test then works by comparing your estimated distribution with some fixed value. Sometimes you are asking “is my mean different from the value?”, other times you are asking “is my mean less than/greater than the value?”. This depends on the hypothesis. The default that R-uses is that it tests whether the mean of your distribution is different to the fixed value, but in many cases you should really be framing a directional hypothesis. It is helpful to visualise this, so some examples of the pollution threshold test are shown in the figure below (Figure ). The curves illustrate the estimated normal distributions that describe our estimate of the mean pollution level from some data (e.g. each curve might represent samples from different locations). We are interested in whether the mean values (the vertical dashed lines) are significantly greater than the threshold of 100mg/m2 (solid vertical black line) (this gives us a directional hypothesis). Formally we do this by establishing two hypotheses a null hypothesis and an alternative hypothesis. In this case, the null hypothesis is that the mean of the sample measurements is not significantly different from the threshold value we define. The alternative hypothesis is that the sample mean is significantly greater than this threshold value. The degree of confidence that we can have that the mean pollution values are different from the threshold value depend on (A) the position of the distribution relative to the threshold value and (B) on the spread of the distribution (the standard deviation/error). Based on Figure , which of these four different samples shows a mean value significantly greater than 100? (you should be looking at the amount of the normal distribution curve that is overlapping the threshold value.) This should look familiar – it is the same concept as we used in the class on randomisation tests. If you find it confusing, please go back and review the randomisation test materials! Another useful way to think about t-tests is that it is a way of distinguishing between signal and noise: the signal is the mean value of the thing you are measuring, and the noise is the uncertainty in that estimate. This uncertainty could be due to measurement error and/or natural variation. In fact, the t-value that the t-test relies on is a ratio between the signal (difference between mean (\\(\\bar{x}\\)) and threshold (\\(\\mu_{0}\\))) and noise (variability, standard error of the mean (\\(s/ \\sqrt{n}\\))): \\[t = \\frac{\\bar{x}-\\mu_{0}} {s/ \\sqrt{n}}\\] The larger the signal is compared to the noise, the higher the t-value will be. e.g. a t-value of 2 means that the signal was 2 times the variability in the data. A t-value of zero, or close to zero, means that the signal is “drowned out” by the noise. Therefore, high t-values give you more confidence that the difference is true. To know if the t-value means that the difference is significant, the t-value is compared to a known theoretical distribution (the t-distribution). The area under the curve of the distribution is 1, but its shape depends on the degrees of freedom (i.e. sample size - 1). The plot below (Figure ) shows three t-distributions of different degrees of freedom (d.f.). What R is doing when it figures out the p-value is calculating the area under the curve beyond the positive/negative values of the t-statistic. If t is small, then this value is large (p-value). If t is large then the area (and the p-value) is small. In the olden-days (&gt;15 years ago) you would have looked these values up in printed tables, but now R does that for us. 9.2 One sample t-test Enough theory. Here’s how you would apply such a test in R. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. Firstly, lets load some data. Because this is a very small example, you can simply cut and paste the data in rather than loading it from a CSV file. pollution &lt;- data.frame(mgm3 = c(105, 196, 226, 81, 156, 201, 142, 149, 191, 192, 178, 185, 231, 76, 207, 138, 146, 175, 114, 155)) First I plot the data (Figure ). One reason for doing this is to check that the data look approximately normally distributed. These data are slightly left-skewed but they are close enough. ggplot(pollution,aes(x=mgm3))+ geom_histogram(bins=8) + geom_vline(xintercept = 100) Figure 9.1: Histogram of the pollution data. Now we can run a t-test in R like this. The command is simple - the first two arguments are the data (x) and the the fixed value you are comparing the data to. The final argument defines the alternative hypothesis. This can take values of “two.sided”, “less” or “greater” (the default is two.sided). In this example, the alternative hypothesis is that the mean of our sample is greater than the threshold of 100. t.test(x = pollution$mgm3, mu = 100, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: pollution$mgm3 ## t = 6.2824, df = 19, p-value = 2.478e-06 ## alternative hypothesis: true mean is greater than 100 ## 95 percent confidence interval: ## 145.0803 Inf ## sample estimates: ## mean of x ## 162.2 The output of the model tells us (1) what type of t-test is being fitted (“One Sample t-test”). Then it gives some values for the t-statistic, the degrees of freedom and the p-value. The model output also tells us that the alternative hypothesis “true mean is greater than 100”. Because the p-value is very small (p&lt;0.05) we can reject the null hypothesis and accept the alternative hypothesis. Finally, the output gives you the confidence interval (the area where we strongly believe the true mean to lie) and the estimate of the mean. We could report these results like this: \"the mean value of the sample was 162.2 mg/m3, which is significantly greater than the acceptable threshold of 100 mg/m3 (t-test: t = 6.2824, df = 19, p-value = 2.478e-06). 9.3 Doing it “by hand” - where does the t-statistic come from? At this point, to ensure that you understand where the t-statistic comes from we will calculate the t-statistic using the equation from above. The purpose of this is to illustrate that this is not brain surgery - it all hinges on a straightforward comparison between signal (the difference between mean and threshold in this case) and noise (the variation, or standard error of the mean). To do this we first need to know the mean value and the threshold (the signal: \\(\\bar{x} - \\mu_{0}\\)). We can then divide that by the standard error of the mean (the noise: \\(s/ \\sqrt{n}\\)) Here goes… I first create a vector (x) of the values to save typing. Then I show how to calculate mean and standard error, before dividing the “signal” by the “noise”. #First create a vector of the values x &lt;- pollution$mgm3 #mean mean(x) ## [1] 162.2 #standard error of the mean sd(x)/sqrt(length(x)) ## [1] 9.900718 #Putting it all together (mean(x) - 100) / (sd(x) / sqrt(length(x))) ## [1] 6.282373 This matches exactly with the t-statistic above! One can obtain a p-value from a given t-statistic and degrees of freedom like this for a t-test like the one fitted above (the d.f. is the sample size minus one for a one-sample t-test): 1 - pt(6.282373, 19) ## [1] 2.477945e-06 Again, this matches the value from the t.test function above. 9.4 Paired t-test It’s actually quite hard to find examples of one-sample t-tests in biology. In most cases, the one-sample t-tests are really paired t-tests, which are a special case of the one sample test where rather than using the actual values measured, we use the difference between them instead (Figure ). Here’s a simple example. Anthropologists studied tool use in women from the indigenous Machinguenga of Peru8. They estimated the amount of cassava obtained in kg/hour using either a wooden tool (a broken bow) or a metal machete. The study focused on 5 women who were randomly assigned to groups to use the wooden tool then the machete (or vice versa). The anthropologists hypothesised that using different tools led to different harvesting efficiency. The null hypothesis is that there is no difference between the two groups and that a woman was equally efficient at foraging using either tool. The alternative hypothesis was that there is a difference between the two tools. (NOTE - this could also be formulated as a directional hypothesis e.g. with the expectation that machete is more efficient than the bow.) First let’s import and look at the data. Make sure you understand it. A plot will be fairly useless to tell if the data are normally distributed, so we will simply have to assume that they are. In fact, t-tests are famously robust to non-normality. toolUse &lt;- read.csv(&quot;CourseData/toolUse.csv&quot;) toolUse ## subjectID tool amount ## 1 1 machete 119 ## 2 1 bow 39 ## 3 2 machete 216 ## 4 2 bow 114 ## 5 3 machete 240 ## 6 3 bow 150 ## 7 4 machete 129 ## 8 4 bow 51 ## 9 5 machete 137 ## 10 5 bow 60 We should now plot our data (Figure ). A nice way of doing this for paired data is to plot points with lines joining the pairs. This way, the slope of the lines is a striking visual indication of the effect. ggplot(toolUse,aes(x=tool,y=amount,group=subjectID)) + geom_line()+ geom_point() + xlab(&quot;Tool used&quot;) + ylab(&quot;Casava harvested (kg/hr)&quot;) Figure 9.2: An interaction plot for the tool use data We can also look at the mean values and standard deviations: toolUse %&gt;% group_by(tool) %&gt;% summarise(meanAmount = mean(amount),sdAmount = sd(amount)) ## # A tibble: 2 x 3 ## tool meanAmount sdAmount ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bow 82.8 47.3 ## 2 machete 168. 55.6 Now lets do a paired t-test to compare the means using the two tools. There are several ways to do a paired t-test. The first is to use a model formula in the command. The formula takes the form measurements ~ group. You must also specify the name of the data.frame and that the data are paired (paired = TRUE). IMPORTANT: it is very important that the pairs are grouped together in the data frame so that the pairs match up when you filter the data to each group. It is therefore advisable to use arrange to sort the data by first the pairing variable (in this case, subjectID), and then the explanatory variable (the variable that defines the group - in this case, tool. toolUse &lt;- toolUse %&gt;% arrange(subjectID, tool) t.test(amount ~ tool, data = toolUse, paired = TRUE) ## ## Paired t-test ## ## data: amount by tool ## t = -17.98, df = 4, p-value = 5.625e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -98.58738 -72.21262 ## sample estimates: ## mean of the differences ## -85.4 An alternative way is to give the two samples separately in the t.test command. To do this you will need to create two vectors containing the data from the two groups like this, using the dplyr command pull to extract the variable along with filter to subset the data: A &lt;- toolUse %&gt;% filter(tool == &quot;machete&quot;) %&gt;% pull(amount) B &lt;- toolUse %&gt;% filter(tool == &quot;bow&quot;) %&gt;% pull(amount) t.test(A, B, data = toolUse, paired = TRUE) ## ## Paired t-test ## ## data: A and B ## t = 17.98, df = 4, p-value = 5.625e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 72.21262 98.58738 ## sample estimates: ## mean of the differences ## 85.4 You would report these results something like this: “Women harvested cassava more efficiently with a machete (168.2 kg/hr) than with a wooden tool (82.8kg/hr). The difference of 85.4 kg/hr (95% CI 72.2-98.6 kg) was statistically significant (paired t-test: t = 17.98, df = 4, p-value = 5.625e-05).” NOTE: you could add the argument alternative = \"less\" or greater to these t-tests to turn them into directional one-tailed hypotheses. However, you should also be aware that the p-value for a one-tailed t-test is always half that of the two-tailed test. Therefore, you could also simply half the p-value when you report it rather than adding the “alternative” argument. 9.5 A paired t-test is a one-sample test. A paired t-test is the same as a one-sample t-test really. Here’s proof. First we need to calculate the difference between the two measures difference &lt;- A - B Then we can fit the one-sample t-test from above, with the mu set as 0 (because the null hypothesis is that there is no difference between the groups). Compare this result with the paired t-test above. t.test(x = difference, mu = 0) ## ## One Sample t-test ## ## data: difference ## t = 17.98, df = 4, p-value = 5.625e-05 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 72.21262 98.58738 ## sample estimates: ## mean of x ## 85.4 9.6 Two sample t-test The two sample t-test is used for comparing the means of two samples [no shit!? :)] You can visualise this by picturing your two distributions (Figure ) and thinking about their overlap. If they overlap a lot the difference between means will not be significant. If they don’t overlap very much then the difference between means will be significant. The underlying mathematical machinery for the two-sample t-test is similar to the one-sample and paired t-tests. Again, the important value is the t-statistic, which can be thought of as a measure of signal:noise ratio (see above). It is harder to detect a signal (the true difference between means) if there is a lot of noise (the variability, or spread of the distributions), or if the signal is small (the difference between means is small). Figure 9.3: A two-sample t-test. The mathematics involved with calculating the t-statistic is very similar to the one-sample t-test, except the numerator in the fraction is the difference between two means rather than between a mean and a fixed value. \\[t = \\frac{\\bar{x_1}-\\bar{x_2}} {s/ \\sqrt{n}}\\] So far so good… let’s push on and use R to do some statistics. In this example, we can revisit the class data and ask the question, Is the reaction time of males different than that of females? The null hypothesis for this question is that there is no difference in mean reaction times between the two groups. The alternative hypothesis is that there is a difference in the mean reaction time between the two groups. Import the data in the usual way, and subset it to the right year. x &lt;- read.csv(&quot;CourseData/classData.csv&quot;) Then look at the data. Here I do this using a box plot with jittered points (a nice way of plotting data with small sample sizes) (Figure ). From the Figure it looks like males have a faster reaction time than females, but there is a lot of variation. We need to apply the t-test in a similar way to above. ggplot(x,aes(x=Gender,y=Reaction)) + geom_boxplot() + geom_jitter(col=&quot;grey60&quot;,width=0.2) t.test(Reaction ~ Gender, data = x,var.equal=F) ## ## Welch Two Sample t-test ## ## data: Reaction by Gender ## t = 1.6602, df = 28.63, p-value = 0.1078 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.009270184 0.088978588 ## sample estimates: ## mean in group Female mean in group Male ## 0.3460471 0.3061929 This output first tells us that we are using something called a “Welch Two Sample t-test”. This is a form of the two-sample t-test that relaxes the assumption that the variance in the two groups is the same. This is a good thing. Although it is possible to fit a t-test with equal variances, I recommend that you stick with the default Welch’s test and not make this limiting assumption. Then we are told the t-statistic (1.660), the degrees of freedom (28.630) and the p-value (0.108). We must therefore accept the null hypothesis: there is no significant difference between the two groups. Males are not faster than females. We could write report this something like this: “Although females had a slightly slower reaction time than males (0.346 seconds compared to 0.306 seconds), this difference was not statistically significant (Welch t-test: t= 1.660, d.f.= 28.630), p=0.108).” Note: With a t-test that did assume equal variances in the two groups, the d.f. is calculated as the sample size - 2 (the number of groups). You can do this by adding the argument “var.equal = TRUE” to the t-test command. With the Welch test, the appropriate degrees of freedom are estimated by looking at the sample sizes and variances in the two groups. The details of this are beyond the scope of this course. 9.7 t-tests are linear models It is also possible to formulate t-tests as linear models (using the lm function). To do this with the paired t-test you would specify a model that estimates an intercept. In R you can do this by writing the formula as x ~ 1. So, for the tool use example you can write the code like this: mod1 &lt;- lm(difference ~ 1) summary(mod1) ## ## Call: ## lm(formula = difference ~ 1) ## ## Residuals: ## 1 2 3 4 5 ## -5.4 16.6 4.6 -7.4 -8.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.40 4.75 17.98 5.62e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.62 on 4 degrees of freedom If you look at the summary will notice that the estimate of the intercept (the average difference between the two pairs), the degrees of freedom and the t-value and the p-value are all the same as the value reported when using t.test. In fact, all of the t-tests, and ANOVA (below) are kinds linear models and can be also fitted with lm. Here is the paired t-test investigating gender differences in reaction time. You can see that the test statistics and coefficients match those obtained from t.test. mod &lt;- lm(Reaction ~ Gender, data = x) anova(mod) ## Analysis of Variance Table ## ## Response: Reaction ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Gender 1 0.012194 0.0121945 2.5921 0.1182 ## Residuals 29 0.136429 0.0047044 summary(mod) ## ## Call: ## lm(formula = Reaction ~ Gender, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.08705 -0.04712 -0.01479 0.03755 0.22775 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.34605 0.01664 20.80 &lt;2e-16 *** ## GenderMale -0.03985 0.02475 -1.61 0.118 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06859 on 29 degrees of freedom ## Multiple R-squared: 0.08205, Adjusted R-squared: 0.0504 ## F-statistic: 2.592 on 1 and 29 DF, p-value: 0.1182 9.8 Exercise: Sex differences in fine motor skills Some people have suggested that there might be sex differences in fine motor skills in humans. Use the data collected on the class to address this topic using t-tests. The relevant data set is called classData.csv, and the columns of interest are Gender and Precision. Carry out a two-sample t-test. Plot the data (e.g. with a box plot, or histogram) Formulate null and alternative hypotheses. Use the t.test function to do the test. Write a sentence or two describing the results. 9.9 Exercise: Therapy for anorexia A study was carried out looking at the effect of cognitive behavioural therapy on weight of people with anorexia. Weight was measured in week 1 and again in week 8. Use a paired t-test to assess whether the treatment is effective. The data is called anorexiaCBT.csv The data are in “wide format”. You may wish to convert it to “long format” depending on how you use the data. You can do that with the gather function, which rearranges the data: anorexiaCBT_long &lt;- anorexiaCBT %&gt;% gather(&quot;week&quot;,&quot;weight&quot;,-Subject) Plot the data (e.g. with an interaction plot like Figure 5 in the t-tests PDF) Formulate a null and alternative hypothesis. Use t.test to conduct a paired t-test. Write a couple of sentences to report your result. 9.10 Exercise: Compare t-tests with randomisation tests Try re-fitting some of these tests as randomisation tests (or analyse the randomisation test data using t.test). Do they give the same results? Try answering the question - “are people who prefer dogs taller than those who prefer cats?” using the courseData.csv. Can you think of any problems with this analysis? A.M. Hurtado, K. Hill (1989). “Experimental Studies of Tool Efficiency Among Machinguenga Women and Implications for Root-Digging Foragers”, Journal of Anthropological Research, Vol.45,2,pp207-217.↩︎ "],
["linear-models-with-a-single-categorical-explanatory-variable.html", "Chapter 10 Linear models with a single categorical explanatory variable 10.1 One-way ANOVA 10.2 Fitting an ANOVA in R 10.3 ANOVA calculation “by hand”. 10.4 Exercise: Apple tree crop yield", " Chapter 10 Linear models with a single categorical explanatory variable With the previous work on t-tests (and also with randomisation tests), you are now equipped to test for differences between two groups, or between one group and some fixed value. But what if there are more than two groups? The answer is to use a one-way analysis of variance (ANOVA). Conceptually, this works the same way as a t-test. 10.1 One-way ANOVA The one-way ANOVA is illustrated below with two cases (Figure ). In both cases there are three groups. These could represent treatment groups in an experiment (e.g. different fertiliser addition to plants). In figure A, the three groups are very close, and the means are not significantly different from each other. In figure B, there is one group that stands apart from the others. The ANOVA will tell us whether at least one of the groups is different from the others. After figuring out if at least one of the groups is significantly different from the others it is often enough to examine plots (or summary statistics) to see where the differences are (e.g. which group(s) are different from each other). In other cases it might be necessary to do follow up post-hoc multiple comparison tests. We will come to those later. 10.2 Fitting an ANOVA in R New coffee machines use “pods” to make espresso. These have become much more popular than the traditional “bar machines”. This data looks at the amount of “crema” or foam produced (a sign of quality!) using three methods: bar machines (BM), Hyper Espresso Pods (HIP) and Illy Espresso System (IES). Are any of these methods better than the others? Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. Import the data and look at it. espresso &lt;- read.csv(&quot;CourseData/espresso.csv&quot;, stringsAsFactors = TRUE) head(espresso) ## foamIndx method ## 1 36.64 BM ## 2 39.65 BM ## 3 37.74 BM ## 4 35.96 BM ## 5 38.52 BM ## 6 21.02 BM (ggplot(espresso,aes(x=method,y=foamIndx)) + geom_boxplot()+ geom_jitter(width=0.2)) Figure 10.1: A box and whisker plot, with jittered points, for the espresso foam data. You can see that the categorical explanatory variable (“method”) defines the three treatment groups and has the three levels representing the different coffee types: BM, HIP and IES. Let’s first fit the ANOVA using R. One way ANOVAs are fitted using the lm function (lm stands for “linear model” - yes, an ANOVA is a type of linear model). foam_mod &lt;- lm(foamIndx ~ method, data = espresso) Before proceeding, we need to check the assumptions of the model. This can be done visually using the autoplot function in the ggfortify package. If you don’t have the package installed, install it now (install.packages(\"ggfortify\")). library(ggfortify) autoplot(foam_mod) Figure 10.2: Diagnostic plots for the ANOVA model. This looks great. The main thing to look at here in Figure is the “Q-Q” plot on the top right. We want those points to be approximately along the line. If that is the case, then it tells us that the model’s residuals are normally distributed (this is one of the assumptions of ANOVA). We may cover these diagnostic plots more thoroughly later. You can find more details on pages 112-113 of the Beckerman et al textbook, or at the following if you are interested: https://data.library.virginia.edu/diagnostic-plots/. Trust me, everything here looks great. Now let’s evaluate our ANOVA model. We do this using two functions: anova and summary (it sounds strange, but yes we do use a function called anova on our ANOVA model). First, the anova. This gives us the following summary: anova(foam_mod) ## Analysis of Variance Table ## ## Response: foamIndx ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## method 2 4065.2 2032.59 28.413 4.699e-07 *** ## Residuals 24 1716.9 71.54 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This gives some numbers (degrees of freedom, sum of squares, mean squares). These are the important values that go into calculating an F value (also called an F-statistic). We will not worry about these details now, except to say that large F-statistics mean that we are more certain that there is a difference between the groups (and that the p-value is smaller). In this case, the F-value is 28.413. As with the t-test, R compares this value to a theoretical distribution (a “table”), based on two degrees of freedom. The first one is number of groups minus one, i.e. 2.000 in this case. The second one is the overall sample size, minus the number of groups, i.e. 24.000, in this case. This results in a p-value of 0.0000004699 (very highly significant!). Based on this p-value we can reject the null hypothesis that there is no difference between groups. We might report this simple result like this: The foam index varied significantly among groups (ANOVA: F = 28.413, d.f. = 2 and 24, p = 0.000). 10.2.1 Where are the differences? This model output doesn’t tell us where those differences are, nor does it tell us what the estimated mean values of foaminess are for the three groups: what are the effects? We need to dig further into the model to get to these details. There are several ways to do this and we’ll look at one of them. We do this using the summary function. summary(foam_mod) ## ## Call: ## lm(formula = foamIndx ~ method, data = espresso) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.62 -6.60 0.41 5.73 16.49 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.400 2.819 11.492 3.04e-11 *** ## methodHIP 28.900 3.987 7.248 1.73e-07 *** ## methodIES 7.300 3.987 1.831 0.0796 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.458 on 24 degrees of freedom ## Multiple R-squared: 0.7031, Adjusted R-squared: 0.6783 ## F-statistic: 28.41 on 2 and 24 DF, p-value: 4.699e-07 To properly interpret this output you need to understand something called “treatment contrasts”. Essentially, contrasts define how model coefficients (the estimates made by the model) are presented in R outputs. They are a bit hard to wrap your head aroudn and I STRONGLY recommend that you always do this with reference to a plot of the actual data, and the mean values for your groups. To do this you can use group_by and summarise to calculate the means for your the levels of your explanatory variable. espresso %&gt;% group_by(method) %&gt;% summarise(gp_mean = mean(foamIndx)) ## # A tibble: 3 x 2 ## method gp_mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 BM 32.4 ## 2 HIP 61.3 ## 3 IES 39.7 Look at the coefficients of the model. Remember that you have three levels in your explanatory variable, but only two levels are shown in the summary. Which one is missing? The “missing” group is the first one alphabetically (i.e. BM). The estimate (of the mean) for this group is labelled “(Intercept)” (with a value of 32.4. This is like a baseline or reference value, and the estimates for the other groups (HIP and IES), are differences between this baseline value and the estimated mean for those groups. In other words, the second group (HIP) is 28.9 more than 32.4 (32.4 + 28.9 = 61.3). Similarly, the third group (IES) is 7.3 more than 32.4 (32.4 + 7.3 = 39.7). Compare these values with the ones you got above using summarise - they should be the same. This is illustrated below in Figure A. You can see that the coefficients of the model are the same as the lengths of the arrows that run from 0 (for the first level of method (BM), the Intercept) or from this reference value. It is often a good idea to sketch something like this on paper when you are trying to understand your model outputs! Likewise, the t-values and p-values, are evaluating differences between the focal group and this baseline. Thus in this case, the comparisons (the “contrasts”) are between the intercept (BM) and the second level (HIP), and the intercept (BM) and the third level (IES). There is no formal statistical comparison between HIP and IES. You can see that it is very important to understand the levels of your explanatory variable, and how these relate to the summary outputs of the model. It can be useful to use the function relevel to manipulate the explanatory variable to make sure that the output gives you the comparisons you are interested in. Another simple trick would be to always ensure that your reference group (e.g. “control”) comes first alphabetically and is therefore selected by R as the intercept (reference point). For example, we can relevel the method variable so that the levels are re-ordered as HIP, BM, then IES so that the comparisons are between zero-HIP, HIP-BM and HIP-IES. (make sure that you understand this before proceeding). #This is what the original data looks like: levels(espresso$method) ## [1] &quot;BM&quot; &quot;HIP&quot; &quot;IES&quot; #releveling changes this by changing the reference. espresso_2 &lt;- espresso %&gt;% mutate(method = relevel(method,ref = &quot;HIP&quot;)) levels(espresso_2$method) ## [1] &quot;HIP&quot; &quot;BM&quot; &quot;IES&quot; Now we can refit the model with this modified data set and see what difference that made: foam_mod2 &lt;- lm(foamIndx ~ method, data = espresso_2) anova(foam_mod2) ## Analysis of Variance Table ## ## Response: foamIndx ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## method 2 4065.2 2032.59 28.413 4.699e-07 *** ## Residuals 24 1716.9 71.54 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(foam_mod2) ## ## Call: ## lm(formula = foamIndx ~ method, data = espresso_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.62 -6.60 0.41 5.73 16.49 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 61.300 2.819 21.743 &lt; 2e-16 *** ## methodBM -28.900 3.987 -7.248 1.73e-07 *** ## methodIES -21.600 3.987 -5.417 1.45e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.458 on 24 degrees of freedom ## Multiple R-squared: 0.7031, Adjusted R-squared: 0.6783 ## F-statistic: 28.41 on 2 and 24 DF, p-value: 4.699e-07 Now the coefficients in the model summary look different, but the model is actually the same. Compare the two graphs in Figure - can you see the differences/similarities? Figure 10.3: Comparison illustrating the difference between ANOVA models using (A) BM and (B) HIP as references in the espresso foam data set. So, from the first of the model outputs above you can say that BM is significantly different than HIP (t= 7.248, p &lt; 0.0001), but that BM is not significantly different from IES (t = 1.831, p = 0.0796). Then, from the second one you can see that HIP is significantly different from IES (t=-5.417, p &lt; 0.0001). You could write this into the main text, include the information in a figure caption (i.e. add it to Figure . e.g. The foam index varied significantly among groups (ANOVA: F = 28.413, d.f. = 2 and 24, p = 0.000). The pairwise comparisons in the ANOVA model showed that means of BM and HIP were significantly different (t= 7.248, p &lt; 0.0001), as were those of HIP and IES (t=-5.417, p &lt; 0.0001), but the BM-IES comparison showed no significant difference (t= 1.831, p= 0.0796). 10.2.2 Tukey’s Honestly Significant Difference (HSD) An alternative way to approach these comparisons between groups is to use something called a post-hoc multiple comparison test. The words “post-hoc” mean “after the event” – i.e. after the ANOVA in this case – while the “multiple comparison” refers to the (potentially many) pairwise comparisons that you would like to make with an ANOVA. One of the most widely used post-hoc tests is called Tukey’s Honestly Significant Difference (Tukey HSD) test. There is a convenient R package called agricolae that will do these for you. #You only need to do this once! install.packages(&quot;agricolae&quot;) When you have the package installed, you can load it (using library). Then you can run the Tukey HSD test using the function HSD.test. The first argument for the function is the name of the model, followed by the name of the variable you are comparing (in this case method) and finally console = TRUE tells the function to print the output to your computer screen. library(agricolae) HSD.test(foam_mod2, &quot;method&quot;, console=TRUE) ## ## Study: foam_mod2 ~ &quot;method&quot; ## ## HSD Test for foamIndx ## ## Mean Square Error: 71.5383 ## ## method, means ## ## foamIndx std r Min Max ## BM 32.4 7.300060 9 21.02 39.65 ## HIP 61.3 10.100604 9 46.68 73.19 ## IES 39.7 7.700768 9 32.68 56.19 ## ## Alpha: 0.05 ; DF Error: 24 ## Critical Value of Studentized Range: 3.531697 ## ## Minimun Significant Difference: 9.957069 ## ## Treatments with the same letter are not significantly different. ## ## foamIndx groups ## HIP 61.3 a ## IES 39.7 b ## BM 32.4 b The output is long-winded, and the main thing to look at is the part at the end. The key to understanding this is actually written in the output, “Treatments with the same letter are not significantly different”. You could include these in a figure or a table with text like, “Means followed by the same letter did not differ significantly (Tukey HSD test, p&gt;0.05)”. 10.3 ANOVA calculation “by hand”. The following is an optional exercise designed to help you understand how ANOVA works. The ANOVA calculation involves calculating something called an F-value or F-statistic (the F stands for Fisher, who invented ANOVA). This is similar to the t-statistic in that it is a ratio between two quantities, in this case variances. In ANOVA, the F-statistic is calculated as the treatment variance divided by the error variance. What does that mean? Let’s consider the espresso data set again. In the Figure , you can see on the left (A) the black horizontal line which is the overall mean foam index. The vertical lines are the “errors” or departures from the overall mean value, colour coded by treatment (i.e. method). These can be quantified by summing up their square values (squaring ensures that the summed values are all positive). We call this quantity the Total Sum of Squares (SSTotal). If there is a lot of variation, the sum of squares will be large, if there is little variation the sum of squares will be small. On the right hand side (Figure B) we see the same data points. However, this time the horizontal lines represent the treatment-specific mean values, and the vertical lines illustrate the errors from these mean values. Again we can sum up these as sum of squares, which we call the Error Sum of Squares (SSError). The difference between those values is called the Treatment Sum of Squares (SSTreatment) and is the key to ANOVA - it represents the importance of the treatment: \\[SSTreatment = SSTotal - SSError\\]. If that doesn’t make sense yet, picture the case where the treatment-specific means are all very similar, and are therefore very close to the overall mean. Now the difference between the Total Sum of Squares and the Error Sum of Squares will be small. Sketch out a couple of examples with pen on paper if that helps. You should now see that you can investigate differences among means by looking at variation. Figure 10.4: The relative size of the squared residual errors from the overall mean (SSTotal) (A) and from the treatment-specific means (SSError) (B) tell us about the importance of the treatment variable. The difference between the two values is the “treatment sum of squares”. In the following I will show how these calculations can be done “by hand” in R. The purpose of showing you this is to demonstrate exactly how the lm model that you fitted above works, and prove to yourself that it is not rocket science… you will never need to do this in real life, because you have the wonderful lm function. Here goes… First, calculate the total sum of squares: (SSTotal = sum((overallMean-espresso$foamIndx)^2)) ## [1] 5782.099 Now calculate the group-specific means: (groupMeans&lt;-espresso %&gt;% group_by(method) %&gt;% summarise(groupMean = mean(foamIndx)) %&gt;% pull(groupMean)) ## [1] 32.4 61.3 39.7 Now add those group-specific mean values to the dataset using left_join so that you can calculate the group-specific errors. espresso &lt;- left_join(espresso,espresso %&gt;% group_by(method) %&gt;% summarise(groupMean = mean(foamIndx))) %&gt;% mutate(groupSS = (foamIndx - groupMean)^2) head(espresso) ## foamIndx method groupMean groupSS ## 1 36.64 BM 32.4 17.9776 ## 2 39.65 BM 32.4 52.5625 ## 3 37.74 BM 32.4 28.5156 ## 4 35.96 BM 32.4 12.6736 ## 5 38.52 BM 32.4 37.4544 ## 6 21.02 BM 32.4 129.5044 Then, to calculate the errors: (SSError &lt;- espresso %&gt;% summarise(sum(groupSS)) %&gt;% pull()) ## [1] 1716.919 From there, you can calculate the Treatment Sum of Squares (SSTreatment &lt;- SSTotal - SSError) ## [1] 4065.18 So far, so good - but we can’t just look at the ratio of SSTreatment/SSError, because sum of square errors always increase with sample size. We can account for this by dividing We need to take account of sample size (degrees of freedom) by dividing these sum of squares by the degrees of freedom to give us variances. There are 3 treatment groups and 9 samples per group. Therefore there are 2 degrees of freedom for the treatment, and 8 degrees of freedom per each of the three treatments, giving a total of 8*3 = 24 error degrees of freedom. Now we need to correct for degrees of freedom, which will give us variances. (meanSSTreatment &lt;- SSTreatment/2) ## [1] 2032.59 (meanSSError &lt;- SSError/24) ## [1] 71.5383 The F-statistic is then the ratio of these values. (Fstat &lt;- meanSSTreatment/meanSSError) ## [1] 28.41261 We can “look up” the p-value associated with this F-statistic using the pf function (pf stands for probability of f) like this: 1-pf(Fstat,df1=2,df2=24) ## [1] 4.698636e-07 As you can see, the method is a bit laborious and time consuming but it is conceptually fairly straightforward - it all hinges on the ratio of variation due to treatment effect vs. overall variation. Signal and noise. 10.4 Exercise: Apple tree crop yield An experiment was conducted to look at the effect of tree spacing on apple crop yield (total kg per tree of apples between 1975-1979) in different spacing conditions (i.e. distances between trees). There were 40 trees in each treatment group. The spacing was 6, 10 and 14 feet, and should be treated as a categorical variable. There may be some NA missing yield values. Import the data (apples.csv) and analyse it using the techniques you have learned in the ANOVA lecture, and the previous chapter, to answer the question “What is the effect of tree spacing on apple yields?” Import and look at the data (e.g. summary or str or head) Ensure that the explanatory variable (spacing) is defined as a categorical variable (i.e. a “factor”, in R-speak). You can use mutate and as.factor functions for this. Plot the data using ggplot (a box plot with (optionally) added jittered points would be good). Fit an ANOVA model using lm. Check the model using a diagnostic plot (i.e. using autoplot from the ggfortify package). Use anova to get the ANOVA summary. You should see that there are differences among treatments. But where are those differences? Use summary on your model to find out. Use a Tukey test to make all the pairwise comparisons among the treatments. Write a few sentences that summarise your findings. What biological processes do you think drive the effects that you have detected? Optional. Instead of using a Tukey test, use the alternative “relevel” approach to make the missing comparison. If you get this far, try using the ANOVA approach on one of the previous t-test examples (remember that ANOVA can be used when your single explanatory variable has TWO or more levels). You should notice that the results are the same whether you use the t.test function or the ANOVA approach with lm. "],
["linear-models-with-a-single-continuous-explanatory-variable.html", "Chapter 11 Linear models with a single continuous explanatory variable 11.1 Some theory 11.2 Evaluating a hypothesis with a linear regression model 11.3 Assumptions 11.4 Worked example: height-hand width relationship 11.5 Exercise: Chirping crickets", " Chapter 11 Linear models with a single continuous explanatory variable Linear regression models, at their simplest, are a method of estimating the linear (straight line) relationships between two continuous variables. As an example, picture the relationship between two variables height and hand width (Figure ). In this figure there is a clear relationship between the two variables, and the straight line running through the cloud of data points is the fitted linear regression model. The aim of linear regression is to (1) determine if there is a meaningful statistical relationship between the explanatory variable(s) and the response variable, and (2) to quantify those relationships by estimating the characteristics of those relationships. These characteristics include the slope and intercepts of fitted models, and the amount of variation explained by variables in the model. Figure 11.1: A linear regression model fitted through data points. 11.1 Some theory To understand linear regression models it is important to know that the equation of a straight line is \\(y = ax+b\\). In this equation, \\(y\\) is the response variable and \\(x\\) is the explanatory variable, and \\(a\\) and \\(b\\) are the slope and intercept of the line with the vertical axis (y-axis). These (\\(a\\) and \\(b\\)) are called coefficients. These are illustrated in Figure . Figure 11.2: The equation of straight lines. When looking at data points on a graph, unless all of the data points are arranged perfectly along a straight line, there will be some distance between the points and the line. These distances, measured parallel to the vertical axis, are called residuals (you have encountered them before in this course). These residuals represent the variation left after fitting the line (a linear model) through the data. Because we want to fit a model that explains as much variation as possible, it is intuitive that we should wish to minimise this residual variation. One way of doing this is by minimising the sum of squares of the residuals (again, you have come across this concept a few times before). In other words, we add up the squares of each of the residuals. We square the values, rather than simply adding up the residuals themselves because we want to ensure that the positive and negative values don’t cancel each other out (a square of a negative number is positive). This method is called least squares regression and is illustrated in Figure : Which is the best fitting line? Figure 11.3: Residuals and least squares: which is the best fitting line? In fact, these residuals represent “error” caused by factors including measurement error, random variation, variation caused by unmeasured factors etc. This error term is given the label, \\(\\epsilon\\). Thus we can write the model equation as: \\[y = ax+b+\\epsilon\\] Sometimes, this equation is written with using the beta symbol (\\(\\beta\\)) for the coefficients, so that the slope is \\(\\beta_0\\) and the intercept is \\(\\beta_1\\) for example. \\[y = \\beta_0 x+\\beta_1+\\epsilon\\] The idea is that this equation, and its coefficients and error estimates, describe the relationship we are interested in (including the error or uncertainty). Together this information allows us to not only determine if there is a statistically significant relationship, but also what the nature of the relationship is, and the uncertainty in our estimates. 11.2 Evaluating a hypothesis with a linear regression model Usually, the most important hypothesis test involved with a linear regression model relates to the slope: is the slope coefficient significantly different from 0?, or should we accept the null hypothesis that the slope is no different from 0. Sometimes hypotheses like this are a bit boring, because we already know the answer before collecting and analysing the data. What we usually don’t know is the nature of the relationship (the slope, intercept, their errors, and amount of variation explained). Usually it is more interesting and meaningful to focus on those details. The following example, where we focus on the relationship between hand width and height, is one of these “boring” cases: we already know there is a relationship. Nevertheless, we’ll use this example because it helps us understand how this hypothesis test works. The aim of this section is to give you some intuition on how the hypothesis test works. We can address the slope hypothesis by calculating an F-value in a similar way to how we used them in ANOVA. Recall that F-values are ratios of variances. To understand how these work in the context of a linear regression we need to think clearly about the slope hypothesis: The null hypothesis is that the slope is not significantly different to 0 (that the data can be explained by random variation). The alternative hypothesis is that the slope is significantly different from 0. The first step in evaluating these hypotheses is to calculate what the total sum of squares9 is when the null hypothesis is true (Figure A) - this value is the total variation that the model is trying to explain. Then we fit our model using least squares and figure out what the residual sum of squares is from this model (Figure B). This is the amount of variation left after the model has explained some of the total variation - it is sometimes called residual error, or simply error. The difference between these two values is the explained sum of squares, which measures the amount of variation in \\(y\\) explained by variation in \\(x\\). The rationale for this is that the model is trying to explain total variation. After fitting the model there will always be some unexplained variation (“residual error”) left. If we can estimate total variation and unexplained variation, then the amount of variation explained can be calculated with a simple subtraction: \\[Total = Explained + Residual\\] … and, therefore … \\[Explained = Total - Residual\\] Before using these values we need to standardise them to control for sample size. This is necessary because sum of squares will always increase with sample size. We make this correction by dividing our sum of squares measures by the degrees of freedom. The d.f. for the explained sum of squares is 1, and the d.f. for the residual sum of squares is the number of observations minus 2. The result of these calculations is the mean explained sum of squares (mESS) and the mean residual sum of squares (mRSS). These “mean” quantities are variances, and the ratio between them gives us the F-value. Notice that this is very similar to the variance ratio used in the ANOVA. \\[F = \\frac{mESS}{mRSS}\\] If the explained variance (mESS) is large compared to the residual error variance (mRSS), then F will be large. The size of F tells us how likely or unlikely it is that the null hypothesis is true. When F is large, the probability that the slope is significantly different from 0 is high. To obtain the actual probabilities, the F-value must be compared to a theoretical distribution which depends on the two degrees of freedom (explained and residual d.f.). Once upon a time you would have looked this up in a printed table, but now R makes this very straightforward. Figure 11.4: (A) the total variation around the overall mean Height value (B) the residual error of the model. 11.3 Assumptions These models have similar assumptions to the other linear models10. These are (1) that the relationship between the variables is linear (hence the name); (2) that the data are continuous variables; (3) that the observations are randomly sampled; (4) that the errors in the model (the “residuals”) can be described by a normal distribution; and (5) and that the errors are “homoscedastic” (that they are constant through the range of the data). You can evaluate these things by looking at diagnostic plots after you have fitted the model. See page 112-113 in GSWR for a nice explanation. 11.4 Worked example: height-hand width relationship Let’s now use R to fit a linear regression model to estimate the relationship between hand width and height. One application for such a model could be to predict height from a hand print, for example left at a crime scene. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. First, load the data: classData &lt;- read.csv(&quot;CourseData/classData.csv&quot;) We should then plot the data to make sure it looks OK. ggplot(classData,aes(x=HandWidth,y=Height)) + geom_point() This looks OK, and the relationship looks fairly linear. Now we can fit a model using the lm function (same as for ANOVA!).11 The response variable is always the one we would like to predict, in this case Height. The explanatory variable (sometimes called the predictor) is HandWidth. These are added to the model using a formula where they are separated with the ~ (“tilde”) symbol: Height ~ HandWidth. In the model expression, we also need to tell R where the data are using the data = argument. We can save the model as an R object by naming it e.g. mod_A &lt;-. mod_A &lt;- lm(Height ~ HandWidth, data = classData) Before proceeding further we should evaluate the model using a diagnostic plot. We can do this using the autoplot function in the ggfortify package (you may need to install and/or load the package). library(ggfortify) autoplot(mod_A) These diagnostic plots allow you to check that the assumptions of the model are not violated. On the left are two plots which (more or less) show the same thing. They show how the residuals (the errors in the model) vary with the predicted value (height). Looking at the plots allows a visual test for constant variance (homoscedasticity) along the range of the data. In an ideal case, there should be no pattern (e.g. humps) in these points. On the top right is the QQ-plot which shows how well the residuals match up to a theoretical normal distribution. In an ideal case, these points should line up on the diagonal line running across the plot. The bottom right plot shows “leverage” which is a measure of how much influence individual data points have on the model. Outliers will have large leverage and can mess up your model. Ideally, the points here should be in a cloud, with no points standing out from the others. Please read the pages 112-113 in the textbook GSWR for more on these. In this case, the model looks pretty good. Now that we are satisfied that the model doesn’t violate the assumptions we can dig into the model to see what it is telling us. To test the (slightly boring) slope hypothesis we use the anova function (again, this is the same as with the ANOVA). anova(mod_A) ## Analysis of Variance Table ## ## Response: Height ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HandWidth 1 1038.5 1038.46 28.397 1.017e-05 *** ## Residuals 29 1060.5 36.57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 When you run this function, you get a summary table that looks exactly like the one you got with an ANOVA. There are degrees of freedom (Df), Sums of Squares (Sum Sq), Mean Sums of Squares (Mean Sq) and the F value and p-value (Pr(&gt;F)). The most important parts of this table are the F value (28.3968) and the p-value (0.00001017): as described above, large F values lead to small p-values. This tells us that it is VERY unlikely that the null hypothesis is true and we should accept the alternative hypothesis (that height is associated with hand width) We could report the results of this hypothesis test like this: There was a statistically significant association between hand width and height (F = 28.3968, d.f. = 1,29, p &lt; 0.001) Now we can dig deeper by asking for a summary of the model. summary(mod_A) ## ## Call: ## lm(formula = Height ~ HandWidth, data = classData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.498 -3.748 -0.188 5.502 8.812 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 133.9464 7.7862 17.203 &lt; 2e-16 *** ## HandWidth 4.6552 0.8736 5.329 1.02e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.047 on 29 degrees of freedom ## Multiple R-squared: 0.4947, Adjusted R-squared: 0.4773 ## F-statistic: 28.4 on 1 and 29 DF, p-value: 1.017e-05 This summary has a lot of information. First we see Call which reminds us what the formula we have used to fit the model. Then there is some summary information about the residuals. Ideally these should be fairly balanced around 0 (i.e. the Min value should be negative but with the same magnitude as Max). If they are wildly different, then you might want to check the data or model. In this case they look OK. Then we get to the important part of the table - the Coefficients. This lists the coefficients of the model and shows first the Intercept and then the slope, which is given by the name of the explanatory variable (HandWidth here). For each coefficient we get the Estimate of its value, and the uncertainty in that estimate (the standard error (`Std. Error)). These estimates and errors are each followed by a t value and a p-value (Pr(&gt;|t|)). These values provide a test of whether the slope/intercept is different from zero. In this case they both are. The t-tests are indeed doing t-tests of these estimates, in the same way that a regular t-test works, so that the significance depends on the ratio between signal (the estimate) and the noise (the standard error). This is illustrated for the coefficient estimates for our model in Figure . Figure 11.5: Illustration of the coefficient estimates for our model. The peak of the distribution is at the coefficient estimate, and the spread of the distribution indicates the standard error of the mean for the estimate. The statistical significance of the coefficient is determined by the degree of overlap with 0. The summary then gives some information about the amount of residual variation left after the model has been fitted (this is the \\(\\epsilon\\) term in the equations at the start of this chapter). Then we are told what the \\(R^2\\) value is 0.4947. The adjusted \\(R^2\\) is for use in multiple regression models, where there are many explanatory variables and should not be used for this simple regression model. So what does \\(R^2\\) actually mean? \\(R^2\\) is the square of the correlation coefficient \\(r\\) and is a measure of the amount of variation in the response variable (Height) that is explained by the model. If all the points were sitting on the regression line, the \\(R^2\\) value would be 1. This idea is illustrated in Figure . We could describe the model like this: There is a statistically significant association between hand width and height (F = 28.3968, d.f. = 1,29, p &lt; 0.001) The equation of the fitted model is: Height = 4.66(\\(\\pm\\) 0.87) \\(\\times\\) HandWidth + 133.95(\\(\\pm\\) 7.79). The model explains 49% of the variation in height (\\(R^2\\) = 0.495). … or maybe, The model, which explained 49% of the variation in height, showed that the slope of the relationship between hand width and height is 4.66 \\(\\pm\\) 0.87 which is significantly greater than 0 (t = 17.20, p &lt; 0.01) Figure 11.6: An illustration of different R-squared values. A plot is usually a good idea because it is easier for the reader to interpret than an equation, or coefficients. The ggplot2 package has a neat and simple function called geom_smooth which will add the fitted regression line to simple models like this. For linear regression models you simply need to tell it to use method = \"lm\". This will plot the fitted regression model, and will add, by default\" a shaded “ribbon” which represents the so called “95% confidence interval” for the fitted values. These are 2 time the standard error. ggplot(classData,aes(x = HandWidth,y = Height)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Question: If police find the 9.8cm wide hand print at a crime scene, what is your best guess of the height of the person involved? 11.5 Exercise: Chirping crickets Male crickets produce a “chirping” sound by rubbing the edges of their wings together: the male cricket rubs a sharp ridge on his wing against a series ridges on the other wing. In a 1948 study on striped ground cricket (Allonemobius fasciatus), the biologist George W. Pierce recorded the frequency of chirps (vibrations per second) in different temperature conditions. Crickets are ectotherms so their physiology and metabolism is influenced by temperature. We therefore believe that temperature might have an effect on their chirp frequency. The data file chirps.csv contains data from Pierce’s experiments. Your task is to analyse the data and find (1) whether there is a statistically significant relationship between temperature and chirp frequency and (2) what that relationship is. The data has two columns - chirps (the frequency in Hertz) and temperature (the temperature in Fahrenheit). You should express the relationship in Celsius. Import the data Use mutate to convert Fahrenheit to Celsius (Google it) Plot the data Fit a linear regression model with lm Look at diagnostic plots to evaluate the model Use anova to figure out if the effect of temperature is statistically significant. Use summary to obtain information about the coefficients and \\(R^2\\)-value. Summarise the model in words. Add model fit line to the plot. Can I use cricket chirp frequency as a kind of thermometer? sum of squares is simply a way to estimate variation.↩︎ t-tests, ANOVA and linear regression are all types of linear model, mathematically↩︎ R knows that this is a linear regression rather than an ANOVA because the explanatory variable is numeric rather than categorical - smart!.↩︎ "],
["linear-models-with-categorical-and-continuous-explanatory-variables.html", "Chapter 12 Linear models with categorical and continuous explanatory variables 12.1 The height ~ hand width example. 12.2 Summarising with anova 12.3 The summary of coefficients (summary) 12.4 Simplifying the model", " Chapter 12 Linear models with categorical and continuous explanatory variables In the previous chapter we looked at linear models where there is a continuous response variable and two categorical explanatory variables (we call this type of linear model two-way ANOVA). In this chapter we will look at linear models where the explanatory variables are both continuous and categorical. You can think of these as a kind of cross between ANOVA and linear regression. These type of models are often given the name “ANCOVA” or Analysis of Covariance. In a simple case, you might be interested in a model with a continuous response variable (e.g. height) and continuous and a categorical explanatory variables (e.g. hand width and gender). The categorical variable may have any number of levels, but the simplest case is with two (e.g. gender with male and female levels). Some of these different possible outcomes of this type of analysis are illustrated in Figure . We might see that neither of the two explanatory variables has a significant effect. We might see that one of them does but not the other one. We might see an interaction effect (where the effect of one variable (e.g. hand width) depends on the other (e.g gender). We might also see an interaction effect but no main effect. Figure 12.1: Some potential results of the experiment. There may be a significant effect (or not) of both of the main effects (diet and genotype) and there may be a significant interaction effect (or not). 12.1 The height ~ hand width example. In a previous class (linear regression) you explored the relationship between hand width and height. The aim there was (1) to determine if the relationship (i.e. the slope) was significantly different from 0. and (2) to make an estimate of what the equation of the relationship would be so you could make predictions of height from hand width. Here we will extend that example by asking whether there are differences between males and females. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. We’ll begin by plotting the data (Figure ). classData &lt;- read.csv(&quot;CourseData/classData.csv&quot;) (A&lt;-ggplot(classData,aes(x = HandWidth,y = Height,colour = Gender)) + geom_point() + geom_smooth(method =&quot;lm&quot;,se = FALSE)) #This shows the ANCOVA model Figure 12.2: ANCOVA on hand width vs. height data in males and females #before we have even fit it! You can see that our two continuous variables, Height (the response variable) and HandWidth (one of the explanatory variables) are associated: There is an overall positive relationship between HandWidth and Height You can also see that Gender (the categorical explanatory variable) is important: males tend to be taller than females for any given hand width. For example, a female with hand width of 9cm is ~172cm tall while a male would be about 180cm tall. This shows us that males have a higher intercept than females. There is also a slight difference in the slope of the relationship, with males having a slightly steeper slope than females. We already know that the overall relationship between hand width and height is significant (from the linear regression chapter). These new observations leave us with the following additional questions: (1) are the intercepts for males and females significantly different? (2) are the slopes for males and females significantly different (or would a model with a single slope, but different intercepts be better)? Now we can fit our model using the lm function. The model formula is Height ~ HandWidth + Gender + HandWidth:Gender. The HandWidth and Gender are the so called main effects while HandWidth:Gender represents the interaction between them (i.e. it is used to address the question “does the effect of hand width differ between the sexes?”). R knows that is fitting an ANCOVA type model rather than a two-way ANOVA because it knows the type of variables that it is dealing with. You can see this if you ask R to tell you what the class of the variables are: class(classData$Gender) ## [1] &quot;character&quot; class(classData$HandWidth) ## [1] &quot;numeric&quot; mod_A &lt;- lm(Height ~ HandWidth + Gender + HandWidth:Gender, data = classData) The first step should, as before, be to check out the diagnostic plots. We should not read to much into these in this case, because we have a small sample size. Nevertheless, lets keep with good habits: library(ggfortify) autoplot(mod_A) These look good. No evidence of non-normality in the residuals, no heteroscedasticity and no weird outliers. 12.2 Summarising with anova Now we can get the anova table of our ANCOVA model (yes, I know that sounds strange). anova(mod_A) ## Analysis of Variance Table ## ## Response: Height ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HandWidth 1 1038.46 1038.46 40.8621 7.576e-07 *** ## Gender 1 369.46 369.46 14.5378 0.0007246 *** ## HandWidth:Gender 1 4.89 4.89 0.1922 0.6645538 ## Residuals 27 686.17 25.41 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This type of sequential sum of squares Analysis of Variance table should be getting fairly familiar to you now, but let’s unpack what this means. There are four rows in the summary table - one for each of the terms in the model (HandWidth, Gender and HandWidth:Gender), and one for the Residuals (the unexplained variation that remains after fitting the model). The table includes degrees of freedom (Df), sum of squares (Sum Sq), mean sum of squares (Mean Sq) and the associated F and p-values (F value and Pr(&gt;F). You can interpret the mean sum of squares column in terms of the amount of variation in the response variable (Height) that is explained by the term: The table first tells us the amount of variation (in terms of Mean Sum of Squares) in Height that is captured by a model that includes a common slope for both genders (1038.46). Then it tells us that an additional bit of variation (369.46) is captured if we allow the intercepts to vary with gender. Then it tells us that a small additional amount of variation is explained by allowing the slope to vary between the genders (4.89). Finally, there is a bit of unexplained variation left over (Residuals) (25.41). So you can see that hand width explains most variation, followed by gender, followed by the interaction between them. You would report from this table something like this: Hand width and gender both explain a significant amount of the variation in height (ANCOVA - Handwidth: F = 40.862, d.f. = 1 and 27, p&lt;0.001; Gender: F = 14.538, d.f. = 1 and 27, p&lt;0.001). The interaction effect was not significant, which means that the slopes of the relationship between hand width and height are not significantly different (ANCOVA - F = 0.192, d.f. = 1 and 27, p = 0.665). It is of course useful to take the interpretation a bit further. You could do this with reference to the plot - e.g. Figure X shows the clear positive relationship between hand width and height and shows that the intercept for females is smaller than that for males. This which means that, for a given hand width, males tend to be taller. 12.3 The summary of coefficients (summary) To put some quantitative numbers on this description of the pattern we need to get the summary from R. summary(mod_A) ## ## Call: ## lm(formula = Height ~ HandWidth + Gender + HandWidth:Gender, ## data = classData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.480 -4.096 1.845 3.195 7.821 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 154.9285 10.3206 15.012 1.26e-14 *** ## HandWidth 1.7667 1.2707 1.390 0.176 ## GenderMale 1.3710 18.4712 0.074 0.941 ## HandWidth:GenderMale 0.8839 2.0160 0.438 0.665 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.041 on 27 degrees of freedom ## Multiple R-squared: 0.6731, Adjusted R-squared: 0.6368 ## F-statistic: 18.53 on 3 and 27 DF, p-value: 9.892e-07 This summary table gives the coefficients of the statistical model, their standard errors, and the t-test results of whether the estimate is greater than 0. This is the same as the summary tables given for ANOVA and linear regression. In the ANOVA summary tables, the estimates were given in relation to the reference level – the (Intercept) and these ANCOVA summary tables are no difference. Interpreting is best done with reference to the graph of the data and fitted model outputs (the graph above). The reference level (the (Intercept)) is the intercept for the line for the first level of the categorical variable (Females, in this case). Here the model estimates that the intercept for Females is at 154.9 (i.e. if you extended the line out to the left it would eventually cross the y-axis at this point). The next coefficient HandWidth is the slope of this Female line (1.7667). Then we have GenderMale: this coefficient is the difference in intercept between the Female and Male lines. This is followed by the intercept for the interaction term HandWidth:GenderMale: this is the difference between slopes for the two genders. We can therefore do some simple arithmetic to get the equations (i.e. slopes and intercepts) of the lines for both genders. For females this is easy (they are reference level, so you can just read the values directly from the table) - the intercept is 154.93 and the slope is 1.77. For males the intercept is 154.93 + 1.37 = 156.30. The slope is 1.77 + 0.88 = 2.65 We could add these equations to our reporting of the results. Figure X shows the clear positive relationship between hand width and height and shows that the intercept for females is smaller than that for males. This which means that, for a given hand width, males tend to be taller. The model fit for males is Height = 2.65\\(\\times\\)HandWidth + 156.30 and the fit for females is Height = 1.77\\(\\times\\)HandWidth + 154.93 You could check these by using geom_abline to add lines with those equations to the plot (just as a “sanity check”). A + geom_abline(intercept = 154.93, slope = 1.77) + geom_abline(intercept = 156.30, slope = 2.65) At the bottom of the summary output we are given the \\(R^2\\) values. Because this model has several terms (i.e. variables) in it we should use the adjusted \\(R^2\\) values. These have been corrected for the fact that the model has extra explanatory variables. So in this case, we could report that the model explains 64% of variation in Height (Adjusted \\(R^2\\) = 0.6368) - not bad! So, to describe this summary table more generally - the coefficients can be slopes, intercepts, differences between slopes, and differences between intercepts. They are slopes and intercepts for the first level of the categorical variable, and for the subsequent levels they are differences. Piecing these together can be hard to figure out without reference to the plot of the data and model fits - another good reason to plot your data! 12.4 Simplifying the model Our results above showed that the interaction between the gender and hand width was not significant. Think about what that means? It means that the effect of hand width on height (the slope) does not depend on gender. Therefore, one can argue that we don’t need to have a model that estimates both slopes - we could have a simpler model with one slope for both genders. In fact, creating models that are as simple as possible to explain the observations is a useful goal that is captured by the law of parsimony or “Occam’s razor”, which essentially states that simple explanations for a phenomenon are favorable to complex explanations. Let’s refit the model without this non-significant interaction: mod_B &lt;- lm(Height ~ HandWidth + Gender, data = classData) anova(mod_B) ## Analysis of Variance Table ## ## Response: Height ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HandWidth 1 1038.46 1038.46 42.076 4.992e-07 *** ## Gender 1 369.46 369.46 14.970 0.0005963 *** ## Residuals 28 691.05 24.68 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now all the terms in the model are significant. summary(mod_B) ## ## Call: ## lm(formula = Height ~ HandWidth + Gender, data = classData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.613 -3.797 1.446 3.203 8.210 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 152.0964 7.9323 19.174 &lt; 2e-16 *** ## HandWidth 2.1179 0.9722 2.179 0.037945 * ## GenderMale 9.3971 2.4288 3.869 0.000596 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.968 on 28 degrees of freedom ## Multiple R-squared: 0.6708, Adjusted R-squared: 0.6472 ## F-statistic: 28.52 on 2 and 28 DF, p-value: 1.758e-07 The coefficient summary now gives us a two intercept estimates (152.0964 for females and 152.0964 + 2.1179 = 154.2143 for males) and single estimate for a slope that applies to both genders (2.1179). Unfortunately, the handy geom_smooth function cannot handle this simpler model! We must take a slightly different, and sadly mode complicated approach: What we need to do is predict using the model what the height will be under different conditions. Think of this as “plugging values into an equation”. We want to predict heights across the range of hand widths (from 6.5cm to 11cm), and we need to do this for males and females. We do this by creating a “fake” dataset to predict from using the useful function expand.grid. This function takes inputs from columns of data and “expands” them to ensure that all possible combinations are included. predictData &lt;- expand.grid(HandWidth = c(6.5,11),Gender = c(&quot;Male&quot;,&quot;Female&quot;)) predictData ## HandWidth Gender ## 1 6.5 Male ## 2 11.0 Male ## 3 6.5 Female ## 4 11.0 Female Now we can use these values to predict what the heights will be for those particular combinations of values. The arguments for the predict function are the model name, then newdata = to give the function the data that you want to predict from. Here we can use the function to add the models predicted fitted value (fit) to the predictData object we just created. predictData$Height &lt;- predict(mod_B,newdata = predictData) predictData ## HandWidth Gender Height ## 1 6.5 Male 175.2598 ## 2 11.0 Male 184.7902 ## 3 6.5 Female 165.8626 ## 4 11.0 Female 175.3931 Now we can add lines for these predicted values to our plot. We do this using the geom_smooth function as before, but this time we use the arguments data = predictData to tell R to use the new data, and stat = \"identity\" and to ensure that we plot the data rather than fitting any model. You may wish to add an error ribbon to these lines. We will cover this in a later class (but see pages 159-164 in the GSWR textbook). ggplot(classData,aes(x = HandWidth,y = Height,colour = Gender)) + geom_point() + geom_smooth(data = predictData,stat=&quot;identity&quot;) We could report this in the usual way but first saying something like: “The interaction term was not significant (F = 0.1922, d.f. 1 and 27, p = 0.665) and I therefore simplified the model to remove this term. The resulting model with just HandWidth and Gender …” "],
["linear-models-with-1-categorical-explanatory-variables.html", "Chapter 13 Linear models with &gt;1 categorical explanatory variables 13.1 Fitting a two-way ANOVA model 13.2 Summarising the model (anova) 13.3 Summarising the model (summary) 13.4 Exercise: Fish behaviour", " Chapter 13 Linear models with &gt;1 categorical explanatory variables In the one-way ANOVA we covered in the previous chapter we were interested in understanding the effect of a single categorical explanatory variable with two or more levels on a continuous response variable. Although the explanatory variable must be categorical (i.e. with discrete levels), it could represent a continuous variable. For example, the explanatory variable could be a two-level soil nutrient level (high or low), even though nutrient level is a continuous variable and one could measure the actual quantitative value of nutrients in mg/g. The two-way ANOVA is an extension of one-way ANOVA that allows you to investigate the effect of two categorical variables. This can be useful in an experimental context. For example, one might have run an experiment investigating in the effect of two types of diet (lowProtein and highProtein), and genotype (gt1 and gt2), on adult size of a pest species. It is worth thinking about what potential outcomes there are for this experiment. There may be no effect of diet, and no effect of genotype. There may be an effect of one of these variables but not the other. The effect of the diet might be the same for the different genotypes, or it might be different. Some of these different possible outcomes are illustrated in Figure . the titles indicate with Y (yes) or N (no) whether the figure shows a significant diet, genotype (gt) or interaction (int) effect. The dotted lines joining the estimates for the two genotypes are a kind of interaction plot: where they are parallel, there is no interaction. Figure 13.1: Some potential results of the experiment. There may be a significant effect (or not) of both of the main effects (diet and genotype) and there may be a significant interaction effect (or not). In the model we aim to quantify these effects, and ask if they are statistically significant (i.e. if the effect sizes are &gt;0). We divide the effects of the explanatory variables into two types: main effects and interaction effects. The main effects are the overall effect of the explanatory variables (genotype and diet in this case) while the interaction effect allows us to ask whether one main effect depends on another. In this case we are asking whether the effect of diet depends on genotype (and vice versa). Make sure that you understand this important concept. 13.1 Fitting a two-way ANOVA model Let’s use R to fit a two-way ANOVA model using data from the example I just described. As with one-way ANOVA, you can fit a two-way ANOVA model in R using lm. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. First, import the insectDiet.csv data and plot it, to produce a plot like in Figure . From looking at the graph in Figure you can see (a) genotype 1 tends to be larger than genotype 2; (b) insects raised on a high protein diet tend to be larger than those on a low protein diet; and (c) the effect of the diet (i.e. the difference in size between the insects raised on the different diets) is larger for genotype 1 than it is for genotype 2. But are these differences statistically meaningful? insectDiet &lt;- read.csv(&quot;CourseData/insectDiet.csv&quot;) ggplot(insectDiet,aes(x = genotype,y = lengthMM,fill=diet)) + geom_boxplot() + xlab(&quot;Genotype&quot;)+ ylab(&quot;Length (mm)&quot;) Figure 13.2: The effect of diet protein content and genotype on adult size of an insect species To address this question, we will fit a linear model (the two-way ANOVA) to estimate the effects of diet and genotype. The model formula is lengthMM ~ genotype + diet + genotype:diet. Let’s try to understand this. The genotype + diet part represents the main effects of these two variables, and the genotype:diet part represents the interaction effect between them. This formulacan be shortened to lengthMM ~ genotype * diet (i.e. this is exactly equivalent to the more complicated-looking formula), but I recommend to use the longer version because it is clearer. So we fit the model like this - putting the formula first, then telling R which data to use: mod_A &lt;- lm(lengthMM ~ genotype + diet + genotype:diet, data = insectDiet) Then we can look at diagnostic plots, as with ANOVA etc.: library(ggfortify) autoplot(mod_A) These all look OK. The slightly odd structure in the QQ-plot is caused by the fact that the length data are rounded to the nearest millimeter. There is no evidence of heteroscedasticity (left hand plots) now any major outliers. 13.2 Summarising the model (anova) Since we are satisfied with the diagnostic plots we can proceed by summarising the model using first anova and then summary. anova(mod_A) ## Analysis of Variance Table ## ## Response: lengthMM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 1 426.02 426.02 99.575 7.135e-13 *** ## diet 1 111.02 111.02 25.949 7.064e-06 *** ## genotype:diet 1 54.19 54.19 12.665 0.0009073 *** ## Residuals 44 188.25 4.28 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This summary Analysis of Variance Table is similar to the ones you have already seen for one-way ANOVA and linear regression. It just has some extra rows because you have extra explanatory variables. It shows you the degrees of freedom for the different terms in the model (all 1, because they have two levels), the sum of squares (Sum Sq) and mean sum of squares (Mean Sq) and the associated F value and p-value (Pr(&gt;F)). Those F values are all large, leading to highly-significant p-values. This means that all of those terms in the model explain a significant proportion of the variation in insect length. But as you know, this summary table doesn’t tell you the direction of the effects. The obvious way to understand your data is to simply look at the plot you have already produced. You could also make an interaction plot which is a simplified version of the plot of the raw data. To do this you first need to create a summary table using dplyr tools summarise and group_by to get the mean and standard errors of the mean: insectDiet_means &lt;- insectDiet %&gt;% group_by(genotype, diet) %&gt;% # &lt;- remember to group by *both* factors summarise(MeanLength = mean(lengthMM),SELength = sd(lengthMM)/sqrt(n())) insectDiet_means ## # A tibble: 4 x 4 ## # Groups: genotype [2] ## genotype diet MeanLength SELength ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gt1 highProtein 25.8 0.672 ## 2 gt1 lowProtein 20.7 0.527 ## 3 gt2 highProtein 17.8 0.698 ## 4 gt2 lowProtein 16.8 0.458 Then you can make a simple plot of this information by plotting points, and lines joining them: (A&lt;-ggplot(insectDiet_means, aes(x = genotype, y = MeanLength, colour = diet, group = diet)) + geom_point(size = 4) + geom_line()) You could add error bars to the points by adding a line defining the ymin and ymax values from the data summary like this: ggplot(insectDiet_means, aes(x = genotype, y = MeanLength, colour = diet, group = diet, ymin = MeanLength - SELength, ymax = MeanLength + SELength)) + geom_point(size = 4) + geom_line() + geom_errorbar(width = 0.1) But are these points statistically significantly different from each other? To answer that question we need to use a post-hoc test library(agricolae) HSD.test(mod_A, trt = c(&quot;diet&quot;, &quot;genotype&quot;), console = TRUE) ## ## Study: mod_A ~ c(&quot;diet&quot;, &quot;genotype&quot;) ## ## HSD Test for lengthMM ## ## Mean Square Error: 4.278409 ## ## diet:genotype, means ## ## lengthMM std r Min Max ## highProtein:gt1 25.83333 2.329000 12 23 31 ## highProtein:gt2 17.75000 2.416797 12 14 22 ## lowProtein:gt1 20.66667 1.825742 12 17 24 ## lowProtein:gt2 16.83333 1.585923 12 14 19 ## ## Alpha: 0.05 ; DF Error: 44 ## Critical Value of Studentized Range: 3.775958 ## ## Minimun Significant Difference: 2.254643 ## ## Treatments with the same letter are not significantly different. ## ## lengthMM groups ## highProtein:gt1 25.83333 a ## lowProtein:gt1 20.66667 b ## highProtein:gt2 17.75000 c ## lowProtein:gt2 16.83333 c The important part of this output is at the bottom where it tells us Treatments with the same letter are not significantly different.. You can see that the mean lengths between diets for genotype 1 are significantly different (they do not share a letter). However, there is no significant difference between diets for genotype 2 (they share the same letter, c). The two genotypes are also significantly different from each other. 13.3 Summarising the model (summary) This (above) is generally enough information for a complete write up of results. However, you can ask R to provide the model summary that includes the \\(R^2\\) values, coefficient estimates and standard errors using summary. summary(mod_A) ## ## Call: ## lm(formula = lengthMM ~ genotype + diet + genotype:diet, data = insectDiet) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7500 -1.0417 0.1667 1.2500 5.1667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.8333 0.5971 43.264 &lt; 2e-16 *** ## genotypegt2 -8.0833 0.8444 -9.572 2.53e-12 *** ## dietlowProtein -5.1667 0.8444 -6.118 2.26e-07 *** ## genotypegt2:dietlowProtein 4.2500 1.1942 3.559 0.000907 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.068 on 44 degrees of freedom ## Multiple R-squared: 0.7585, Adjusted R-squared: 0.742 ## F-statistic: 46.06 on 3 and 44 DF, p-value: 1.254e-13 The most useful thing shown here is the \\(R^2\\) value. Because we have several terms in the model we should use the Adjusted R-squared value of 0.742. This indicates that our model explains 74.2% of variation in insect length. The next bit is not 100% necessary most of the time… We already have a good idea of the mean values and standard errors for these data look because we calculated them above directly from the data. For completeness though I will now run through the coefficient estimates part of the summary table. The coefficient Estimates here are interpreted in a similar way to a one-way ANOVA. Again, it is important to know what the reference point is. When you understand this you can reconstruct the mean values for the various levels of the variables that are estimated by the model. You will see that the model estimates lead to precisely the same estimates as obtained from summarising the data. Here you can see that: The (Intercept) is 25.8333 and must refer to the point for genotype 1 on a high protein diet (look at the value of the intercept and compare to the graph/summary table, and/or the output from the Tukey test). The second coefficient (genotypegt2) is -8.0833 which is the difference between the reference (intercept) and the value for genotype 2 on a high protein diet: (25.8333 + (-8.0833) = 17.75). The third coefficient (dietlowProtein) is -5.1667 which is the difference between the reference point and for genotype 1 on a low protein diet: (25.8333 + (-5.1667) = 20.6666). The final coefficient dietlowProtein:genotypegt2 is 4.25 and is “interaction effect” of diet and genotype and represents the additional effect of genotype when it is on diet. In other words, in comparison to the reference point (genotype 1 &amp; high protein diet), the effect of a low protein diet is negative (-8.0833), as is the effect of being genotype 2 (-5.1667). However, having both a low protein diet and being genotype 2 leads to an additional positive effect (4.25) on length. The resulting estimate of mean length for genotype 2 on a low protein diet is 25.8333 + (-8.0833) + (-5.1667) + 4.25 = 16.833. This is a bit complicated so my advice is generally to refer to the figures and the outputs of the Tukey.HSD function to obtain the estimate in the different groups. The logic and methods of the two-way ANOVA can be extended to produce \\(n\\)-way ANOVA with \\(n\\) categorical variables. 13.4 Exercise: Fish behaviour Individual differences in animal personality and external appearance such as colouration patterns have both been extensively studied separately. A significant body of research has explored many of pertinent ecological and biological aspects that can be affected by them and their impact upon fitness. Currently little is known about how both factors interact and their effect on reproductive success. Researchers carried out a study looking at differences in personality and its interaction with colour phenotype in zebra fish (Danio rerio). They used two colour morphs, “homogeneous” which has clearly defined lateral stripes, and “heterogeneous” which has more variable and less clear patterns. They also assigned individuals to two personality types which they called “Proactive” (adventurous, risk taking) and “Reactive” (timid, less risk taking). They did this by recording how they explore a new environment The two variables of interest are: Colour pattern (homogeneous and heterogeneous) Personality (proactive and reactive) The research questions are: What is the relative influence of colour pattern and personality? Which is more important? How do the variables interact to determine fitness? e.g. do proactive individuals do better than reactive ones, and does this depend on colour pattern? Or some other pattern? Import the data set, fishPersonality.csv Plot the data (e.g. as a box plot) Fit an ANOVA model using lm. Look at diagnostic plots from this model (autoplot) Use anova to get an Analysis of Variance summary table, and interpret the results. Get the coefficient summary (summary) and interpret the output. Do post-hoc Tukey tests (e.g. using HSD.test from the agricolae package). Interpret the results. Sum up your findings with reference to the initial research questions. "],
["generalised-linear-models.html", "Chapter 14 Generalised linear models 14.1 Count data with Poisson errors. 14.2 Exercise: Maze runner", " Chapter 14 Generalised linear models The models we have covered so far are ordinary linear models (including ANOVA, ANCOVA, ordinary linear regression etc.) that assume thatthe relationship between the explanatory variables and the response variable is linear, and that the systematic error in the model is constant (homoscedastic, i.e. the standard deviation of the data does not depend on the magnitude of the explanatory variable). In many cases this will not be the case. Non-linearity and heteroscedasticity tend to go hand-in-hand. Sometimes, for example, the data show an exponential growth type of pattern, and/or may be bounded in such a way that the errors cannot be homoscedastic. For example, counts of number of species on islands of different sizes have a lower bound at zero (you can’t have negative numbers of species!) and increase exponentially while the standard deviations are are small for small islands and large for large islands.; ratio data or percentage data such as proportion of individuals dying/surviving is bounded between 0 and 1 (0 - 100%). Transformation of the response variable could an option to linearise these data (although there would be problems with 0 values (because log(0) = -Infinity)), but a second problem is that the ordinary linear model assumes “homoscedasticity” - that the errors in the model are evenly distributed along the explanatory variables. This assumption will be violated in most cases. For example, with count data (e.g. number of species in relation to island size), the errors for very small islands will be smaller than those for large islands. In fact, even if we transform the response variable, for example by log transformation, the predictions of the model will allow errors that include negative counts. This is clearly a problem! Generalised linear models (GLMs) solve these problems by not only applying a transformation but also explicitly altering the error assumption. They do this using a link function to carry out the transformation and by choosing an error structure (sometimes referred to as family, or variance function). The choice of link and error structure can be a bit confusing, but there are so-called “canonical links” that are commonly associated with particular error structures. For example, a model for count data would usually have a log link and a Poisson error structure. The flexibility of the GLM approach means that one can fit GLM versions of all of the models you have already learned about until this point: ANOVA-like GLMs, ANCOVA-like GLMs, ordinary regression-like GLMs and so on. In this chapter I will focus on this count data and in the next chapter I will broaden the focus to illustrate uses of other data types. 14.1 Count data with Poisson errors. The most common kind of count data where Poisson errors would be expected are frequencies of an event: we know how many times an event happened, but not how many times it did not happen (e.g. births, deaths, lightning strikes). In these cases: Linear model could lead to negative counts. Variance of response likely to increase with mean (it usually does with count data). Errors are non-normal. Zeros difficult to deal with by transformation (e.g. log(0) = -Inf). Other error families do not allow zero values. The standard (“canonical”) link used with the Poisson error family is the log link. The log link ensures that all fitted (i.e. predicted) values are positive, while the Poisson errors take account of the fact that the data are integer and the variance scales 1:1 with the mean (i.e. variance increases linearly and is equal to the mean). There are other potential link and error families that could be used with this kind of data, but we’ll stick with the standard ones here. Lets look at a couple of examples… 14.1.1 Example: Number of offspring in foxes. This example uses the fox.csv data set. This data set gives the number of offspring produced by a group of foxes, alongside the weight (in kg) of the mothers. Let’s import and plot the data. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. The first thing to notice is that, like all count data, the data are formed into horizontal rows of data reflecting the fact that the response data are integer values. There is clearly an increasing pattern, but how can we formally test for a statistical relationship. It is obvious that fitting an ordinary linear model though this data would not be the right approach: this would lead to the prediction of negative number of offspring for small foxes, and also, the variance appears to increase with weight/number of offspring. Therefore this is a good candidate for a GLM. The data are bounded at 0, and are integer values, and for this reason the usual approach would be to fit a GLM with Poisson errors (and the standard log link). mod1 &lt;- glm(noffspring ~ weight, data = fox, family = poisson) After fitting the model it is a good idea to look at the model diagnostics, using autoplot from the ggfortify package. Now we can ask for the Analysis of Variance table for this model. This is exactly the same procedure as for the previous linear models (ANOVA, ANCOVA etc.) except for GLMs one must also specify that you would like to see the results of significance tests using the test = \"F\" or test = \"Chi\". For Poisson and binomial GLMs the chi-squared test is most appropriate while for Gaussian (normal), quasibinomial and quasipoisson models the F test is most appropriate. anova(mod1,test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: noffspring ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 99 166.84 ## weight 1 44.124 98 122.72 3.082e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This summary table tells us that the single explanatory variable (weight) is fantastically important (p-value is very small indeed). We can then ask for the coefficient summary using summary. summary(mod1) ## ## Call: ## glm(formula = noffspring ~ weight, family = poisson, data = fox) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3891 -0.9719 -0.1183 0.5897 2.3426 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.74981 0.31107 -2.410 0.0159 * ## weight 0.63239 0.09502 6.655 2.83e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 166.85 on 99 degrees of freedom ## Residual deviance: 122.72 on 98 degrees of freedom ## AIC: 405.56 ## ## Number of Fisher Scoring iterations: 5 GLM model coefficients and predicted values, are expressed on the scale of the linear predictor (i.e. the transformed data scale). It is usually desirable to “backtransform” to the natural scale before plotting. See below. The model coefficients and their standard errors are given on the scale of the linear predictor. They tell us that there is a significant association between the weight of the fox mother and the number of offspring she will produce: larger foxes produce more offspring. Because the coefficients are given on the scale of the linear predictor rather than on the real scale it is useful to plot predictions of the model to visualise the relationship. To do that we must (1) tell the model what to predict from i.e. we must provide a suitable sequence of numbers to predict from using seq, (2) use the predict function to predict values (fit) from the model. We use the argument type = \"response\" to tell the function that we want the predictions on the back-transformed (real) scale rather than on the scale of the linear predictor. We add the argument se.fit = TRUE to tell the function to give us the standard error estimates of the fit. The se.fit values are added or subtracted from the fit to obtain the plus/minus standard errors. We can multiply these by 1.96 to get the 95% confidence intervals of the fitted values. #Vector to predict from newData &lt;- data.frame(weight = seq(1.7,4.4,0.01)) #Predicted values (and SE) predVals &lt;- predict(mod1,newData,type=&quot;response&quot;,se.fit = TRUE) #Create new data for the predicted fit line newData &lt;- newData %&gt;% mutate(noffspring = predVals$fit) %&gt;% mutate(ymin = predVals$fit - 1.96*predVals$se.fit) %&gt;% mutate(ymax = predVals$fit + 1.96*predVals$se.fit) Take a look at this data to make sure it looks OK. head(newData) ## weight noffspring ymin ymax ## 1 1.70 1.384392 0.9649333 1.803850 ## 2 1.71 1.393174 0.9734837 1.812865 ## 3 1.72 1.402013 0.9821017 1.821924 ## 4 1.73 1.410907 0.9907879 1.831026 ## 5 1.74 1.419858 0.9995426 1.840173 ## 6 1.75 1.428865 1.0083663 1.849364 This looks OK. Now we can plot the data and add a the model fit line, and a “ribbon” representing the errors (the 95% confidence interval for the line). So we could summarise this something like this: Methods: I modelled the association between mother’s weight and number of pups produced using a generalised linear model with a log link and Poisson error structure. This is appropriate because the data are count data (number of pups) that are bounded at 0 with increasing variance with increased maternal weight. Results: The GLM showed that maternal weight was significantly associated with the number of pups produced (GLM: Null Deviance = 166.8, Residual Deviance = 122.7, d.f. = 1 and 98, p &lt;0.001). The slope of the relationship was 0.63 (on the log scale). The equation of the best fit line was log(nOffspring) = -0.75 + 0.63\\(\\times\\)MotherWeight (see Figure XXX) 14.1.2 Example: Cancer clusters This data show counts of prostate cancer and distance from a nuclear processing plant. Lets take a look at the data. Let’s first import the data and use summary to examine it by plotting it: First we can see that there are no negative count values. Again, you will notice that the data are formed into horizontal rows of integer response values. There are lots of zero values at all distances, but the biggest cluster (6 cases), is very close to the plant. But is there a relationship between the distance from the nuclear plant and the number of cancers? Let’s fit a Generalised Linear Model to find out. As before will assume that the error is Poisson (that they variance increases directly in proportion to the mean), and we will use the standard log link to ensure that we don’t predict negative values: mod1 &lt;- glm(Cancers ~ Distance, data = cancer, family = poisson) Next, plot the diagnostic plots. These look a bit dodgy, but we’ll stick with it for the moment. Next ask for the Analysis of Variance table. anova(mod1,test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: Cancers ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 93 149.48 ## Distance 1 2.8408 92 146.64 0.0919 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The ANOVA table tells us that there is no significant effect of the Distance variable. In other words a model that includes the Distance term does not explain significantly more variation than the NULL model that includes no terms and instead assumes that variation in cancer incidence is simply caused by random variation. We needn’t go further with this model, but go ahead and plot the model in any case (just for practice). summary(mod1) ## ## Call: ## glm(formula = Cancers ~ Distance, family = poisson, data = cancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5504 -1.3491 -1.1553 0.3877 3.1304 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.186865 0.188728 0.990 0.3221 ## Distance -0.006138 0.003667 -1.674 0.0941 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 149.48 on 93 degrees of freedom ## Residual deviance: 146.64 on 92 degrees of freedom ## AIC: 262.41 ## ## Number of Fisher Scoring iterations: 5 Use the approach from the fox example as guidance to make a plot with a fit line. 14.2 Exercise: Maze runner In an experiment, researchers studied the ability of children and adults to navigate through a maze. They recorded the number of mistakes each person made before successfully completing the maze. The data (maze.csv) has two columns: Age (a categorical variable with two levels - Adult and Child) and nErrors a count of the number of errors that each subject makes. In this example, you will be fitting a GLM equivalent of a t-test that is appropriate for count data. Import the data and graph it (geom_boxplot). Try adding the points to the ggplot using the new (to you) function geom_dotplot(binaxis = \"y\", stackdir = \"center\"). Fit an appropriate GLM. Examine the diagnostic plots of the model (autoplot). Get the analysis of variance (deviance) table (anova). What does this tell you? Obtain the summary table. What does this tell you? Use the coefficient information in the summary table to get the model predictions for average number of mistakes (plus/minus 95% Confidence interval). Remember that (i) the model summary is on the scale of the linear predictor, and (ii) the 95% CI can be calculated as 1.96 times the standard error. You can do these calculations “by hand”, or using the predict function. Ask for help if you get stuck. "],
["extending-use-cases-of-glm.html", "Chapter 15 Extending use cases of GLM 15.1 Binomial response data 15.2 Example: NFL field goals 15.3 Example: Sex ratio in turtles 15.4 Example: Smoking", " Chapter 15 Extending use cases of GLM In the previous chapter we used the case of modelling count data, which is bounded at 0 and takes integer values, to understand how Generalised Linear Models work. In this chapter we extend our understanding by looking at another type of data, namely binomial data. 15.1 Binomial response data There are three types of Binomial data, but all three types have the idea of “success” and “failure” at their heart. It is up to you, the modeller, to decide what these successes and failures are, but they can be any kind of data that can be coerced into two discrete categories. Common examples include pass/fail, survived/died, presence/absence and yes/no. The three data types are as follows: The data can be in the form of a two-level factor - often coded as zeros and ones (0/1 data). For example, this could represent whether an died (0) or survived (1) in a study; or it could represent male (M) or female (F) in a study trying to predict sex from other variables (e.g. size or morphology). The data could be expressed as a numeric vector of proportions, bounded between 0 and 1 (i.e. can take any value between 0 and 1). For example these data could be percentages expressed as proportion, such as proportion (or percent) of individuals in a group surviving. In this case, the total number of cases that contributed to the proportion can be optionally included as weights in the regression model.12 Finally, the data could be expressed as numbers in a two column matrix where the first column gives the number of “successes” and the second column gives the number of “failures”. The aim of the GLM is usually to estimate probability of “success” (e.g. survival, passing, scoring a goal…). Thus, the outcome (the predicted response, or fit, of the model) is a value between 0 and 1 on the natural scale, that can be interpreted as a probability (e.g. of survival, of success, of presence etc.) The link function used for a binomial GLM is usually the logit transformation, and therefore another name for this type of regression is logistic regression. The logit transformation linearises an S-shaped curve and therefore allows a linear regression line to be fitted from data that follows this pattern. The function is \\(y = log(\\frac{p}{1-p})\\), where \\(p\\) is the probability or proportion. The inverse logit (the anti-logit) is \\(p = \\frac{exp(y)}{exp(y)+1}\\). We could linearise the data and then fit an ordinary linear model using lm but (like with the Poisson regression) the other assumption of the ordinary linear model, homoscedasticity, would cause problems. With S-shaped binomial relationships the expected variance is small at the two ends of the range and large in the middle of the range. This contrasts strongly with the constant variance assumption of ordinary linear models. Therefore it is wise to account for this using a Generalised Linear Model that explicitly accounts for this variance structure. Let’s try a couple of examples. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. 15.2 Example: NFL field goals In this example you will be dealing with binary data (0/1, failure/success) from the American National Football League (NFL). The data are a record of field goals, which are a relatively rare method of scoring where someone kicks the ball through over the crossbar and through the uprights of the goal during play. Our aim is to estimate how the probability of success changes with distance from the goal. We already have a good idea that success probability will decline with increasing distance! But at what rate does the probability decline? And at what distance is there a probability of 0.5 (i.e. 50% chance of success)? First, import the data and convert the distance from yards to meters. NFL &lt;- read.csv(&quot;CourseData/NFLfieldgoal.csv&quot;) %&gt;% mutate(Distance = Distance * 0.9144) Next, plot the data with ggplot (geom_jitter would be a good option, but you might like to adjust the height and alpha arguments. e.g. geom_jitter(height = 0.05,alpha = 0.5)). You can see that the data is distributed in two bands with 1 representing success and 0 representing failure. (A&lt;-ggplot(NFL,aes(x = Distance,y = Success)) + geom_jitter(height = 0.05, alpha = 0.5)) Now we can fit a GLM using an appropriate binomial error structure. nflMod &lt;- glm(Success ~ Distance, data = NFL, family = binomial) As usual, we should look at the model diagnostics. These look pretty bad. One reason that these diagnostics (e.g. the QQ plot) can look bad is when we are missing important variables in the model. In this case, for example, the distribution could look “off” because we don’t include important information on other aspects of the game (e.g. was the team winning or losing at the time? how many minutes until the end of the game? how experienced is the player?). We can be fairly sure that binomial is the most appropriate variance structure because of the nature of the 0/1 data so let’s stick with it for now! library(ggfortify) autoplot(nflMod) We proceed in the normal way by obtaining the ANOVA table for the model. We need to specify that we want to calculated p-values using a “Chi” squared test. This shows us that indeed distance is an important variable in determining probability of success (not so surprising!) anova(nflMod, test = &quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: Success ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 947 955.38 ## Distance 1 137.8 946 817.58 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We get more useful information from the coefficient summary of the model. This gives the intercept and slope of the model on the scale of the linear predictor (see the figure above). summary(nflMod) ## ## Call: ## glm(formula = Success ~ Distance, family = binomial, data = NFL) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6681 0.2704 0.4067 0.7094 1.4708 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.68958 0.45021 12.64 &lt;2e-16 *** ## Distance -0.13118 0.01263 -10.38 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 955.38 on 947 degrees of freedom ## Residual deviance: 817.58 on 946 degrees of freedom ## AIC: 821.58 ## ## Number of Fisher Scoring iterations: 5 We could report this something like this: The binomial GLM showed that distance was significantly associated with the probability of goal success (GLM: Null Deviance = 955.38, Residual Deviance = 817.58, d.f. = 1 and 946 p &lt;0.001). The slope and intercept of the relationship is -0.131 and 5.690 respectively on the logit scale. The equation of the best fit line was therefore logit(nOffspring) = 5.690 - 0.131\\(\\times\\)distance (see Figure XXX) The equation of the model if you want to express it on the natural scale works out to be: \\(y =\\frac{1}{1+exp(-(\\beta_0 + \\beta_1 x))}\\), or \\(probability =\\frac{1}{1+exp(-(5.690 - 0.131\\times distance))}\\). Let’s make sure that works by creating a set of predicted data from this equation and plotting it onto the graph: d1 &lt;- data.frame(x = 10:50) %&gt;% mutate(y = 1/(1+exp(-(5.690 - 0.131 *x)))) A + geom_line(data = d1,aes(x,y),colour=&quot;red&quot;) Rather than using the equation, there is an easier way by using the R’s predict function. We will use predict to get the predicted probability of success across the range of distances that we provide as a new data frame that we are here calling newDat. We will also predict the 95% Confidence Interval (CI) for these estimates. We will use these data to add the CI ribbon and line to the plot. The predict function returns an object that includes the fit and the se.fit which are the predicted value of the regression and the standard error of that predicted value respectively. Thus, if the output of the predict function is stored as pv we can address those parts as pv$fit and pv$se.fit, and we can save these values alongside the data in newDat. Remember that the model, and the predicted values from it, are expressed on the scale of the linear predictor (i.e. the transformed data scale). It is usually desirable to “backtransform” to the natural scale before plotting. See below. First we obtain the predictions and CI on the scale of the linear predictor (logit scale). We calculate 95% CI from the standard errors simply by multiplying by 1.96. #Dataset to predict FROM newDat &lt;- data.frame(Distance = 14:52) #Get predictions from the model pv &lt;- predict(nflMod, newdata = newDat,se.fit = TRUE) #Add those predictions to newDat newDat &lt;- newDat %&gt;% mutate(Success_LP = pv$fit) %&gt;% mutate(lowerCI_LP = pv$fit - 1.96*pv$se.fit) %&gt;% mutate(upperCI_LP = pv$fit + 1.96*pv$se.fit) Now we can first obtain the inverse link from the model object family(nflMod)$linkinv, and use that to backtransform the data onto the natural scale ready for plotting. #Get the inverse link function inverseFunction &lt;-family(nflMod)$linkinv #transform predicted data to the natural scale newDat &lt;- newDat %&gt;% mutate(Success = inverseFunction(Success_LP), ymin= inverseFunction(lowerCI_LP), ymax = inverseFunction(upperCI_LP)) Now we can finally plot the model predictions and the 95% confidence intervals for them. #The plot and ribbon (A &lt;- ggplot(NFL,aes(x= Distance,y=Success)) + geom_ribbon(data = newDat,aes(x =Distance,ymin=ymin,ymax=ymax), fill=&quot;grey75&quot;,alpha = 0.5)+ geom_smooth(data = newDat,stat=&quot;identity&quot;)+ geom_jitter(height = 0.05,alpha = 0.5)) So, at what distance does the probability of success fall to just 50%? You could read this directly from the plot as “approximately 44m”. Alternatively, you could obtain the value from the newDat dataset you created above with predictions from the model, by filtering it. This confirms the probability of success reaches 0.5 somewhere between 43-44m. newDat %&gt;% filter(Success &lt; 0.55) %&gt;% filter(Success &gt; 0.45) %&gt;% select(Distance,Success) ## Distance Success ## 1 42 0.5449184 ## 2 43 0.5122432 ## 3 44 0.4794630 15.3 Example: Sex ratio in turtles In this example we will look at sex ratio of hawksbill turtles (Eretmochelys imbricata)13. The data are counts of males and females in clutches of eggs incubated at different temperatures. The sex ratio in the species varies with temperature during incubation. We are interested in what the “tipping point” temperature is between male-female biased ratios. hawksbill &lt;- read.csv(&quot;CourseData/hawksbill.csv&quot;) This is a small dataset, you can look at the whole thing: hawksbill ## Temperature Total Nmale Nfemale ## 1 27.9 12 12 0 ## 2 28.4 29 29 0 ## 3 28.9 31 26 5 ## 4 29.3 20 18 2 ## 5 29.5 22 14 8 ## 6 29.8 10 2 8 ## 7 30.4 10 0 10 We are interested in sex ratio which we can calculate as the proportion of the population that is female (i.e. number of females divided by total number). hawksbill &lt;- hawksbill %&gt;% mutate(propFemale = Nfemale/(Nmale+Nfemale)) Let’s plot that data. We can use the trick of telling R to plot the points different sizes depending on the sample size (using the size = argument): (A&lt;-ggplot(hawksbill,aes(x = Temperature,y = propFemale,size = Total))+ geom_point()) You can see that the proportion of females increases with temperature. Let’s fit a model to these data to better understand them. We could use the propFemale data as the response variable, but there is a big problem with that: we would be giving equal weight to each of the data points, even though the sample size for each one ranges from 10 to 31 This is not good because we would have much more faith in the very large sample sizes than the small ones. Instead we can bind the data into a two column matrix using cbind. The first column is our “success” and the second column is our “failure”. If we put females in the first column the model will predict “probability of being female”, which is what we want. This two-column approach provides the model with the sample size which it can use to weight the regression appropriately. y &lt;- cbind(hawksbill$Nfemale,hawksbill$Nmale) Now lets fit the model. modA &lt;- glm(y ~ Temperature,data=hawksbill,family = binomial) As ever, we should take a quick look at the model diagnostic plots - these look OK. library(ggfortify) autoplot(modA) Now we can look at the anova table. This will tell us what we already know - there is a strong effect of temperature on sex ratio. anova(modA,test=&quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 6 70.353 ## Temperature 1 61.869 5 8.484 3.671e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now for the coefficients: summary(modA) ## ## Call: ## glm(formula = y ~ Temperature, family = binomial, data = hawksbill) ## ## Deviance Residuals: ## 1 2 3 4 5 6 7 ## -0.2010 -0.8013 1.7632 -1.6999 -0.4885 0.8010 0.9607 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -111.7200 22.4260 -4.982 6.30e-07 *** ## Temperature 3.7754 0.7625 4.951 7.37e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 70.3528 on 6 degrees of freedom ## Residual deviance: 8.4841 on 5 degrees of freedom ## AIC: 24.184 ## ## Number of Fisher Scoring iterations: 5 This table gives us the coefficients for the formula of our relationship. We could use these to produce a formula of the form \\(y =\\frac{1}{1+exp(-(\\beta_0 + \\beta_1 x))}\\). It is perhaps more useful to plot the model fit onto the plot. First we need to create a data frame to predict from: newDat &lt;- data.frame(Temperature = seq(27.9,30.4,0.1)) Now we can predict the values (and 95% CI) on the scale of the linear predictor (logit). pv&lt;-predict(modA,newDat,se.fit = TRUE) newDat &lt;- newDat %&gt;% mutate(propFemale_LP = pv$fit, lowerCI_LP = pv$fit - 1.96*pv$se.fit, upperCI_LP = pv$fit + 1.96*pv$se.fit) Now we can use the inverse link function to backtransform to the natural probability scale. #Get the inverse link function inverseFunction &lt;-family(modA)$linkinv #transform predicted data to the natural scale newDat &lt;- newDat %&gt;% mutate(propFemale = inverseFunction(propFemale_LP), ymin= inverseFunction(lowerCI_LP), ymax = inverseFunction(upperCI_LP)) We can add these values to the plot like this: #The plot and ribbon (A&lt;-ggplot(hawksbill,aes(x = Temperature,y = propFemale, size = Total))+ geom_ribbon(data = newDat,aes(x =Temperature,ymin=ymin,ymax=ymax), fill=&quot;grey75&quot;,alpha = 0.5,inherit.aes=FALSE)+ geom_smooth(data = newDat,aes(x = Temperature,y=propFemale),stat=&quot;identity&quot;,inherit.aes=FALSE)+ geom_point()) Can you give read off the graph the approximate estimated temperature at which sex ratio is 50:50? Try refitting the model using simply the proportion female (propFemale) data rather than the two-column (cbind) approach. Try writing up the result in the same way as shown for the NFL field goals example. 15.4 Example: Smoking As I mentioned above binomial regression can be applied to anything where there data can be classified into two groups. I’ll illustrate that now with an example about smoking. The data set is very small and looks like this: Student smokes Student does not smoke Parent(s) smoke 816 3203 No parents smoke 188 1168 The dataset is the number of students smoking and not smoking grouped according to whether their parents smoke. These data are binomial/proportion data because the values in the cells of the table are constrained by the overall total and students must fall into one of the categories. We can use these data to calculate the probability of the student being a smoker. Before fitting a GLM lets just work out these probabilities by hand. What is the probability that a child of smoking parents is a smoker themselves? This is simply 816/(816+3203) = 0.2030. (i.e. it is the number of smokers divided by the total number). Similarly, the probability that the child of non-smokers smokes is 188/(188+1168) = 0.1386. But is this a significant difference? That is what we are trying to find out using a GLM. To do this, we can turn these data into a two column matrix of success (yes - smoker) and failure (no - non-smoker). y &lt;- cbind(&quot;1_yes&quot; = c(816,188),&quot;0_no&quot; = c(3203,1168)) y ## 1_yes 0_no ## [1,] 816 3203 ## [2,] 188 1168 So we can see “success” on the left and “failure” on the right“. We now create a (tiny) data.frame for the parental status (smoker =”1_yes“, non-smoker =”0_no\"). smoke &lt;- data.frame(parentSmoke = c(&quot;1_yes&quot;,&quot;0_no&quot;)) Now we can fit the model. Pause now and think about what the NULL hypothesis is here. It is that parental smoking does not have any influence on whether the child smokes, and that the probability that the student smokes is unrelated to parental smoking. smokeMod &lt;- glm(y~parentSmoke,data = smoke,family=binomial) With such a small dataset, diagnostic plots will not tell us anything useful so there’s no point in doing them for this case. As usual, we first ask for the (Analysis of Deviance table using anova. anova(smokeMod,test=&quot;Chi&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 1 29.121 ## parentSmoke 1 29.121 0 0.000 6.801e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This tells us that the status of the parents (whether they smoke or not) is highly significant: it explains a lot of the pattern that we see in the data. We can find out what this pattern is by examining the summary table. summary(smokeMod) ## ## Call: ## glm(formula = y ~ parentSmoke, family = binomial, data = smoke) ## ## Deviance Residuals: ## [1] 0 0 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.82661 0.07858 -23.244 &lt; 2e-16 *** ## parentSmoke1_yes 0.45918 0.08782 5.228 1.71e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2.9121e+01 on 1 degrees of freedom ## Residual deviance: -3.7170e-13 on 0 degrees of freedom ## AIC: 19.242 ## ## Number of Fisher Scoring iterations: 2 This shows us the estimates on the logit scale. We can use predict to get a sense for these predictions on the more intuitive probability scale. First we calculate the fitted values and 95% confidence intervals on the scale of the linear predictor (logit): pv&lt;-predict(smokeMod,smoke,se.fit = TRUE) smoke &lt;- smoke %&gt;% mutate(prob_LP = pv$fit, lowerCI_LP = pv$fit - 1.96*pv$se.fit, upperCI_LP = pv$fit + 1.96*pv$se.fit) Then we can backtransform these values to the probability scale using the inverse link function: #Get the inverse link function inverseFunction &lt;-family(smokeMod)$linkinv #transform predicted data to the natural scale smoke &lt;- smoke %&gt;% mutate(prob = inverseFunction(prob_LP), ymin= inverseFunction(lowerCI_LP), ymax = inverseFunction(upperCI_LP)) smoke ## parentSmoke prob_LP lowerCI_LP upperCI_LP prob ymin ymax ## 1 1_yes -1.367429 -1.444287 -1.290570 0.2030356 0.1908823 0.2157563 ## 2 0_no -1.826606 -1.980629 -1.672583 0.1386431 0.1212518 0.1580801 This table shows us that the probability of the students smoking is 0.2030 (95% CI = 0.191-0.216) for children of smokers, and 0.139 (95% CI = 0.121-0.158) for children of non-smokers. We can plot this using ggplot like this. ggplot(smoke,aes(x=parentSmoke,y = prob,ymin = ymin,ymax=ymax))+ geom_point()+ geom_segment(aes(xend = parentSmoke,y = ymin,yend = ymax)) weights allow some data points to be more influential than others - for example you would want to give more importance to points that represent large sample sizes↩︎ data from: Godfrey et al. (1999) Can. J. Zool. 77: 1465–1473↩︎ "],
["power-analysis-by-simulation.html", "Chapter 16 Power analysis by simulation 16.1 Type I and II errors and statistical power 16.2 What determines statistical power? 16.3 An example of calculating statistical power. 16.4 Summary 16.5 Extending the simulation (optional, advanced) 16.6 Exercise 1: Snails on the move 16.7 Exercise 2: Mouse lemur strength", " Chapter 16 Power analysis by simulation This chapter will first focus on how we can answer questions like “what sample size should I use in my experiment?” and “with this sample size, what difference could I detect?” As you learned in the chapters on t-tests and ANOVA, the detection of a significant difference between treatment groups (if there is one) depends on two things: (1) the actual difference between mean values for the groups (the “signal”) and (2) the amount of variation there is in the groups (the “noise”). When there is a lot of noise it is hard to detect the signal. In most cases we will already have some idea about what to expect when doing a study. Previous work on similar topics, or pilot studies, will have given us an idea of typical values for the response variable, and will give us a ballpark estimate of how much variation to expect. This information can be used to conduct a power analysis by simulation. The basic idea of this approach is to simulate the experiment, by drawing random numbers from appropriate distributions, before actually carrying out the experiment. 16.1 Type I and II errors and statistical power Before setting out to run an experiment or an observational study is is natural to wonder “how much work do I really need to do here?” In other words, \"what sample size do I need in order to address the hypothesis? Similar questions are also relevant after running an experiment. For example, imagine you have run an experiment that failed to find a significant effect of your treatment. There are two explanations for this finding (i) there really is no efffect of your treatment; (ii) there is an effect of your treatment but you did not have enough power to detect it. So the question arising is: “what difference could you have detected, based on the results you have?” The answer could be “Based on my experiment I can see that if there is really a difference it must be smaller than \\(X\\)” and/or “I would need to increase my sample size to X to detect a difference if the difference is \\(Y\\)” These type of questions are closely related to the two types of error that one can make when testing hypotheses: A Type I error is the rejection of a true null hypothesis. For example, when there truly is no effect of an experimental treatment, but you detect one in error. A Type II error is the failure to reject a false null hypothesis. For example, when there truly is an effect of an experimental treatment, but you fail to detect it.14 The probability of making these errors depends on the statistical power of your study. Statistical power ranges between 0 and 1 and, for a simple hypothesis test, it is defined as the probability that the test rejects the null hypothesis (\\(H_0\\)) when the alternative hypothesis (\\(H_1\\)) is true. In other words it is the probability of NOT making a Type II error. For example, a power of 0.80 means that you have an 80% chance of correctly detecting that \\(H_1\\) is true and a 20% chance (1.0-0.8 = 0.2) of making a Type II error. As power decreases, the probability of making a Type II error increases. 16.2 What determines statistical power? Statistical power is determined by several factors including the actual effect size, the natural variation in the effect size, the sample size in the study, and the (arbitrary) criterion you have chosen for significance: The magnitude of the actual effect of interest. If the effect under investigation is very small then it will be harder to detect (i.e. the power to detect it will be small). Variation. Variation in the data introduces noise into the statistical test. Where there is large amounts of natural variation it is harder to detect significant effects (i.e. the power to detect it will be reduced). The sample size. Larger sample sizes reduce the amount of sampling error and therefore reduce the amount of “noise” in the data. Therefore large sample sizes increase power to detect a difference between groups. Sampling error can be reduced by improving the precision of measurements. Therefore power can also be increased by improving measurement precision. The significance criterion. We normally use p = 0.05, but this is arbitrary. We could increase the power of the statistical test by using a more relaxed criterion e.g. p = 0.10. There are several ways to estimate statistical power, required sample sizes etc. In this chapter we will look at one of them – power analysis by simulation. This is best communicated using a simple example. 16.3 An example of calculating statistical power. In an experiment, researchers would like to test a hypothesis that a high protein diet increases the size of adult insects from a pest species. Previous work has shown that the average size on a normal diet is 12mm with a standard deviation of 4mm. Remember to load the dplyr, magrittr and ggplot packages, and to set your working directory correctly. We can simulate a distribution of measurements from this distribution using the rnorm function. This draws n numbers from a theoretical normal distribution with a particular mean and standard deviation (sd). For example: (control &lt;- rnorm(n = 10, mean = 12, sd = 3)) ## [1] 8.378803 12.832288 15.253324 4.962907 13.287374 13.518168 10.275780 10.360104 10.306644 9.329887 Because this is a random process your results will be different (and they will be different each time you run the code). Suppose that other studies on a different species has shown that a high protein diet leads to a 20% increase in size. This means that we expect our treatment group to have a body length of 12 \\(\\times\\) 1.2 = 14.4mm. If we assume the standard deviation will be the same, we can simulate a distribution for a high protein treatment group in the same way as for the control group: (treatment &lt;- rnorm(n = 10,mean =14.4,sd = 3)) ## [1] 12.96842 11.40484 12.07124 14.59338 17.27848 14.06914 12.86697 11.66641 11.88848 21.64751 Now we have two simulated samples, and we can conduct a t-test on those samples and store the result: res &lt;- t.test(control,treatment) We can print this result to the screen like this: res ## ## Welch Two Sample t-test ## ## data: control and treatment ## t = -2.3071, df = 17.914, p-value = 0.03321 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -6.1053569 -0.2845633 ## sample estimates: ## mean of x mean of y ## 10.85053 14.04549 Or just get the p-value like this: res$p.value ## [1] 0.03320735 16.3.1 The simulation The idea with power analysis by simulation is to repeat this procedure many times (at least 1000) to estimate the probability that you get it right (that you detect a significant difference between groups when you know there is a difference15). You have come across this sort of simulation approach before when we talked about randomisation tests. As before we will use the replicate function to repeatedly do t-tests on randomly generated data with the characteristics you select (i.e. sample size, mean, and standard deviation). We first set up our simulation by defining the mean, standard deviation, and sample size of our simulated experiment. controlMean &lt;- 12 treatmentMean &lt;- 14.4 sdValue &lt;- 3 sampleSize &lt;- 10 Now we can “wrap” a t.test on simulated control and treatment data sets within a replicate function like this. Take some time to study this part of the script - it is important that you understand what it is doing. In this case the replicate command is telling R to repeat the t-test 5 times. replicate(5, t.test(rnorm(sampleSize,controlMean,sdValue), rnorm(sampleSize,treatmentMean,sdValue))$p.value ) ## [1] 0.248412775 0.023369962 0.765183387 0.001161126 0.006412789 Let’s repeat the t-test 1000 times so we can get a good idea of the number of times that the test correctly detects that there is a difference between the two groups. I don’t want to print 1000 p-values to the screen so I will collect them in a vector called powerResults. powerResults &lt;- replicate(1000, t.test(rnorm(sampleSize,controlMean,sdValue), rnorm(sampleSize,treatmentMean,sdValue))$p.value) I can now ask how many of the p-values stored in powerResults were less than 0.05. sum(powerResults&lt;0.05) ## [1] 399 So in this case, 399 of the 1000 tests were correct. The statistical power can be calculated by turning this into a percentage - i.e. Power = 39.9%. We can ask what sample size do we need to get a better power, e.g. 90%, by increasing sample size incrementally. For example, here I increase sample size to 15: controlMean &lt;- 12 treatmentMean &lt;- 14.4 sdValue &lt;- 3 sampleSize &lt;- 15 #Increased powerResults &lt;- replicate(1000, t.test(rnorm(sampleSize,controlMean,sdValue), rnorm(sampleSize,treatmentMean,sdValue))$p.value) (sum(powerResults&lt;0.05)/1000)*100 ## [1] 54.3 The power has increased to 54.3%. 16.3.2 Some questions for you to address: What sample size would give you 80% power? What power would you have if the variation was larger (e.g. sd = 4)? What power would you have if the difference between groups was only 10% instead of 20%? 16.4 Summary Simulation can be a powerful tool to help design and understand the results of hypothesis-based studies. The example above focuses on data that are normally distributed and where the test involved is a t-test. However, the same principles apply for other types of data. One can adapt the approach to use different distributions e.g. rpois for Poisson or rbinom for binomial. One can also alter the tests being done. This case study used a t.test but one could alternatively simulate the results of other ordinary linear models with lm or GLMs with glm. 16.5 Extending the simulation (optional, advanced) This section is for illustration only. It is intended to show the utility of using R to quickly address experimental design questions but it goes beyond what you are expected to learn.I hope you find it interesting nevertheless. To this you will need to add the purrr library (using install.packages(“purrr”)). This package includes a useful function, map_dbl, which allows you to repeatedly apply functions over many different input values. I use the double colon (::) notation like this purrr::map_dbl to use this function without the need to load the whole package. One can extend the simulation to cover a range of sample sizes (or differences between treatments, or standard deviations etc.) using by turning the calculations of the t-test into a function, and then applying that function over a range of values. I illustrate this below by varying sample size between 5 and 40. # Set up a data frame for the simulation results simulData &lt;- data.frame(sampleSize = 5:40) # Set basic values controlMean &lt;- 12 treatmentMean &lt;- 14.4 sdValue &lt;- 3 # Function to do the t-test pwr &lt;- function(n){sum(replicate(1000, t.test(rnorm(n,controlMean,sdValue), rnorm(n,treatmentMean,sdValue))$p.value)&lt;0.05)/1000} # map_dbl applying the function for every value of simulData$sampleSize simulData$Power &lt;- purrr::map_dbl(simulData$sampleSize,pwr) These results can then be graphed with ggplot. #Plot the output ggplot(simulData,aes(x = sampleSize,y = Power)) + geom_point() + geom_line() + geom_hline(yintercept = 0.8,linetype=&quot;dashed&quot;) 16.6 Exercise 1: Snails on the move Your supervisor has collected pilot study data on the distance travelled by a particular snail species during one day. In this study the mean distance travelled was 7.4m and there was a standard deviation of 2.76m. There are two colour morphs of the snails: One is bright and colourful with a striking black spiral while the other is drab and kinda boring-looking. Your supervisor focused on the colourful snails, but assume that standard deviation is the same for both morphs. For your masters project you hypothesise that the boring snail will cover more distance because it is less scared of visual predators and willing to expose itself to more risk while foraging. You don’t have a good feel for the difference, but you decide that a 25% difference between the morphs would be biologically interesting. Simulate a t-test-based analysis in R to figure out what sample size would result in 80% power. 16.7 Exercise 2: Mouse lemur strength Previous work on grey mouse lemurs (Microcebus murinus) has found that they are quite strong. They can lift and hold 10 times their own body weight!16 This work was done on prime-age animals. Researchers believe that older individuals will have experienced physiological senescence and that their strength will have deteriorated. The body weight of these lemurs is about 60grams. The prime-age animals could lift and hold 600grams with a standard deviation of 145grams. A research institute has agreed to you carrying out this study for your masters project. They have 25 young age lemurs, but only have 8 old aged animals. You can assume the standard deviation is the same in both age classes. What difference in strength could you reliably detect (with power &gt;80%) with these sample sizes? To remember this, think of the story about “the boy who cried wolf”. First the villagers believe the boy when there was no wolf (Type I error). Second, the villagers don’t believe the boy but there IS a wolf (Type II error).↩︎ You know there is a difference because you have set this difference!↩︎ Thomas, P., Pouydebat, E., Brazidec, M., Aujard, F., &amp; Herrel, A. (2015). Determinants of pull strength in captive grey mouse lemurs Journal of Zoology DOI: 10.1111/jzo.12292↩︎ "],
["coming-soon-maybe.html", "Chapter 17 Coming soon (maybe)! 17.1 Multivariate statistics 17.2 Transforming variables 17.3 Non-linear regression 17.4 Nested or blocked study designs", " Chapter 17 Coming soon (maybe)! 17.1 Multivariate statistics This will cover Principal Component Analysis (PCA) which is a handy statistical tool to reduce the complexity of high-dimensional data (i.e. data sets with many columns). 17.2 Transforming variables It is often desirable to transform the response variable in order to linearise the relationship between it and the explanatory variables. Generalised Linear Models (GLMs) with non-normal error distributions do this, but sometimes it is useful to do this for ordinary linear regression. It is also useful to understand the different transformations that are part of a GLM. I will cover common transformations, and include “Tukey’s ladder” of transformations. 17.3 Non-linear regression Most of the statistics shown in this book focuses on modelling linear relationships between response variables (or their transformed values) and explanatory variables. Sometimes it is useful to explicitly model a known functional form and non-linear regression is a way of doing that. 17.4 Nested or blocked study designs Sometimes a study will have a design that has some inherent structure. This chapter covers how to account for this with linear models. "]
]

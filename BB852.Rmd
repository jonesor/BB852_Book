--- 
title: "BB852 - Data handling, visualisation and statistics"
author: "Owen Jones"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
pandoc_args: --listings
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is the course book for BB852."
papersize: a4paper
---
```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE, message = FALSE
)
```

# Preface

This book has been written to accompany the course, BB852 - planning and evaluation of biological studies.

It is a "work in progress" and will change during the course. The latest version can always be found by downloading it again.

It is designed to be followed along with the course lectures. 

Most subject chapters finish with some exercises to apply the material covered.

The course is divided into three parts:

* Data wrangling
* Data visualisation
* Statistics

## Data wrangling

The term data wrangling covers manipulation of data, for example collected from an experiment or observational study, from its raw form to a form that is ready for analysis, or summarised into tables. It includes reshaping, transforming, filtering and augmenting from other data. This book covers these processes in R mainly using the tools from the `dplyr` and `tidyr` packages.

## Data visualisation

Graphing data is a crucial analytical step that can both highlight problems with the data (e.g. errors and outliers) and can inform on appropriate analytical approaches to take. This book covers the use of `ggplot2` to make high quality, publication-ready plots.

## Statistics

Statistics is a huge field and this book does not attempt to cover more than a small fraction of it. Instead it focusses on (ordinary) linear models and generalised linear models. In a nutshell, linear models model the effects of explanatory variables on a continuous response variable with a gaussian (normal) error distribution while generalised linear models (GLMs) offer a more flexible approach that allows the response variable to have non-normal error distributions. This flexibility allows the more-appropriate modelling of phenomena including integer counts (e.g. number of individuals, or species, or events), binary (0/1) data (e.g. survived/died) or binomial count data (e.g. counts of successess and failures). It is important to realise that most commonly-used statistical methods including t-tests, ANOVA, ANCOVA, n-way ANOVA, and of course linear and multiple regression are all special cases of linear models. 



## General approach

My general approach with communicating these methods and ideas is to teach using examples. Therefore, the bulk of the text here consists of walk-throughs of manipulating, plotting and analysing real data. For the statistics section I focus on communicating the "gist" of the underlying mathematical machinery rather than the nitty-gritty details. If you find yourself interested in these details then there are more specialist textbooks available. 

## Data sources

This book uses numerous data sets in examples, most of which are real datasets obtained from published works, or collected by me.

The data sets can be found at the following link:
https://www.dropbox.com/sh/z8iv9fl9l00bm0w/AAD1WjFwcrr1ERugpunp_YH-a?dl=0

## Acknowledgements

These materials are inspired by the excellent textbook, "Getting Started With R" (2nd edition) by Andrew Beckerman, Dylan Childs and Owen Petchey and Dylan Childs', which is the recommended textbook for BB852, and by materials for the Sheffield University course "AP 240 - Data Analysis And Statistics With R" ([https://dzchilds.github.io/stats-for-bio/]).

## Error reporting

Please report any errors you find (even small ones) to me at [jones@biology.sdu.dk](mailto:jones@biology.sdu.dk)

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Getting acquainted with R

## Introduction

`R` is a programming language for data analysis and statistics. It is free and very widely used. One of its strengths is its very wide user base which means that there are hundreds of contributed packages for every concievable type of analysis. The aim of these introductory classes is to give a basic introduction to the programming language as a tool for importing, manipulating, and exploring data. There will be very little 'statistics', but you will be doing statistics using R during subsequent lectures.

Before proceeding you will need to ensure you have intalled R and RStudio on your computer.


## Getting started with R

I encourage you to do all of your coding from within RStudio.  

In RStudio, create a new "R Script" file. Scripts are essentially programs that can be saved to allow you to return to your work in the future. They also make debugging of errors much easier. You can use the menu to do creat a new R Script, but there's also a keyboard shortcut (Windows: Ctrl+Shift+N; Mac: Cmd+Shift+N). If you save (Windows: Ctrl+S; Mac: Cmd+S), you will be prompted for a file name. Make sure it has the suffix ".R" which denotes an R script file. When you double click on this file in future, it should automatically open in RStudio (if it doesn't you should be able to right-click and select 'Open with...').

In RStudio you can execute commands using the "run" icon at the top of the script window, or by selecting the text and typing the shortcut Ctrl+Enter (Windows) or Cmd+Enter (Mac). Another helpful feature of RStudio is that it will colour-code the syntax that you type, making it easier to read and debug. Note that the colours you see may be different from the ones shown in this handout.

You can customise the look of RStudio using by clicking Tools > Options menu on Windows or  RStudio > Preferences on a Mac. I will point out some of this in the lecture, or you can ask me to show you.

Over the next few pages I will introduce the basics of the R programming language. Try typing them into the scripting window (top left) in RStudio and ensuring that you understand what the commands are doing. It is almost impossible to "break" R by typing the wrong command so I encourage you to experiment and explore the R language I introduce to you here as much as possible - it really is the best way to learn!

## Getting help


`R` features a wealth of commands, which are more properly termed `functions`. You will learn many of these over the next few weeks. Functions often feature a several options which are specified with `arguments`. For example, the function `sum`, has the argument `...`, which is intended to be one or more *vectors* of numbers (see below), and the argument `na.rm`, which is a logical argument specifying whether or not missing values should be removed or not. Usually the arguments have default options which are used you choose not to specify them. In addition, you don't necessarily need to fully-specify the argument if they are specified in the *correct order*. 

You can get `help` on R functions from within R/RStudio with the `?` and `help.search` commands. `?` requires that you know the function name while *help.search* will search all the available help files for a particular word or phrase. `??` is a synonym for *help.search*:

```{r comment = '#', eval=FALSE, echo=TRUE, results='asis', tidy=FALSE}
?rep
help.search("bar plot")
??"bar plot"
```


In RStudio, the help results will appear in the lower right hand area.



## R as a fancy calculator


R features the usual arithmetic operations for addition, subtraction, division, multiplication: 

```{r comment = '#'}
4+3 
9-12
6/3
7*3
(2*7)+2-0.4
```

R also has commands for square root (sqrt), raising to powers (\^), taking the absolute value (abs), and rounding (round), natural log (log), antilog (exp), log to base-10 (log10):

```{r comment = '#'}
sqrt(945)
3^5
abs(-23.4)
round(2.35425,digits=2)
log(1.2)
exp(1)
log10(6)
```
Another thing you can do is evaluate TRUE/FALSE conditions:

```{r}
3<10
5>7
5==5
6!=5
3 %in% c(1,2,3,4,5)
6 %in% c(1,2,3,4,5)
```




## Objects in R


R is an object oriented programming language. This means that it represents concepts as `objects` that have data fields describing the object. These objects can be manipulated by `functions`. Objects can include data, but also models. Don't worry about these distinctions too much for now - all will become clear as you proceed!

Objects are assigned names in R like this. The "<-" command is pronounced "gets" so I would pronounce the following "x gets four":

```{r comment = '#'}
x <- 4
```

To look at any object (function or data), just type its name.
```{r comment = '#'}
x
```

The main data object types in R are: *vectors*, *data frames*, *lists* and *matrices*. We will focus on the first two of these during this course.

A vector is simply a series of data (e.g. the sequence _1, 2, 3, 4, 5_ is a vector, so is the non-numeric sequence _M, F, F, M, M_ ). Each item in a vector is termed an `element`. Therefore, both of these examples contain 5 elements.

There are several ways to create vectors in R. For example, you can make vectors of integers using the *colon (:)* argument (e.g. *1:5*), or vectors of any kind of variable using the *c* function. *c* stands for `concatenate` and simply means to *link (things) together in a chain or series*. Other convenient functions for making vectors are `seq`, which builds a sequence of numbers, and `rep` which builds a vector by repeating elements a specified number of times.

Try the following:
```{r comment = '#'}
A <- 1:5
B <- c(1,3,6,1,7,9)
C <- seq(1,10,2)
D <- seq(1,5,0.1)
E <- rep(c("M","F"),each = 3)
G <- rep(c("M","F"),c(2,4))
```

In these examples, the commands `c`, `seq` and `rep` are *functions*. 

You can *concatonate* vectors using the `c` function. E.g. concatonating the vectors A and B from above:
```{r comment = '#'}
c(A,B)  
```

When functions are applied to vectors, they are applied element-by-element. For example, multiplying a vector will multiply every element in that vector:

```{r comment = '#'}
B
B*3
```

Other manipulations are also done "element-by-element". For example, here we multiply the first element of B by 1, the second by 2, the 3rd by 3 and so on...:

```{r comment = '#'}
B * c(1,2,3,4,5,6)
```

## Missing values, infinity and "non-numbers"


By convention, *missing values* in R are coded by the value "NA". The way that particular functions handle missing values varies: sometimes the NA values are stripped out of the data, other times the function may fail.

For example, if we asked for the mean value of a vector of numbers with an NA value, it will fail:

```{r comment = '#'}
mean(c(1,3,6,1,7,9,NA))
```

In this case you need to specify that any NA values should be removed before calculating the mean:
```{r comment = '#'}
mean(c(1,3,6,1,7,9,NA),na.rm=TRUE)
```

Calculations can sometimes lead to answers that are plus, or minus, infinity. These values are represented in R by Inf or -Inf:

```{r comment = '#'}
5/0
-4/0
```

Other calculations lead to answers that are not numbers, and these are represented by NaN in R:
```{r comment = '#'}
0/0
Inf-Inf
```



## Basic information about objects


You can obtain information about most objects using the `summary` function:

```{r comment = '#'}
summary(B)
```

The functions `max`, `min`, `range`, and `length` are also useful:
```{r comment = '#'}
max(B)
min(B)
range(B)
length(B)
```

## Data frames


Data frames are the usual way of storing data in R. It is more-or-less the same as a worksheet in Excel. A data frame is usually made up of a number of vectors (of the same length) bound together in a single object. You can make a data frame by binding together vectors, or you can import them from outside R.

This example shows the creation of a data frame in R, from 3 vectors:
```{r comment = '#'}
height <- c(173, 145, 187, 155, 179, 133)
sex <- c("Male", "Female", "Male", "Female", "Male", "Female")
age <- c(17, 22, 32, 20, 27, 30)

mydata <- data.frame(height = height, age = age, sex = sex)
mydata
```

Data frames can be summarised using the `summary` function (or the `glimpse` or `str` functions, which give you different views of the same data):

```{r comment = '#'}
summary(mydata)
#glimpse(mydata)
str(mydata)
```

Data frames can be subsetted using the square brackets `[]`, or `subset` functions. With the square brackets, the first number specifies the row number, while the second number specifies the column number:

```{r comment = '#'}
mydata[1,]
mydata[,2]
mydata[1,2]
subset(mydata,sex == "Female")
```

It would be incredibly tedious to enter real data by typing it in like this. Thankfully, R can import data from a range of different data formats. The most commonly used data format is `comma separated value (CSV)` so I will use that. You can also import from Excel, but the data must be formatted in a particular way to enable this (I'll cover this in a later class).

At this point you should create a folder/directory somewhere on your computer called "IntroToR". We will use this as the `working directory` for the remainder of the session. In R you can set the working directory using the `setwd` function. 

Note that file paths in Windows and Apple OSX are expressed differently. Apple systems use the forward-slash (/) to separate folders whereas Windows can use the forward-slash (/) or double-backslash (\\). In windows you also need to define the drive (e.g. C:).

So, to set the working directory in Apple OSX you would use something like this (obviously, you need to put YOUR path!):

```{r comment = '#', eval = FALSE}
setwd("/Users/orj/Desktop/IntroToR")
```

While in Windows the equivalent command would be something like this (both of the following should work):

```{r comment = '#', eval = FALSE}
setwd("C:\\Users\\orj\\Desktop\\IntroToR")
setwd("C:/Users/orj/Desktop/IntroToR")
```

Typing the path in can be annoying but there are ways to speed it up. In Windows you can copy paths from the Windows Explorer location/address bar, or you can hold down the Shift key as you right-click the file, and then choose Copy As Path.

On a Mac you can copy file paths from Finder: Select your file/folder > Right click > Press the option key (on my keyboard this is the `alt` key) and click "Copy X as Pathname"


I can check what the current working directory is using the `getwd` function:
```{r comment = '#', eval = FALSE}
getwd()
```

On the Blackboard site for BB852 I have put a link to a Dropbox folder containing data files for use in the course. In there you should be able to find a file called "carnivora.csv". Download this to your new working directory. You can check the contents of your working directory with the `list.files` function:

```{r comment = '#', eval = FALSE}
list.files()
```

You can now import this file into R using the `read.csv` function. The specification of the argument `header = TRUE` signifies that the fiest row of our CSV file contains the column names. Note that your file path will be different to mine:

```{r comment = '#', eval = TRUE}
carni <- read.csv("CourseData/carnivora.csv",header = TRUE)
```

We can get some basic information on the *carni* data frame using the `summary` function, but also the `dim` and `nrow/ncol` functions:

```{r comment = '#', eval = FALSE}
summary(carni)
```

```{r comment = '#', eval = TRUE, results = 'markup'}
dim(carni)
nrow(carni)
ncol(carni)
```

We can find the names of the columns of a data frame with the `names` function:

```{r comment = '#', eval = TRUE, results = 'markup'}
names(carni)
```

The first few columns are to do with the taxonomic placement of the species (Order, SuperFamily, Family, Genus and Species). There then follow several columns of life history variables: FW = Female body weight (kg),
SW = Average body weight of adult male and adult female (kg), FB = Female brain weight (g), SB = Average brain weight of adult male and adult female (g), LS = Litter size, GL = Gestation length (days), BW = Birth weight (g), WA = Weaning age (days), AI = Age of independance (days), LY = Longevity (months), AM = Age of sexual maturity (days), IB = Inter-birth interval (months).

You can refer to the sub-parts of a `data.frame`(the columns) using the `$` syntax:

```{r comment = '#', eval = TRUE, results = 'markup'}
summary(carni$FW)
```


## Classes in R


I have already mentioned the different object types in R (e.g. vectors and data frames). The object types are technically known as "classes". You can find out what "class" an object is by using the `class` function:

```{r comment = '#', eval = TRUE, results = 'markup'}
class(carni)
```

In this case, the data frame is, unsurprisingly, of class "data.frame". However, the vectors that compose the data frame also have classes. There are several classes of vectors including "integer" (whole numbers), "numeric" (real numbers), "factor" (categorical variables) and "logical" (true/false values).

I expect you have heard of the first two data types, but "factor" might be puzzling. Factors are defined as variables which can take on a *limited* number of different values. They are often referred to as `categorical variables`. For example, in the carnivore dataset, the taxonomic variables are factors. The different values that a factor can take are known as `levels` and you can check on the levels of a vector with the `levels` function.

```{r comment = '#', eval = TRUE, results = 'markup'}
class(carni$Family)
levels(carni$Family)
```

## Tables and summary statistics


For vectors of class "factor" you can use the `table` function to give the counts for each level:

```{r comment = '#', eval = TRUE, results = 'markup'}
table(carni$Family)
```

You can use the function `tapply` ("table apply"), to get more complex summary information. For example, I could ask what the mean female weight (FW) is in each of the families using the argument `mean`:

```{r comment = '#', eval = TRUE, results = 'markup'}
tapply(carni$FW, carni$Family, mean)
```


## Plotting data


Basic plots can be made using the `plot` command. For example, let's have a look at the relationship between log gestation length and log female body weight (see Figure 1, below):

```{r comment = '#', eval = TRUE, results = 'markup', fig.cap=('A simple scatter plot'), fig.show=('asis')}
plot(log(carni$FW), log(carni$GL))
```


<!-- Try the following -->
<!-- ------------------------- -->

<!-- _Make sure you can do the following things successfully. If in doubt please ask for help!_ -->

<!-- Q1. Make a table of the number of species in each Family of the Superfamily *Caniformia*. -->

<!-- Q2. Using the carnivore data set, produce a box plot featuring female weight (FW) for the *Canidae*, *Felidae* and *Ursidae* together on the same plot. Hint: you will need to use `subset` and `droplevels` before plotting. -->

<!-- Q3. Using the carnivore data set, make a table showing the average (mean) birth weight (BW) for Families in Superfamily *Feliformia*. Hint: Use `subset`, `droplevels` and `tapply`. -->

<!-- Q4. Plot the relationship between log female weight (FW) and litter size (LS) in the *Mustelidae*. -->

<!-- Q5. Identify the largest and smallest (by female weight (FW)) species in the *Viverridae* family. What are their brain sizes (FB)? Hint: use subset and the `which.min`/`which.max` functions (use help), and maybe square brackets [ ]. -->




## Exercises - Getting acquainted with R

### Background

In the 1950s-1970s there was rapid growth in the number of houses being built in California, with suburbs sprawling out into the new sites in the countryside. What effect would this have on local bird communities? 

Surveys on bird abundances were carried out in several locations near Oakland, California (Vale & Vale 1976). The locations were of different ages, enabling us to investigate what changes might happen through time. Although there were no surveys before the developments, we can regard the bird abundance in the very youngest developments as the baseline predevelopment condition.

Think about what you might expect to happen to bird species diversity through time in a newly developing suburb.

### The data

The relevant data file is called `suburbanBirds.csv`. This file contains data on bird abundances surveyed in 1975. The columns of the data are `Name` (name of the suburb), `Year` (the year that the suburb was built), `HabitatIndex` (an index of habitat quality, related to tree height, garden maturity etc.), `nIndividuals` (number of indivual birds seen in a standard survey) and `nSpecies` (number of species seen in a standard survey).

Additional surveys found an average species richess of 3.5 in nearby undisturbed habitats of grassland savanna.

### Try the following

1. First import the data. Check that the columns look as they should. (e.g. use `summary` or `str` functions). Tip: use the "Wizard" in RStudio to guide you.

2. What is the mean, minimum, and maximum number of species seen? (there is more than one way to do this)

3. How old are the youngest and oldest suburbs? (hint: the survey was carried out in 1975, do the math!)

4. Plot the relationship between `Year` and `nSpecies` as a scatterplot using base-R graphics (using the `plot` function).

5. The patern might be easier to see if you could replace `YearBuilt` with suburb age. Create a new vector in your data frame for this variable (e.g. `df$Age <- 1975 - Year)`). Replot your results.

6. What do the data show? What might be the mechanisms for the patterns you see? Do they match your expectations?

7. Export your plots and paste them into a Word Document.

8. If you get this far, try plotting the other variables in the dataset.

### References

Vale, T. R., & Vale, G. R. (1976). Suburban bird populations in west-central California. Journal of Biogeography, 157–165.



<!--chapter:end:01-gettingAcquainted.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Data wrangling with dplyr

## Introduction

We are now going to use the package `dplyr` which is designed to make working with data in R easier. 
The package has several key "workhorse" functions, sometimes called **verbs**. These are: `filter`, `select`, `mutate`, `arrange` and `summarise`. I covered these in the lecture, and they are also discussed in the textbook. This worksheet guides you throigh worked examples to illustrate their use.

We will also be using *pipes* from the `magrittr` package. These are implemented using the command `%>%`. 


First you must install the packages (unless you have already done so). Conveniently, everything you need is already in the `Tidyverse` family of packages, so you can install them all like this:

```{r install, eval=FALSE}
install.packages("tidyverse")
```

Note that you only need to do this ONCE! After you have installed them you only need to load them on each session. I usually do this at the start of a script I am writing. 

R will report some information to the screen telling you that it has attached a bunch of packages, and that there are some conflicts for some of them. This is nothing to worry about. It means that there are functions in different packages with the same name so R will get confused. For example, there is a function called `filter` in both the `dplyr` and `stats` package. The `dplyr` package will take priority, but you can still access the other one by using `::` double colons like this to specify which package the function is in (`stats::filter` or `dplyr::filter`).


```{r}
library(tidyverse)
```

To get to know `dplyr` and its functions we'll use a data set 
collected from the university campus at University of Southern Denmark (SDU) The SDU bird project follows the fate of mainly great tits (musvit) and blue tits (blåmejse) in about 100 nest boxes in the woods around the main SDU campus. 

We will address two questions concerning clutch size (the number of eggs laid into the nest) - 

1. How does clutch size differ between blue tits and great tits?
2. How does average clutch size vary among years?

To answer these questions we need to calculate the average clutch size (number of eggs) for each nest in each year. The data are in a file called (`sduBirds.csv`) and are raw data collected while visiting the nests. The data will need to be processed to answer those questions.

Let's import the data and take a look at it. Make sure your data looks OK before moving on.
You should first set up your working directory (e.g. a folder for the course, with a subfolder for course data etc.), and set it (with `setwd`)).
See the earlier material for how to do this, or ask for help.

```{r echo=TRUE}
df <- read.csv("CourseData/sduBirds.csv")
str(df)
```


## select

From the `str` summary (above) you can see that thereare many columns in the data set and we only need some of them. Let's `select` only the columns that we need for our calculations to make things a bit easier to handle. We need the `species`, `Year`, `Day`, `boxNumber` and `nEggs`: 

```{r}
df <- select(df, species, Year, Day, boxNumber, nEggs)
head(df)
```

The output of `head` shows you the first few rows of the data set. You can see that each row represents a visit of a researcher to a particular nest. The researcher records the bird species if it is known (GT = Great tit, BT = Blue tit, NH = Nuthatch etc.), and then records the number of eggs, number of chicks, activity of the adults and so on. We need to convert this huge dataset into one which contains clutch size for each nest, for each year of the study. 

The information given by `str` (above) shows that there are data on species other than our target species. We are only interested in the great tits and blue tits so we can first filter the others out using the `species` variable. 

We can check what the make up of this part of the data is using the `table` function which will count up all of the entries.

```{r}
table(df$species)
```

## filter

Now let's filter this data and double check that this has worked:

```{r}
df <- filter(df,species %in% c("GT","BT"))
table(df$species)
```

You will notice that all the levels of the variable are retained. This is not a problem, and can usually be ignored. You can also tidy this up using the `droplevels` function, which removes all unused factor levels.

```{r}
df <- droplevels(df)
table(df$species)
```



## arrange
Recall that the data are records of visits to each nest a few times per week. To ensure that the data are in time order I can first `arrange` by first `Year` and then `Day`. To illustrate this we can make a temporary data set (called `temp`) to look at a particular nest in a particular year to get a record of the progress for that particular nest, and then plot it (this is an ugly plot and we will learn how to make beautiful ones soon):

```{r}
df <- arrange(df,Year,Day)

temp <- filter(df,boxNumber == 1,Year == 2014)
max(temp$nEggs) # get the max value
plot(temp$Day,temp$nEggs,type="b")
```

Eggs are usually laid one per day, and the clutch size is the maximum number of eggs reached for each nest box. In this case, the clutch size is `r max(temp$nEggs)` eggs. The rapid decline in number of eggs after this peak value shows when the eggs have hatched and the researcher finds chicks instead of eggs!

## summarise and group_by

The next part is the crucial part of our investigation. We need to get the maximum number of eggs seen at each nest. Of course we could repeatedly use `filter`, followed by `max`, for each nest-year combination but this would be incredibly tedious.

Instead, we will use the dplyr `summarise` function to do this by asking for the maximum value of `nEggs`. To make this work we need to first use the `group_by` function tell R to group the data by the variables we are interested in. If we don't do this we just get the overall maximum. We can ungroup the data using the `ungroup` function.

Because there are missing data (`NA` values) we need to specify `na.rm = TRUE` in the argument.

So first, let's get the max per species, just to illustrate how this works:

```{r}
df <- group_by(df,species)
summarise(df,clutchSize = max(nEggs,na.rm= TRUE))
```

We can see how the data are grouped by asking for a summary:

```{r}
summary(df)
```

We can ungroup the data again like this:

```{r}
df <- ungroup(df)
```

So both species lay the same maximum number of eggs, but maybe this is just caused by outliers for one of the species. We'll need to dig deeper.

How can we calculate the average? We cannot simply ask for the `mean` because the data run through time following the development in each nest. We need to calculate the maximum `nEggs` for each nest, and then calculate the average of those. We can do this in two steps. 

We first calculate the clutch size for each box for each species in each year:

```{r}
df <- group_by(df, species, Year, boxNumber)
df <- summarise(df, clutchSize = max(nEggs,na.rm= TRUE))
```

Let's first look at all the clutch size data:

```{r}
hist(df$clutchSize)
```

You can see here that there are a lot of zero values. This is because nests were recorded even if they did not attempt to lay eggs. We should remove these from our data using `filter` again:

```{r}
df <- filter(df, clutchSize > 0)
hist(df$clutchSize)
```

That looks better.
Now we can plot them again but this time split apart the species (again - this plot is ugly and we'll learn to plot nicer ones soon).

```{r}
plot(as.factor(df$species),df$clutchSize)
```

From these distributions it looks like the average clutch size is greater in the blue tit. We can use `summarise` to calculate the means.

```{r}
df <- group_by(df,species)
summarise(df, mean = mean(clutchSize),sd = sd(clutchSize))
```

Lets now turn to the other question - how does the clutch size vary with year?

```{r}
df <- group_by(df,species,Year)
df2 <- summarise(df, meanClutchSize = mean(clutchSize))
head(df2)
```

We can plot this bt first making a plot for blue tits, and then adding the points for great tits. I have used the `pch` argument to use filled circles for the great tits:

```{r}
plot(df2$Year[df2$species == "BT"],df2$meanClutchSize[df2$species == "BT"],type="b",
     ylim=c(0,12),xlab="Year",ylab="Clutch Size")
points(df2$Year[df2$species == "GT"],df2$meanClutchSize[df2$species == "GT"],
       pch = 16, type="b")
```

So it looks like the clutch size varies a fair amount from year to year, but that generally blue tits have large clutch sizes than great tits.

## Using pipes, saving data.

I have walked you through a step-by-step data manipulation. During that process you made made (and replaced) new data sets at each step. In practice this can be dones more smoothly using *pipes* (`%>%`) to pass the result of one function into the next, and the next, and the next...

You'll get some more practice with this as we go on.
Below I show how do do this to create a clutch size data set from the raw data (note that your file path will differ from mine):

```{r}
#Import and process data using pipes
SDUClutchSize <- read.csv("CourseData/sduBirds.csv") %>% 
  filter(species %in% c("GT","BT")) %>% #include only GT and BT
  droplevels() %>% #drop unwanted factor levels
  select(species, Year, Day, boxNumber, nEggs) %>% #select columns needed
  group_by(species, Year, boxNumber) %>% #group data
  summarise(clutchSize = max(nEggs,na.rm=TRUE)) %>% #calculate clutch size (max eggs)
  filter(clutchSize > 0)

head(SDUClutchSize)
```

You can save this out using the `write.csv` function. You will need to set the argument `row.names = FALSE` to stop the data including row names.

```{r}
write.csv(x = SDUClutchSize,file = "CourseData/SDUClutchSize.csv",row.names = FALSE)
```

## Exercises - data wrangling

### Exploring the Amniote Life History Database

In this exercise the aim is to use the "Amniote Life History Database" ^[https://esajournals.onlinelibrary.wiley.com/doi/10.1890/15-0846R.1] to investigate some questions about life history evolution.

The questions are:
(1) what are the records and typical life spans in different taxonomic classes? [*what is the longest, shortest and median life span in birds, mammals and reptiles?*]
(2) is there a positive relationship between body mass and life span? [*do big species live longer than small ones?*]; 
(3) is there a trade-off between reproductive effort and life span? [*do species that reproduce a lot have short lives, so there is a negative relationship between reproduction and life span?*]; 
(4) is this trade-off universal across all Classes? [*does the trade-off exist in birds, reptiles and amphibians?*]

The database is in a file called `Amniote_Database_Aug_2015.csv` in the course data folder. The missing values (which are normally coded as `NA` in R) are coded as "`-999`". The easiest way to take care of this is to specify this when we import the data using the `na.strings` argument of the `read.csv` function. Thus we can import the data like this:


```{r}
amniote <- read.csv("CourseData/Amniote_Database_Aug_2015.csv",na.strings = "-999")
```

Let's make a start...

1. When you have imported the data, use `dim` to check the dimensions of the whole data frame (you should see that there are `r ncol(amniote)` columns and `r nrow(amniote)` rows). Use `names` to look at the names of all columns in the data in `amniote`.


2. We are interested in longevity (lifespan) and body size and reproductive effort and how this might vary depending on the taxonomy (specifically, with Class). Use `select` to pick relevent columns of the dataset and discard the others. Call the new data frame `x`. 
The relevant columns are the taxonomic variables (`class`, `species`) and `longevity_y`, `litter_or_clutch_size_n`, `litters_or_clutches_per_y`, and `adult_body_mass_g`.


3. Take a look at the first few entries in the `species` column. You will see that it is only the *epithet*, the second part of the *Genus_species* name, that is given.  
Use `mutate` and `paste` to convert the `species` column to a *Genus_species* by pasting the data in `genus` and `species` together. To see how this works, try out the following command, `paste(1:3, 4:6)`. After you have created the new column, remove the `genus` column (using `select` and `-genus`).


4. What is the longest living species in the record? Use `arrange` to sort the data from longest to shortest longevity (`longevity_y`), and then look at the top of the file using `head` to find out. (hint: you will need to use reverse sort (`-`)). Cut and paste the species name into Google to find out more!


5. Do the same thing but this time find the shortest lived species.

6. Use `summarise` and `group_by` to make a table summarising `min`, `median` and `max` life spans (`longevity_y`) for the three taxonomic classes in the database. Remember that you need to tell R to remove the `NA` values using a `rm.na = TRUE` argument.

7. Body size is thought to be associated with life span. Let's treat that as a hypothesis and test it graphically. Sketch what would the graph would look like if the hypothesis were true, and if it was false. Plot `adult_body_mass_g` vs. `longevity_y` (using base R graphics). You should notice that this looks a bit messy.

8. Use `mutate` to create a new `log`-transformed variables, `logMass` and `logLongevity`. Use these to make a "log-log" plot. You should see that makes the relationship more linear, and easier to "read".


9. Is there a trade-off between reproductive effort and life span? Think about this as a hypothesis - sketch what would the graph would look like if that were true, and if it was false. Now use the data to test that hypothesis: Use `mutate` to create a variable called `logOffspring` which is the logarithm of number of litters/clutches per year multiplied by the number of babies in each litter/clutch . Then plot `logOffspring` vs. `logLongevity`.



10. To answer the final question (differences between taxonomic classes) you could now use `filter` to subset to particular classes and repeat the plot to see whether the relationships holds universally. 


Remember that if you struggle you can check back to previous work where you have used `dplyr` commands to manipulate data in a similar way. If you get truly stuck, ask for help from instructors or fellow students.





<!--chapter:end:02-dataWrangling.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Combining data sets

[INSERT PREAMBLE ABOUT DATA MANAGEMENT]


In this exercise you will learn how to combine two data sets to create a new one of combined data to answer a conservation-related question: "**Does threat status vary with species' generation times?**" 

This question is crucial to conservation biologists because it helps us to generalise our ideas about what drives extinction risks. In other words, if we can say "species with slow life histories tend to be more threatened" then this gives useful information that can help with planning. For example, imagine we have some species that have not yet been assessed (we don't know if they are threatened or not). Should we focus attention on the one with a short generation time, or long generation time?

To answer this question we will need to import two large data sets, tidy them up a bit and then combine them for analysis.

Let's start with the  "Amniote Life History Database" ^[https://esajournals.onlinelibrary.wiley.com/doi/10.1890/15-0846R.1], which is a good source of life history data. We have encountered this database before. Recall that the missing values (which are normally coded as `NA` in R) are coded as "`-999`". The easiest way to take care of this is to specify this when we import the data using the `na.strings` argument of the `read.csv` function. Thus we can import the data like this:


```{r}
amniote <- read.csv("CourseData/Amniote_Database_Aug_2015.csv",na.strings = "-999")
```

We first need to load the `tidyverse` packages.

```{r,message=FALSE}
library(tidyverse)
```


We can filter on the taxonomic `class` to subset to only mammals. Then, to address our question, we want data on generation time for mammals. Generation time is often measured as the average age at which females reproduce so we can get close to that with `female_maturity_d`. We will first `select` these columns, along with `genus` and `species`. We can combine these two taxonomic variables using `mutate` and `paste` to get our latin binomial species name.

We have previously learned that log transforming such variables is a good thing to do, so we can use `mutate` again to do this transformation.

Finally, we can use `na.omit` to get rid of entries with missing values (which we cannot use). This is not essential, but keeps things more manageable.

```{r}
mammal <- amniote %>%
  filter(class == "Mammalia") %>% #get the mammals only
  select(genus, species, female_maturity_d) %>% #get useful columns 
  mutate(species = paste(genus, species)) %>%
  select(-genus) %>%
  mutate(logMaturity = log(female_maturity_d)) %>%
  na.omit()
```


Let's take a quick look at what we have:

```{r}
head(mammal)
```

Looks good. Now let's import the IUCN Red List data.


```{r}
redlist <- read.csv("CourseData/MammalRedList.csv")
```

Let's take a look at that.

```{r}
names(redlist)
unique(redlist$Red.List.status)
```

There's a lot of information there but what we really need is simply the latin binomial (for which we need `genus` and `species`) and the threat status `Red.List.status`. 

R treats categorical variables (`factor` variables) as alphabetical, but in this case the red list status has a meaning going from low threat (Least Concern - LC) to Critically Endangered (CR) and even Extinct in the Wild (EX) at the other end of the spectrum. We can define this ordering using `mutate` with the `factor` function. 

```{r}
redlist <- redlist %>%
  mutate(species = paste(Genus, Species))%>%
  select(species, Red.List.status) %>%
  mutate(Red.List.status = factor(Red.List.status, 
                                  levels = c("LC","NT","VU","EN","CR","EW","EX")))

head(redlist)
```

Now we can combine this with the life history data from above using `left_join`. 

```{r}
x <- left_join(mammal,redlist,by = "species")
```

Let's take a look at what we have now:

```{r}
head(x)
summary(x)
```

You can see that there are `r sum(is.na(x$Red.List.status))` missing values for the Red List status. These are either species that have not yet been assessed, or maybe where there are mismatches in the species names between the two databases. We will ignore this problem today. 

Before plotting, I will also use `filter` remove species that are extinct (status = "EX" and "EW"). To do this I use the `%in%` argument to allow me to match a vector of variables. Becuase I want to NOT match them I negate the match using `!`. 

I  then ensure that those levels are removed from the variable using `droplevels`.

```{r}
x <- x %>%
  filter(!Red.List.status %in% c("EX","EW")) %>%
  droplevels()
```

Let's now plot the data to answer the question.

```{r}
plot(x$Red.List.status,x$logMaturity,ylab="Maturity")
```

What can we see? If you focus on the median values, it looks like there is a weak positive relationship between this life history trait and threat status: animals with slower life histories tend to be more threatened.

## Exercise - joining datasets

In this exercise you will practice your skills using `gather` to convert a wide-format data sheet into a long-format tidy data set. You will then use `left_join` to combine two data sets for analysis to answer an interesting biological question.

### The data

Data has been collected on great tits (musvit) at SDU for several years. Your task today is to analyse these data to see if the egg laying date is associated with the temperature during the spring. The hypothesis is that warmer springs will lead to delayed egg laying which could have negative consequences to the population if their caterpillar food source doesn't keep pace with the change. 

You are provided with two datasets: one on the birds and another on weather. You will need to process these, and merge them for analysis.

### The bird data

The first data set, `eggDates.csv`, is data from the SDU birds project. The data are arranged in columns where each column is a year and each row is a nest. The data in each column is the day of the year that the first egg in the nest was laid.

These data do NOT fullfil the "tidy data" standard where each variable gets a column. In this case,a single variable (first egg date) gets many columns (one for each year), and column headers are data (the years). The data will need to be processed before you can analyse it.

You will need to use `gather` to fix this issue so that you produce a version of the data with three columns - `nestNumber`, `Year` and `dayNumber`.

### The weather data

The second dataset, `AarslevTemperature.csv`, is a weather dataset from Aarslev near Odense. This dataset includes daily temperatures records for several years. You will need to `summarise` this data to obtain a small dataset that has the weather of interest - average temperature in the months of February to April for each year.

You will then join these datasets together.


1. Import the data and take a look at it  with `head` or `str`.


2. Use `gather` to reformat the data. This might take a bit of trial and error - don't give up! 

Maybe this will help: The first argument in the gather command indicates what the columns in the main data represent (i.e. here the column represents "Year"). The second argument is the name you would give to the actual data (i.e. "day", in this case). The final argument then tells the function which columns of the data are not to be rearranged (i.e. "boxNumber" in this case).

You should end up with a dataset with three columns as described above.


3. Ensure that year is coded as numeric variable using `mutate`. [Hint, you can use the command `as.numeric`, but first remove the "y" in the name using `gsub`].


4. Calculate the mean egg date per year using `summarise` (remember to `group_by` the year first). Take a look at the data.




5. Import the weather data and take a look at it with `head` or `str`.



6. Use `filter` subset to the months of interest (February-April) and then `summarise` the data to calculate the mean temperature in this period (remember to `group_by` year). Look at the data. You should end up with a dataset with two columns - `year` and `meanSpringTemp`. 



7. Join the two datasets together using `left_join`. You should now have a dataset with columns `nestNumber`, `Year`,  `dayNumber` and `meanAprilTemp`


8. plot a graph of `meanAprilTemp` on the x-axis and `dayNumber` on the y-axis.

Now you should be able to answer the question we started with: is laying date associated with spring temperatures.

<!--chapter:end:03-combiningDatasets.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Visualising data with ggplot2

## Introduction

In this worksheet you will be guided through using the `ggplot` package to make some pretty plots. You will need the `tidyverse`.

```{r,message=FALSE}
library(tidyverse)
```

We will use the SDU birds clutch size data that we produced at the end of the data wrangling practical for these examples.
First you must remember to set your working directory, and then import the data.


```{r}
df <- read.csv("CourseData/SDUClutchSize.csv")
```

## Histograms

The `ggplot` function expects two main arguments (1) the data and (2) the **aesthetics**. The aesthetics are the variables you want to plot, and associated characteristics like colours, groupings etc. The first argument is for the data, then the aesthetics are specified within the `aes(...)` argument. These usually include an argument for `x` which is normally the variable that appears on the horizontal axis, and (often) `y` which is usually the variable on the vertical axis. The details of this depend on the type of plot you are making.

After setting up the plot the graphics are added as **geometric layers** or **geoms**. There are many of these available including `geom_histogram`, `geom_line`, `geom_point` etc.

I will illustrate the construction of a simple plot by making a histogram of the clutch size of all the nests in the dataset.

```{r}
ggplot(df, aes(x = clutchSize))
```

This produces an empty plot because we have not yet specified what kind of plot we want. We want a histogram, so we can add this as follows. I have set binwidth to be 1 because we know we are dealing with counts between just 1 and 14. Try altering the binwidth.

```{r}
ggplot(df, aes(x = clutchSize)) +
  geom_histogram(binwidth = 1)
```
We know that we have two species here and we would like to compare them. This is done within the aesthetic argument. The default is that the bars for different categories are stacked on top of eachother. This is good in some cases, but probably not here.

```{r}
ggplot(df, aes(x = clutchSize,fill = species)) +
  geom_histogram(binwidth = 1,position = "dodge")
```

You can immediately see that there are far fewer blue tit nests than great tit ones. But you can also see that the center of mass for blue tits is further to the right than great tits.

To make it easier to compare distributions with very different counts, we can put density on the y-axis instead of the default count using the argument `stat(density)`.


```{r}
ggplot(df, aes(x = clutchSize, fill = species, stat(density))) +
  geom_histogram(binwidth = 1,position = "dodge")
```

An alternative approach would be to overlay the two sets of bars (using `position = "identity"`) and set the colours to be slightly transparent (using `alpha = 0.7`) so that you can see the overlapping region clearly. 

```{r}
ggplot(df, aes(x = clutchSize, fill = species, stat(density))) +
  geom_histogram(binwidth = 1,position = "identity",alpha=0.7)
```

It is very clear from this plot that blue tits tend to have bigger clutch sizes than great tits. Is this difference *statistically significant*? We will look at testing this in a future class - for now we will be satisfied with our visualisation.

## "Facets" - splitting data across panels

You should recall that there were several years of data represnted here.
`ggplot` has a very clever way of splitting up the plot to examine this.

```{r}
ggplot(df, aes(x = clutchSize, fill = species, stat(density))) +
  geom_histogram(binwidth = 1,position = "identity",alpha=0.7) + 
  facet_grid(.~Year)
```

You could split the data up by species in a similar way, as yet another way of visualising the difference between species:

```{r}
ggplot(df, aes(x = clutchSize)) +
  geom_histogram(binwidth = 1) + 
  facet_grid(species~.)
```

You can change whether the separate graphs are presented in a rows or columns by changing the order of the argument: ` facet_grid(species~.)` or ` facet_grid(.~species)`. Try it.

## box plots

Box plots are suitable for cases where one variable is categorical with 2+ levels, and the other is continuous. Therefore, another way to look at these distributions is to use a box plot.

In a box plot the box shows the quartiles (i.e. the 25% and 75% quantiles) within which 50% of the data are found. The horizontal line in the box is the *median*, Then the whiskers extend from the smallest to largest value *unless they are further than 1.5 times the interquartile range (the length of the box)* away from the edge of the box, in which case they are individually shown as outlier points.

To plot them using `ggplot` you must use a `geom_boxplot` layer. 
The categorical variable is normally placed on the x-axis so is placed as `x` in the `aes` argument, while the continuous variable is on the `y` axis.

```{r}
ggplot(df, aes(x = species, y = clutchSize)) +
         geom_boxplot()
```


Some researchers argue that it is a good idea to add the data as points to these plots as "full disclosure" of what the underlying data look like. These can be added with a `geom_jitter` layer (jitter is random noise added in this case to the horizontal axis). You should set `width` and `alpha` arguments to make it look nice.

```{r}
ggplot(df, aes(x = species, y = clutchSize)) +
  geom_boxplot() + 
  geom_jitter(width = .2, alpha = 0.5, colour="black",fill="black")
  
```


Try splitting the data into different years using `facet_grid` with the box plot.

## lines and points

Perhaps not surprisingly lines and points can be added with the geoms, `geom_line` and `geom_point` respectively.
To illustrate this we will make a plot showing how clutch size changes among years.
First we will use `summarise` to create a dataset with the mean clutch size. We'll start simply, by looking at only great tits.


```{r}
GTclutch <- df %>% 
  filter(species == "GT") %>%
  group_by(Year) %>% 
  summarise(meanClutchSize = mean(clutchSize))
```

Then you can plot this like this.

```{r}
ggplot(GTclutch, aes(x = Year, y = meanClutchSize)) +
         geom_line()
```

I think this looks OK, but we should add both species. I'll first need to produce a mean clutch size dataset that includes both species.


```{r}
meanClutch <- df %>% 
  group_by(species,Year) %>% 
  summarise(meanClutchSize = mean(clutchSize))
```

Now I can do the plot again. The only difference to the command is that I need to tell R that I want to colour the lines by species (`colour = species`).

```{r}
ggplot(meanClutch, aes(x = Year, y = meanClutchSize, colour = species)) +
         geom_line()
```

I can improve on this by (1) changing the y axis limits (using `ylim`) so that it goes through the full range of my data (0 - 14); (2) adding points (using a `geom_point` layer) where my actual data values are; (3) adding a nicely formatted axis label (using `ylab`); adding a title (`ggtitle`)

```{r}
ggplot(meanClutch, aes(x = Year, y = meanClutchSize, colour = species)) +
  geom_line() + 
  geom_point() +
  ylim(0,14) + 
  ylab("Mean clutch size") +
  ggtitle("Clutch size data from SDU Campus")
  
```

## scatter plot

Finally, lets make a scatter plot.
The SDU bird data are not suitable for this type of plot so we'll use the data from a few days ago on suburban bird diversity.

```{r}
df <- read.csv("CourseData/suburbanBirds.csv")
```

Take a look at the data to remind ourselves what it looks like

```{r}
head(df)
```

These data show the result of standardised bird surveys at housing developments of different ages in California. The surveys were carried out in 1975, and the data includes the `Year` and number of individual birds seen `nIndividuals` and number of species seen `nSpecies`. The question being addressed is "How does the age of the housing development affect the number of species?"

To investigate this we should first add a new variable for `Age` to the data set. We can do this using the `mutate` function from `dplyr`. This function creates new variables, for example by manipulating existing ones.

```{r}
df <- mutate(df,Age = 1975 - Year)
```

When we have created this variable we can plot the data. For aesthetic reasons I also would like to set the limits on the y-axis to go extend to zero, and I would like to include proper labels on the axes.

```{r}
ggplot(df, aes(x = Age,y = nSpecies)) + 
  geom_point() + 
  ylim(0,15) + 
  xlab("Age of development") + ylab("Bird species richness")
```

This shows very clearly that older developments have more species, but it also appears to show that there is an asymptote around 13 species.

Compare this plot to the one you made with base graphics in a previous class.


<!--chapter:end:04-visualisingData.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Distributions and summarising data

## Introduction

This worksheet covers two broad topics. First it covers the concept of distributions; Secondly it covers summarising data. We'll use some functions from the `tidyverse` so you should load those libraries before continuing.

```{r, message=FALSE}
library(tidyverse)
```


## Distributions

A statistical distribution is a description of the relative number of times (frequency) possible outcomes will occur in many trials. They are important because (1) they are useful descriptors of data and (2) they form the basis for assumptions in some statistical approaches. For example, normal distribution is often assumed. The normal distribution is symmetrical (centered on the mean) and 68% of observations fall within 1 standard deviation, and 95% of observations fall within 2 s.d..

```{r echo = FALSE, fig.height=3, fig.width = 4,fig.align='center'}
normData <- data.frame(x = seq(-3,3,0.1)) %>% 
  mutate(dens = dnorm(x ,mean = 0, sd=1)) %>%
  mutate(sd1 = ifelse(x <= 1 & x >= -1,TRUE,FALSE)) %>%
  mutate(sd2 = ifelse(x <= 2 & x >= -2,TRUE,FALSE))

ggplot(normData,aes(x=x,y=dens))+
  geom_line() + 
  geom_ribbon(data = normData %>% filter(sd2 == TRUE),
              aes(x = x, ymax = dens, ymin = 0),fill = '#00517f') +
  geom_ribbon(data = normData %>% filter(sd1 == TRUE),
              aes(x = x, ymax = dens, ymin = 0),fill = '#0072B2') +
  geom_line()+
  geom_segment(aes(x = -1, xend = 1,
                   y = .22, yend = .22), 
               colour = "#FFFFFF", size = 1.5) +
  geom_label(aes(label = '1 SD: 68%', x = 0, y = .22), 
             colour = 'gray10', fill = '#FFFFFF') +
  geom_segment(aes(x = -2, xend = 2,
                   y = 0.05, yend = 0.05), 
               colour = "#FFFFFF", size = 1.5) +
  geom_label(aes(label = '2 SD: 95%', x = 0, y = 0.05), 
             colour = 'gray10', fill = '#FFFFFF') +
  xlab("Standard deviation (SD)") + 
  ylab("Density") +
  scale_x_continuous(breaks = seq(from = -4, to = 4, by = 1))+
  theme_minimal() +
  ggtitle("The normal distribution")
  
```


We will use R to simulate some distributions, and explore these to get a feel for them.
R has functions for generating random numbers from different kinds of distributions. For example, the function `rnorm` will generate numbers from a normal distribution and `rpois` will generate numbers from a Poisson distribution.

## Normal distribution

The `rnorm` function has three arguments. The first argument is simply the number of values you want to generate. Then, the second and third arguments specify the the mean and standard deviation values of the distribution (i.e. where the distribution is centered and how spread out it is).

The following command will produce 6 numbers from a distribution with a mean value of 5 and a standard deviation of 2. 

```{r}
rnorm(6,5,2)
```
*Try changing the values of the arguments to alter the number of values you generate, and to alter the mean and standard deviation.*

Let's use this to generate a larger data frame, and then place markers for the various measures of "spread" onto a plot. *Note that here I put a set of parentheses around the plot code to both display the result AND save the plot as an R object called `p1`*

```{r, message=FALSE, fig.height=3, fig.width = 4,fig.align='center'}
rn <- data.frame(d1 = rnorm(500,5,2))
summary(rn) #Take a look

#Plot the data
(p1 <- ggplot(rn,aes(x=d1)) + 
  geom_histogram()
  )
```

We can calculate the mean and standard deviation using `summarise` (along with other estimates of "spread"). The mean and standard deviation values will be close (but not identical) to the values you set when you generated the distribution. 

*Note that here I put a set of parentheses around the code to both display the result AND save the result in an object called `sv`*

```{r, message=FALSE}
(sv <- rn %>% 
  summarise(meanEst = mean(d1),
            sdEst = sd(d1),
            varEst = var(d1),
            semEst = sd(d1)/sqrt(n()))
 )
```

Let's use the function `geom_vline` to add some markers to the plot from above to show these values...

```{r, message=FALSE, fig.height=3, fig.width = 4,fig.align='center'}
(p2 <- p1 + 
  geom_vline(xintercept = sv$meanEst, size=2) + #mean
  geom_vline(xintercept = sv$meanEst+sv$sdEst, size=1) + #upperSD
  geom_vline(xintercept = sv$meanEst-sv$sdEst, size=1) #lowerSD
)
```

We can compare these with the true values (the values we set when we generated the data), by adding them to the plot in a different colour (mean=5, sd=2).

```{r, message=FALSE, fig.height=3, fig.width = 4,fig.align='center'}
(p3 <- p2 + 
  geom_vline(xintercept = 5, size=2, colour="red") + #mean
  geom_vline(xintercept = 5+2, size=1,colour="red") + #upperSD
  geom_vline(xintercept = 5-2, size=1,colour="red") #lowerSD
 ) 

```

*Try repeating these plots with data that has different sample sizes. For example, use sample sizes of 5000, 250, 100, 50, 10. What do you notice? You should notice that for smaller sample sizes, the true distribution is not captured very well.* 


When you calculate the mean and standard deviation, you are actually fitting a simple model: the mean and sd are parameters of the model, which assumes that the data follow a normal distribution.


*Try adding lines for the standard error of the mean to one of your histograms.*


## Compare normal distributions

Because normal distributions all have the same shape, it can be hard to grasp the effect of changing the distribution's parameters viewing them in isolation. In this section you will write some code to compare two normal distributions. This approach can be useful when considering whether a proposed experiment will successfully detect a difference between treatment groups. We'll look at this topic, known as "power analysis", in greater detail in a later class. For now we will simply use `ggplot` to get a better feel for the normal distribution.

Let's use `rnorm` to generate a larger data frame with two sets of numbers from different distributions: (d1: mean = 5, sd = 2; d2: mean = 8, sd = 1). 

```{r, message=FALSE}
rn <- data.frame(d1 = rnorm(500,5,2),d2 = rnorm(500,8,1))
summary(rn)
```

The summaries (above) show that the mean and the width of the distributions vary, but we should always plot our data. So lets make a plot in `ggplot`. In the dataset I created I have the data arranged by columns side-by-side, but `ggplot` needs the values to be arranged in a single column, and the identifier of the sample ID in a second column. I can use the function `gather` to rearrange the data into the required format.

```{r,message=FALSE, fig.height=3, fig.width = 4,fig.align='center'}
rn <- gather(rn,key = "sampleID",value = "value") #rearrange data

#Plot histograms using "identity", and make them transparent
ggplot(rn,aes(x = value,fill=sampleID)) +
  geom_histogram(position = "identity", alpha=0.5)
```

Try changing the distributions and replotting them (you can change the number of samples, the mean values and the standard deviations).

## The Poisson distribution

The Poisson distribution is typically used when dealing with count data. The values must be whole numbers (integers) and they cannot be negative. The shape of the distributions varies with the "`lambda`" parameter. Small values of lambda give more skewed distributions.

```{r, echo=FALSE, fig.height=3, fig.width = 4,fig.align='center'}
poissonData <- data.frame(x = seq(0,10,1)) %>% 
  mutate(dens = dpois(x ,lambda = 2))

L2 <- ggplot(poissonData,aes(x = x,y = dens)) + 
  geom_point() + 
  geom_segment( aes(x=x, xend=x, y=0, yend=dens)) +
  scale_x_continuous(breaks = 0:10)+
  xlab("x") + 
  ylab("Density") + 
  theme_minimal()

poissonData <- data.frame(x = seq(0,10,1)) %>% 
  mutate(dens = dpois(x ,lambda = 5))

L5 <- ggplot(poissonData,aes(x = x,y = dens)) + 
  geom_point() + 
  geom_segment( aes(x=x, xend=x, y=0, yend=dens)) +
  scale_x_continuous(breaks = 0:10)+
  xlab("x") + 
  ylab("Density") +
  theme_minimal()

ggpubr::ggarrange(L2+ggtitle("lambda = 2"),L5 + ggtitle("lambda = 5"),ncol=2,nrow=1)

```


Let's generate and plot some Poisson distributed data.


```{r, message=FALSE, fig.height=3, fig.width = 4,fig.align='center'}
rp <- data.frame(d1 = rpois(500,2.4))
summary(rp) #Take a look

#Plot the data
(p1 <- ggplot(rp,aes(x=d1)) + 
  geom_histogram(binwidth = 1) # we know the bins will be 1
  )
```
*Try changing the value of lambda and look at how the shape changes.*

Let's calculate summary statistics of mean and standard deviation for this distribution

```{r, message=FALSE}
(sv <- rp %>% 
  summarise(meanEst = mean(d1),
            sdEst = sd(d1))
 )
```
Now lets plot the mean and the 2 times the sd on the graph. Remember that for the normal distribution (above) that 95% of the data were within 2 times the sd.

```{r, fig.height=3, fig.width = 4,fig.align='center'}
p1 + 
  geom_vline(xintercept = sv$meanEst, size=2) + #mean
  geom_vline(xintercept = sv$meanEst+2*sv$sdEst, size=1) + #upper2SD
  geom_vline(xintercept = sv$meanEst-2*sv$sdEst, size=1) #lower2SD

```

This looks like a TERRIBLE fit: The mean is not close to the most common value in the data set and the lower limit of the s.d. indicates we should expect some negative values - this is impossible for Poisson data. The reason for this is that mean and standard deviation, and therefore standard error, are intended for normally distributed data. When the data come from other distributions we must take another approach.

So how should we summarise this data?

One approach is to report the median as a measure of "central tendency" instead of the mean, and to report "quantiles" of the data along with the range (i.e. minimum and maximum). Quantiles are simply the cut points that divide the data into parts. For example, the 25% quantile is the point where (if the data were arranged in order) one quarter of the values would fall below; the 50% quantile would mark the middle of the data (= the median); the 75% quantile would be the point when three-quarters of the data are below. You can calculate those things using `dplyr`'s `summarise`. However, you can also simply use the base R `summary` command.

```{r}
(sv <- rp %>% 
  summarise(minVal = min(d1),
            q25 = quantile(d1,0.25),
            med = median(d1),
            q75 = quantile(d1,0.75),
            maxVal = max(d1))
 )

#base R summary is just as good.
summary(rp$d1)
```

## Comparing normal and Poisson distributions

To get a better feel for how these two distributions differ, lets use the same approach we used above to plot two distributions together.

```{r,fig.height=3, fig.width = 4,fig.align='center'}
rn <- data.frame(normDist = rnorm(500,2,2),poisDist = rpois(500,2.4))
summary(rn)
rn <- gather(rn,key = "sampleID",value = "value") #rearrange data

#Plot histograms using "identity", and make them transparent
ggplot(rn,aes(x = value,fill=sampleID)) +
  geom_histogram(position = "identity", alpha=0.5,binwidth = 1)
```

*Try changing the arguments in the `rnorm` and `rpois` commands to change the distributions.* 

Finally, let's take another view of these data and look at them using box plots. Box plots are a handy alternative to histograms and many people prefer them.

```{r,fig.height=3, fig.width = 4,fig.align='center'}
ggplot(rn,aes(x=sampleID, y = value,fill=sampleID)) +
  geom_boxplot()
```

You should see the main features of both distributions are captured pretty well. The normal distribution is approximately symetrical and the Poisson distribution is skewed (one whisker longer then the other) and cannot be <0. Which graph to you prefer? (there's no right answer!)




<!--chapter:end:05-distributionsSummaryStats.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Customising your plots

## Introduction 

The purpose of this worksheet it to learn how to customise plots made with `ggplot` to improve "readability", or just for aesthetic reasons. 

We will cover the following:

1. Modifying axes (log transform, different tick marks/ranges etc.).
2. Colour schemes .
3. "Themes" - built-in sets of styles.
4. Multiple sub-plots in a plot.
5. Saving your plots.

As usual we need to install `tidyverse`.

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
library(tidyverse)
```

For these examples I will use the dataset on animal life history, Anage. 

```{r import data,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
x <- read.csv("CourseData/anage_data.csv")
```
You can remind yourself what this data looks like using commands like `summary`, `str` and `names`.

I will process the data a bit to make it easier to work with. One of the commands might be new to you - `rename`. This is simply a way of renaming columns, in this case to make them more "user friendly" (e.g. I want to rename the column "Metabolic.rate..W." to "BMR" (for basal metabolic rate)).

I will also use `mutate` to (1) convert the Mass from grams to kilograms and (2) to make a new variable called "BMRperKg" which standardises metabolic rate by expressing it as rate per kilogram.

```{r process data,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}

anage <- x %>% 
  mutate(Species = paste(Genus,Species)) %>%
  rename(Longevity = "Maximum.longevity..yrs.",
         Mass = "Body.mass..g." ,
         BMR = "Metabolic.rate..W.") %>%
  select(Class, Order, Species,Mass,Longevity,BMR) %>%
  filter(Class %in% c("Aves","Amphibia","Mammalia","Reptilia")) %>% 
  droplevels() %>% #this removes unused "factor levels" e.g. "Insecta"
  mutate(Mass = Mass/1000,
         BMRperKg = BMR/Mass) 

summary(anage)  
```

```{r, echo=FALSE,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
png("basePlot.png")
plot(log(anage$Mass),log(anage$BMR))
dev.off()
```


## A basic plot

Now lets start with a basic plot. You will see a warning about removing rows with missing values. This is just a warning to let you know that there are missing (`NA`) values in the data you are plotting.

```{r, echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(p1 <- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + 
  geom_point(alpha=0.3)) #use alpha argument to make points transparent
```

## Axis limits

These points are really spread out. One option to deal with this might be to set the range over which the axes are allowed to go using `xlim` and `ylim`.

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p1 + 
  xlim(0,500) + 
  ylim(0,300)
```

## Transforming the axis (log scale)

In this particular case though use of a log scale would be best because even after focusing on a smaller part of the range of values you can see that the points are still concentrated at smaller values. In a moment, you will also see that log-transforming the data makes the cloud of points pleasingly linear.

You can set a log scale by using the commands `scale_x_continuous(trans = "log")` and `scale_y_continuous(trans = "log")`.

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(p2 <- p1 + 
  scale_x_continuous(trans = "log") +
  scale_y_continuous(trans = "log"))
```

```{r,echo = FALSE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(ggplot(anage,aes(x = log(Mass), y = log(BMR), colour = Class)) + 
   geom_point(alpha=0.3))
#ggsave("basicGgplot.png",width = 12, height=7,units="cm")
```


## Changing the axis tick marks

This looks nice. But the numbers on the axis are not very nice. Using `summary(anage$BMR)`  tells us that the range of data is from 0.0001 to 2336.5. We could place tick marks anywhere on this axis, but let's try 0.0001, 0.001,0.1, 1,10, 100, 1000.

```{r, echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(p2 <- p1 + 
  scale_x_continuous(trans = "log") +
  scale_y_continuous(trans = "log", breaks = c(0.0001,0.001,0.01,0.1,1,10,100,1000)))
```

Using `summary(anage$Mass)`  tells us that the range of data is from 0.001 to 3672. We could place tick marks anywhere on this axis, but let's try 0.001,0.1, 1,10, 100, 1000.

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(p2 <- p1 + 
  scale_x_continuous(trans = "log", 
                     breaks = c(0.001,0.01,0.1,1,10,100,1000)) +
  scale_y_continuous(trans = "log", 
                     breaks = c(0.0001,0.001,0.01,0.1,1,10,100,1000))
 )
```

## Axis labels

Now, let's think about the axis labels. The labels in the plots so far have no units indicated, and might not be easy to interpret for the reader. Let's add units, and also spell out more fully what "BMR" and "Mass" means (the axes is basal metabolic rate in Watts and adult body mass in kg).

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(p3 <- p2 + 
  xlab("Adult body mass (kg)") + 
  ylab("Basal metabolic rate (W)")
)
```


## Colours

What about those colours? The `ggplot` package uses some default colours that are OK, but sometimes you will want to make a change. 

You can "manually" adjust colours using the `scale_colour_manual` function. You can either name individual colours (e.g. "red","green","orange","black")^[See https://www.r-graph-gallery.com/42-colors-names.html], or you can find their so-called "hex-codes" from a site like http://colorbrewer2.org/ or https://htmlcolorcodes.com/color-picker/. You can add a two digit number after the hex code to set the "opaqueness" of the colour. For example "#FF000075" is red, with 75% opacity.

With colour names...

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  scale_colour_manual(values = c("red","green","orange","black"))
```


And with some hex codes...

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  scale_colour_manual(values = c("#33FF6475","#3368FF75","#FF33CE75","#FFCA3375"))
```

Another alternative is to use some of `ggplot`'s built in "palettes" of colour combinations. For example, there are several palettes called "`viridis`".

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  scale_colour_viridis_d(option = "D")
```

*Try using other `option` arguments `A`, `B`, `C` and `E`. Try also adding an argument for transparency `alpha = 0.5`.*

Here's a couple more palettes. There's one for shades of grey...


```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  scale_colour_grey()
```

There's another one for various colour schemes, called "colour brewer". *Try using "RdGy", "RdYlBu" and "Spectral"* see `?scale_colour_brewer` for more options.

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  scale_colour_brewer(palette = "BrBG")
```

## Themes

Finally, `ggplot` includes the option to set a  **theme** for the plots. "Themes"" make adjustments to the "look" of the plot.  It is possible to write your own themes, but I recommend to use some ready-made ones. You can implement them by adding them as you would any other addition to the `ggplot` command (e.g. `+ theme_light()`. 

There are several themes included with `ggplot`. Try my favourite, `theme_minimal()`. Then try `theme_classic()` and `theme_dark()`.

```{r,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
(p4 <- p3 + 
  theme_minimal()
)
```
For more theme fun, you can install packages that include more themes. The best one is called `ggthemes` (remember that you only need to install the package once). Try `theme_economist()`, `theme_tufte()` and (ugh!) `theme_excel()`. You can see what other themes there in this package at https://jrnold.github.io/ggthemes/reference/index.html (some of them are really ugly in my opinion!).

```{r,eval=FALSE,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
install.packages("ggthemes")
```
```{r,eval=TRUE,echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
library(ggthemes)
p3 + 
  theme_economist()
```

This package also includes some useful colour scales, including some for colour blind people.

```{r, echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  scale_color_colorblind()
```

## Moving the legend

By default, the legend is placed on the right. You can move it around by adding a `theme` argument to your plot commands. It can also be placed on the "`top`", "`bottom`", or "`left`". You can also remove the legend altogether by using `legend.position = "none"`. You might also want to remove the legend title using the theme argument `legend.title = element_blank()`. 

```{r, echo = TRUE,message=FALSE, fig.height=3, fig.width = 5,fig.align='center',message = FALSE,warning = FALSE}
p3 + 
  theme(legend.position  = "bottom")
```


## Combining multiple plots

It is often useful to combine two or more plots into a single figure. For example, many journals have strict limits on the number of plots so it is useful to combine plots into "Figure 1A and B" etc.

There are several R packages that can do this and my favourite is called `ggpubr`. The important function in the package is called `ggarrange`.

```{r,eval = FALSE}
install.packages("ggpubr") #only need to do this once
```

```{r}
library(ggpubr)
```

I will illustrate it by first making another plot, this time showing the relationship between body mass and *standardised* BMR (BMR per kg). Because I am combining the plots into a smaller space I have decided to remove the figure legend (I could put it in the figure caption instead).

```{r}
#PlotA (this is what you have already created above)
plotA <- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) +
  geom_point(alpha=0.5) + 
  scale_x_continuous(trans = "log",breaks =c(0.001,0.01,0.1,1,10,100,1000)) + 
  scale_y_continuous(trans = "log",breaks =c(0.0001,0.001,0.01,0.1,1,10,100,1000)) +
  xlab("Adult body mass (kg)") + 
  ylab("Basal metabolic rate (W)") +
  theme_minimal() +
  theme(legend.position  = "bottom",
        legend.title = element_blank())

(plotB <- ggplot(anage,aes(x = Mass, y = BMRperKg, colour = Class)) +
    geom_point(alpha=0.5) + 
    scale_x_continuous(trans = "log",breaks =c(0.001,0.01,0.1,1,10,100,1000)) +
    scale_y_continuous(trans = "log",breaks =c(0.0001,0.001,0.01,0.1,1,10)) + 
    xlab("Adult body mass (kg)") + 
    ylab("Basal metabolic rate (W/kg)") +
    theme_minimal() + 
    theme(legend.position  = "none")
) #This one is wrapped in brackets so that R shows it
```

Now I can combine these using the `ggarrange` function from `ggpubr`. I can add titles to the sub-plots using the command `ggtitle`.

```{r,fig.height = 4,fig.width=7}
(AB <- ggarrange(plotA + ggtitle("A"), plotB + ggtitle("B")))
```
You caould place the sub-plots on top of eachother using `nrow` or `ncol` (specifying the number of rows/columns) like this.

```{r,fig.height = 7,fig.width=4}
(AB2 <- ggarrange(plotA + ggtitle("A"), plotB + ggtitle("B"),nrow=2))
```


```{r, echo=FALSE}
(ggarrange(plotA + 
             ggtitle("A") +
             scale_colour_brewer(palette = "BrBG"),
           plotB + 
             ggtitle("B")+
             scale_colour_brewer(palette = "BrBG")))

#ggsave("betterGgplot.png",width = 18, height=10,units="cm")

(ggarrange(plotA + 
             ggtitle("A") +
             scale_colour_brewer(palette = "BrBG")+
             theme_tufte()+
             theme(legend.position  = "none"),
           plotB + 
             ggtitle("B")+
             scale_colour_brewer(palette = "BrBG")+
             theme_tufte()+
             theme(legend.position  = "none")))

#ggsave("betterGgplot2.png",width = 20, height=10,units="cm")

 
(ggarrange(plotA + 
             ggtitle("A") +
             scale_colour_brewer(palette = "BrBG")+
             theme_clean()+
             theme(legend.position  = "none"),
           plotB + 
             ggtitle("B") +
             scale_colour_brewer(palette = "BrBG")+
             theme_clean()+
             theme(legend.position  = "none")))

#ggsave("betterGgplot3.png",width = 18, height=10,units="cm")

```


## Saving your plot

You should, I think, already know about using the "Export" button in RStudio to save out your plot. This is useful and easy, but you should know that you can also save the plots using a typed command (`ggsave`) in your script. This command is handy because it allows you to automatically set the size, and file name of your plot. 

The default setting for `ggsave` is that it will save the last plot that was printed to your computer screen. Therefore easiest way to use the command is to simply place the `ggsave` command immediately after your `ggplot` command. Otherwise, you can use the `plot` argument to tell `ggsave` which plot you want to save.

The command can save to various file types including `png`, `jpeg`, `pdf` (see the `ggplot` help file for more). R knows what file file type is chosen by checking the file extension in the file name (e.g. `.png`). I advise to use `png`.

You should also set the width and height of the plot and the units (the default is inches). It usually takes a few attempts and a bit of trial-and-error to choose the dimensions so that the plot looks nice.

```{r,eval=FALSE}
ggsave("MySavedPlot1.png", plot = AB, width = 18, height=10, units = "cm")

ggsave("MySavedPlot2.png", plot = AB2, width = 10, height=18, units = "cm")
```


## Final word on plots

We have covered a lot of ground here. There is a lot to learn, but don't feel like you have to remember all of these commands (I don't). Mostly it is simply a case of remembering that it is *possible* to do these things, and knowing where to look up the commands. Obvious starting points are these worksheets, and the text book (including the online version!). You can also usually find help by Googling "ggplot" followed by what you are trying to do (e.g. "ggplot change axis ticks"). One of my frequently used web sites is this one http://www.sthda.com/english/ which has an extensive section on `ggplot` (http://www.sthda.com/english/wiki/ggplot2-essentials).

Even though we have covered a lot of ground we have still only gotten a taster of what `ggplot` is capable of. I encourage you to learn more. A useful resource for learning is the online R graph gallery" at https://www.r-graph-gallery.com/, which shows you how to make and modify many types of plot.


<!-- ## A note about the order of point layers -->

<!-- Sometimes the points end up obscuring each other. For example, here the points for birds (Aves) are mostly hidden by the points for mammals. This happens because `ggplot` places the points in the order that they appear in the data frame. Therefore, one way to deal with that would be to change the order of the rows (e.g. so Aves appears last) or to randomise the rows using a new command called `slice` in concert with the `sample` function. -->


<!-- ```{r} -->
<!-- anage <- anage %>% -->
<!--   slice(sample(1:n())) -->

<!-- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + -->
<!--   geom_point(alpha=0.5) +  -->
<!--   scale_x_continuous(trans = "log",breaks =c(0.001,0.01,0.1,1,10,100,1000)) +  -->
<!--   scale_y_continuous(trans = "log",breaks =c(0.0001,0.001,0.01,0.1,1,10,100,1000)) -->
<!-- ``` -->

<!-- Another way would be to create a "helper" variable (e.g. `plotOrder`) and then arrange the data by this variable. This is a bit fiddly, but it gets the job done. -->

<!-- ```{r} -->
<!-- anage <- anage %>% -->
<!--   mutate(plotOrder = as.numeric(Class)) %>% -->
<!--   arrange(-plotOrder) -->


<!-- ggplot(anage,aes(x = Mass, y = BMR, colour = Class)) + -->
<!--   geom_point(alpha=0.5) +  -->
<!--   scale_x_continuous(trans = "log",breaks =c(0.001,0.01,0.1,1,10,100,1000)) +  -->
<!--   scale_y_continuous(trans = "log",breaks =c(0.0001,0.001,0.01,0.1,1,10,100,1000)) -->
<!-- ``` -->

<!--chapter:end:06-pimpYourPlots.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Randomisation Tests

## Introduction

Simple experiments testing for a difference in mean values between two groups usually have the null hypothesis that there is no difference. The alternative hypothesis varies. Sometimes it is simply that the two groups are different (and that the difference could be wither positive or negative). In other cases the alternetive hypothesis is that the mean of Group A is less then the mean of Group B (or that it is greater).

Randomisation tests are an intuitive, but computationally intensive way of testing these hypotheses. They have a long history and were first proposed by R.A. Fisher in the 1930s. However they only became convenient when computers became sufficiently fast to do the calculations.

Carrying out a test in R requires that you put your `dplyr` skills to the test. Here you will be guided through an example.

## The example

A new drug has been developed that is supposed to reduce cholesterol levels in men. An experiment has been carried out where 12 human test subjects have been assigned randomly to two groups: "Control" and "Drug". The pharmaceutical company is hoping that the "Drug" group will have lower cholesterol than the "Control" group. The aim here is to do a randomisation test to check that.

## Setting up
First you need to load the `tidyverse` libraries required (we'll be making use of `dplyr`, `magrittr` and `ggplot` which are all included in the `tidyverse` set of packages.

```{r,message=FALSE}
library(tidyverse)
```

Next, import the data, called `cholesterol`. Note that your path will be different from mine, and that you should set your working directory accordingly.

```{r data real,echo=FALSE}
ch <-read.csv("CourseData/cholesterol.csv")
```

Let's first take a look at the data by plotting it. I will first plot a boxplot first, and add the jittered points for clarity.

```{r}
ggplot(ch,aes(x=Treatment,y=Cholesterol)) + 
      geom_boxplot()+
    geom_jitter(colour="grey",width=.1)
```

It looks like there might be a difference between the groups.
Now let's consider our test statistic and our hypotheses. Our test statistic is the difference in mean cholesterol levels between the two groups: mean of control group minus the mean of the drug group.
The *null hypothesis* is that there is no difference between these two groups (i.e. the difference should be close to 0)
The *alternative hypothesis* is that the mean of the drug group should be less than the mean of the drug group. (i.e.  mean of control group minus the mean of the drug group should be negative).

## Calculate the observed difference

There are a few ways of doing this. In base-R you can use the function `tapply` ("table apply"), followed by `diff` ("difference").

```{r}
tapply(ch$Cholesterol,ch$Treatment,mean)
diff(tapply(ch$Cholesterol,ch$Treatment,mean))
```

Because we are focussing on learning `dplyr`, you can also calculate the means like like this:

```{r}
ch %>% # ch is the cholesterol data
  group_by(Treatment) %>% # group the data by treatment
  summarise(mean = mean(Cholesterol)) # calculate means
```

Here the pipes (`%>%`) are passing the result of each function on as imput to the next. 
You can use further commands, `pull` to get the `mean` vector from the summary table, and then use `diff` to calculate the difference between the groups, before passing that to a value called "`observedDiff`". 

```{r}
observedDiff <- ch %>%
  group_by(Treatment) %>% # group the data by treatment
  summarise(mean = mean(Cholesterol)) %>% # calculate means
  pull(mean) %>% # extract the mean vector
  diff()
```

This is a complicated set of commands. To make sure that you understand it, try running it bit-by-bit to see what is going on.

## Null distribution

Now we ask, what would the world look like if our null hypothesis was *true*. To do this we can dissassociate the treatment group variable from the measured cholesterol values. We do this using by using the `mutate` function to replace the `Treatment` variable with a shuffled version of itself with the `sample` function. 

Let's try that one time:
```{r set seed,echo=FALSE}
set.seed(666)
```

```{r,warning=FALSE}
ch %>%
  mutate(Treatment = sample(Treatment)) %>% #shuffle the Treatment data
  group_by(Treatment) %>%
  summarise(mean = mean(Cholesterol)) %>%
  pull(mean) %>%
  diff()
```

In this instance, the difference with the shuffled `Treatment` values is 0.833, which is rather different from our observed difference of `r observedDiff`.

Doing this one time is not much help though - we need to repeat this many times. I suggest that you do it 1000 times here, but some statisticians would suggest 5000 or even 10000 replicates. 

We can do this easily in R using the function `replicate` which simply a kind of wrapper that tells R to repeat a command `n` times and then pass the result to a vector.
Let's try it first 10 times to see how it works:

```{r, warning=FALSE}
replicate(10,
            ch %>%
            mutate(Treatment = sample(Treatment)) %>%
            group_by(Treatment) %>%
            summarise(mean = mean(Cholesterol)) %>%
            pull(mean) %>%
            diff()
)
```

You can see that the `replicate` command simply does the sampling-recalculation of the mean 10 times.

In the commands below I create 1000 replicates of the shuffled differences. I want to put them in a dataframe to make it easy to plot. Therefore, I first create a `data.frame` called `shuffledData`. This data frame initially has a variable called `rep` which consists of the numbers 1-1000. I then use `mutate` to add the 1000 shuffled differences. 


```{r replicate, warning=FALSE}
shuffledData <- data.frame(rep = 1:1000) %>%
  mutate(shuffledDiffs = replicate(1000,
            ch %>%
            mutate(Treatment = sample(Treatment)) %>%
            group_by(Treatment) %>%
            summarise(mean = mean(Cholesterol)) %>%
            pull(mean) %>%
            diff()
            ))
```

## Testing significance

Before formally testing the hypothesis it is useful to visualise what we have created in a histogram. I can use `ggplot` to do this, to create a plot called `p1`. Note that by putting the command in brackets R will both create the plot object, and print it to the screen. Note that because the shuffling of the data is random process your graph will look slightly different to mine.

```{r,message = FALSE}
(p1<-ggplot(shuffledData,aes(x=shuffledDiffs)) + 
  geom_histogram() + 
  theme_minimal()+
  xlab("Drug mean - Control mean"))
```

You can now add your observed difference (calculated above) to this plot like this:

```{r,message=FALSE}
p1 + geom_vline(xintercept = observedDiff)
```

## Testing the hypothesis

Recall that the alternative hypothesis is that the observed difference (control mean-drug mean) will be less than 0. 
You can see that there are few of the null distribution sample that are as extreme as the observed difference. To calculate a p-value we can simply count these values and express them as a proportion. Note that because the shuffling of the data is random process your result will probably be slightly different to mine.

```{r}
table(shuffledData$shuffledDiffs<=observedDiff)
```
So that is `r sum(shuffledData$shuffledDiffs<=observedDiff)` of the shuffled values that are equal to or less than the observed difference. The p-value is then simply `r sum(shuffledData$shuffledDiffs<=observedDiff)`/1000 = `r sum(shuffledData$shuffledDiffs<=observedDiff)/1000`. 

Therefore we can say that the drug appears to be effective at reducing cholesterol.

## Writing it up

We can report our findings something like this:

"To test whether effect of the drug at reducing cholesterol level is statistically significant I did a 1000 replicate randomisation test with the null hypothesis being that there is no difference between the group means and the alternative hypothesis that the mean for the drug treatment is lower than the control treatment. I compared the observed difference to this null distribution to calculate a p-value in a one-sided test.

The observed mean values of the control and treatment groups 205.667 and 185.500 respectively and the difference between them is therefore -20.167 (drug mean - control mean). Only 25 of the 1000 null distribution replicates were as low or lower than my observed difference value. I conclude that the observed difference between the means of the two treatment groups is statistically significant (p = 0.025)"


## Paired Randomisation Tests

The paired randomisation test is simply a one-sample randomisation test where the distribution is tested against a value of 0 (no difference between the two). I will illustrate this with an example from Everitt (1994) who looked at using cognitive behaviour therapy as a treatment for anorexia. He collected data on weights of people before and after therapy. These data are in the file `anorexiaCBT.csv`

```{r, message=FALSE}
library(tidyverse)
```

```{r}
#Remember to set your working directory first
an <- read.csv("CourseData/anorexiaCBT.csv")
head(an)
```

These data are arrange in a so-called "wide" format. To make plotting and analysis data need to be rearranged into a tidy "long" format so that each observation is on a row. We can do this using the gather function:

```{r}
an <- an %>%
  gather("time","weight",-Subject)

head(an)
```

### Plot the data

We should ALWAYS plot the data. So here goes.

```{r,message=FALSE}
(p1<-ggplot(an,aes(x=weight,fill=time)) +
   geom_histogram(position = "identity",alpha=.7)
 )
```

Another useful way to plot this data is to use an "interaction plot". In these plots the matched pairs (grouped by `Subject`) are joined together with lines. You can plot one like this:

```{r,message=FALSE}
(p2<-ggplot(an,aes(x=time,y=weight,group=Subject)) +
   geom_point() + 
   geom_line(size=1, alpha=0.5)
 )
#ggsave("/Users/jones/Dropbox/_SDU_Teaching/BB852 New Stats Course/PlotsintPlot.png",p2,width=6, height=8, dpi = 300, units="cm")
```

```{r echo=FALSE}

an2 <- an %>% group_by(Subject) %>% mutate(time = sample(time))
p2.shuffled <-ggplot(an2,aes(x=time,y=weight,group=Subject)) +
   geom_point() + 
   geom_line(size=1, alpha=0.5)
 
#ggsave("/Users/jones/Dropbox/_SDU_Teaching/BB852 New Stats Course/Plots/intPlot.shuffled.png",p2.shuffled,width=6, height=8, dpi = 300, units="cm")

```


What we are interested in is whether there has been a change in weight of the subjects after CBT. The null hypothesis is that there is zero change in weight. The alternative hypothesis is that weight has increased.

The starting point for the analysis is to calculate the observed change in weight.

```{r, warning=FALSE}
an <- an %>% 
  group_by(Subject) %>% 
  summarise(change = diff(weight))
```
You have created a dataset that looks like this:

```{r}
head(an)
```

And you can calculate the observed change like this:

```{r}
obsChange <- mean(an$change)
obsChange
```




### The paired randomisation test

The logic of this test is that if the experimental treatment has no effect on weight, then the Before weight is just as likely to be larger than the After weight as it is to be smaller. 
In other words, if the null hypothesis is true, a permutation within any pair of scores is as likely as the reverse. 

Therefore to carry out this test, we can permute the SIGN of the change in weight (i.e. we randomly flip values from positive to negative and vice versa). We can do this by multiplying by 1 or -1, randomly.

```{r}
head(an)

anShuffled <- an %>%
  mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %>%
  mutate(shuffledChange = change * sign)
```

Let's take a look at this new shuffled dataset:

```{r}
head(anShuffled)
```

We need to calculate the mean of this shuffled vector. We can do this by `pull` to get the vector, and then `mean`.

```{r}
an %>%
  mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %>%
  mutate(shuffledChange = change * sign) %>%
  pull(shuffledChange) %>%
  mean()
```

Now we will build a null distribution of changes in weight by repeating this 1000 times. We can do this using the `replicate` function to "wrap" around the function, passing the result into a data frame. We can then compare this null distribution to the observed change.

```{r}
nullDist = data.frame(change =
                        replicate(1000,an %>%
                                    mutate(sign = sample(c(1,-1),size = nrow(an),replace = TRUE)) %>%
                                    mutate(shuffledChange = change * sign) %>%
                                    pull(shuffledChange) %>%
                                    mean())) 
```



### Plot the null distribution

```{r,message=FALSE}
(nullDistPlot <- ggplot(nullDist,aes(x=change)) +
  geom_histogram())
```

We can add the observed change as a line to this:

```{r,message=FALSE}
nullDistPlot + geom_vline(xintercept = obsChange)
```

### The formal hypothesis test

The formal test of significance then is to ask how many of the null distribution replicates are as extreme as the observed change. 

```{r}
table(nullDist$change>=obsChange)
```
So we can see that `r table(nullDist$change>=obsChange)[2]` of `r sum(table(nullDist$change>=obsChange))` replicates were greater than or equal to the observed change. This translates to a p-value of `r table(nullDist$change>=obsChange)[2]/sum(table(nullDist$change>=obsChange))`. We can therefore say that the observed change in weight after CBT was signficantly greater than what we would expect from chance.



## Exercises - Randomisation tests

### Introduction

A hercules beetle is a large rainforest species from South America. Researchers suspect that sexual selection has been operating on the species so that the males are significantly larger than the females. You are given data[^1] on width measurements in cm of a small sample of 20 individuals of each sex. Can you use your skills to report whether males are signficantly larger than females. 

The data are called `herculesBeetle.csv` and can be found via the Dropbox link on Blackboard. 

[^1]: This example from: https://uoftcoders.github.io/rcourse/lec09-Randomization-tests.html

### Your task

Follow the following prompts to get to your answer:

1. What is your null hypothesis? 

2. What is your alternative hypothesis?

3. Import the data.

4. Calculate the mean for each sex (either using `tapply` or using `dplyr` tools)

5. Plot the data as a histogram.

6. Add vertical lines to the plot to indicate the mean values.

7. Now calculate the difference between the mean values using `dplyr` tools, or `tapply`.

8. Use `sample` to randomise the sex column of the data, and recalculate the difference between the mean.

9. Use `replicate` to repeat this 10 times (to ensure that you code works).

10. When your code is working, use `replicate` again, but this time with 1000 replicates and pass the results into a data frame.

11. Use `ggplot` to plot the null distribution you have just created, and add the observed difference.

12. Obtain the p-value for the hypothesis test described above. (1) how many of the observed differences are greater than or equal to the shuffled differences in the null distribution. (2) what is this expressed as a proportion of the number of replicates.

13. Summarise your result as in a report. Describe the method, followed by the result and conclusion.








<!--chapter:end:07-randomisationTests.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Comparing two means with a t-test

We will cover the following:

* One-sample t-test
* Paired t-test
* Two-sample t-test ("Welch t-test")

## Some theory.

In this theory section I focus on the one-sample t-test, but the concepts apply to the other types of t-test. 

The one-sample t-test is used to compare the mean of a sample to some fixed value. For example, we might want to compare pollution levels (e.g. in mg/m^3^) in a sample to some acceptable threshold value to help us decide whether we need to take action to prevent or clean up pollution.

One of the assumptions of t-tests (and many other tests/models) is that the distribution of values in the **sample** of data can be described by a normal distribution. If this assumption is true, you can use these data to estimate the parameters of this sample's normal distribution: the mean and standard error of the mean. 

The mean gives an estimate of location, and the standard error of the mean (which is calculated as $s/ \sqrt{n}$, where *s* = standard deviation and *n* = sample size) gives an estimate of precision of this estimate (i.e. how certain is it that the mean value is really where you think it is?)

The t-test then works by comparing your estimated distribution with some fixed value. Sometimes you are asking "is my mean *different* from the value?", other times you are asking "is my mean less than/greater than the value?". This depends on the hypothesis. The default that R-uses is that it tests whether the mean of your distribution is *different* to the fixed value, but in many cases you should really be framing a **directional** hypothesis.

It is helpful to visualise this, so some examples of the pollution threshold test are shown in the figure below (Figure \ref{fig:one_sample_vis}). The curves illustrate the estimated normal distributions that describe our estimate of the mean pollution level from some data (e.g. each curve might represent samples from different locations). We are interested in whether the mean values (the vertical dashed lines) are significantly **greater** than the threshold of 100mg/m^2^ (solid vertical black line) (this gives us a directional hypothesis).

Formally we do this by establishing two hypotheses a **null hypothesis** and an **alternative hypothesis**. In this case, the null hypothesis is that the mean of the sample measurements is **not** significantly different from the threshold value we define. The alternative hypothesis is that the sample mean is significantly **greater** than this threshold value.

The degree of confidence that we can have that the mean pollution values are different from the threshold value depend on (A) the position of the distribution relative to the threshold value and (B) on the spread of the distribution (the standard deviation/error).

Based on Figure \ref{fig:one_sample_vis}, which of these four different samples shows a mean value **significantly** greater than 100? (you should be looking at the amount of the normal distribution curve that is overlapping the threshold value.)

```{r one_sample_vis, echo=FALSE,fig.width=6,fig.height=2,fig.align='center',fig.cap=""}

d1<-c(150,40)
d2<-c(200,40)

A <- ggplot(data = data.frame(x = c(0, 350)), aes(x)) +
    stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
    stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
    ylab("Probability density") + 
    xlab(expression(Pollution~level~mg/m^3)) +
    scale_y_continuous(breaks = NULL) + 
    geom_vline(xintercept = 100, linetype="solid") +
    geom_segment(data = data.frame(y=.002,yend=.002,x=100,xend=d1[1]),
                 aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="#007F00") +
    geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
                aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
    geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
    geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
    #theme_minimal()+
    NULL

d1<-c(170,40)
d2<-c(170,80)

B <- ggplot(data = data.frame(x = c(0, 350)), aes(x)) +
    stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
    stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
    ylab("Probability density") + 
    xlab(expression(Pollution~level~mg/m^3)) +
    scale_y_continuous(breaks = NULL) + 
    geom_vline(xintercept = 100, linetype="solid") +
    geom_segment(data = data.frame(y=.002,yend=.002,x=100,xend=d1[1]),
                 aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="#007F00") +
    geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
                aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
    geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
    geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
    #theme_minimal()+
    NULL


ggpubr::ggarrange(A+ggtitle("A"),B+ggtitle("B"))
```

This should look familiar -- it is the same concept as we used in the class on randomisation tests. If you find it confusing, please go back and review the randomisation test materials!

Another useful way to think about t-tests is that it is a way of distinguishing between **signal** and **noise**: the signal is the mean value of the thing you are measuring, and the noise is the uncertainty in that estimate. This uncertainty could be due to measurement error and/or natural variation. In fact, the *t-value* that the t-test relies on is a ratio between the signal (difference between mean  ($\bar{x}$) and threshold ($\mu_{0}$)) and noise (variability, standard error of the mean ($s/ \sqrt{n}$)): $$t = \frac{\bar{x}-\mu_{0}} {s/ \sqrt{n}}$$

The larger the signal is compared to the noise, the higher the t-value will be. e.g. a t-value of 2 means that the signal was 2 times the variability in the data. A t-value of zero, or close to zero, means that the signal is "drowned out" by the noise. Therefore, high t-values give you more confidence that the difference is true.

To know if the t-value means that the difference is significant, the t-value is compared to a known theoretical distribution (the t-distribution). The area under the curve of the distribution is 1, but its shape depends on the degrees of freedom (i.e. sample size - 1). The plot below (Figure \ref{fig:t_dist}) shows three t-distributions of different degrees of freedom (d.f.). 

What R is doing when it figures out the p-value is calculating the area under the curve *beyond* the positive/negative values of the t-statistic. If t is small, then this value is large (p-value). If t is large then the area (and the p-value) is small. In the olden-days (>15 years ago) you would have looked these values up in printed tables, but now R does that for us.


```{r t_dist, echo=FALSE,fig.width=3,fig.height=2,fig.align='center',fig.cap=""}

(ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 5), aes(colour="05")) +
  stat_function(fun = dt, args = list(df = 10),aes(colour="10")) +
  stat_function(fun = dt, args = list(df = 30),aes(colour="30")) +
  geom_vline(xintercept = c(-2,2), linetype="dashed") +
  scale_colour_manual("d.f.", values = c("red", "blue", "green")) + 
  xlab("t-value") + 
  ylab("Probability density")
)  
```



## One sample t-test

Enough theory. Here's how you would apply such a test in R.

Firstly, lets load some data. Because this is a very small example, you can simply cut and paste the data in rather than loading it from a CSV file.

```{r}
pollution <- data.frame(mgm3 = c(105, 196, 226, 81, 156, 201, 142, 149, 191, 192,
                                178, 185, 231, 76, 207, 138, 146, 175, 114, 155))

```

First I plot the data (Figure \ref{fig:pollution}). One reason for doing this is to check that the data look approximately normally distributed. These data are slightly left-skewed but they are close enough.

```{r pollution, fig.width=4,fig.height=3,fig.align='center',fig.cap="Histogram of the pollution data."}
ggplot(pollution,aes(x=mgm3))+
  geom_histogram(bins=8) +
  geom_vline(xintercept = 100)
```

Now we can run a t-test in R like this. The command is simple - the first two arguments are the data (`x`) and the the fixed value you are comparing the data to. The final argument defines the alternative hypothesis. This can take values of "`two.sided`", "`less`" or "`greater`" (the default is `two.sided`). In this example, the alternative hypothesis is that the mean of our sample is greater than the threshold of 100.

```{r}
t.test(x = pollution$mgm3, mu = 100, alternative = "greater")
```

The output of the model tells us (1) what type of t-test is being fitted ("`One Sample t-test`"). Then it gives some values for the t-statistic, the degrees of freedom and the p-value. The model output also tells us that the alternative hypothesis "**true mean is greater than 100**". Because the p-value is very small (p<0.05) we can reject the null hypothesis and accept the alternative hypothesis.  Finally, the output gives you the confidence interval (the area where we strongly believe the true mean to lie) and the estimate of the mean. 

We could report these results like this: "*the mean value of the sample was 162.2 mg/m^3^, which is significantly greater than the acceptable threshold of 100 mg/m^3^ (t-test: t = 6.2824, df = 19, p-value = 2.478e-06)*. 


## Doing it "by hand" - where does the t-statistic come from?

At this point, to ensure that you understand where the t-statistic comes from we will calculate the t-statistic using the equation from above. The purpose of this is to illustrate that this is not brain surgery - it all hinges on a straightforward comparison between signal (the difference between mean and threshold in this case) and noise (the variation, or standard error of the mean).

To do this we first need to know the mean value and the threshold (the signal: $\bar{x} - \mu_{0}$). We can then divide that by the standard error of the mean (the noise: $s/ \sqrt{n}$)

Here goes... I first create a vector (`x`) of the values to save typing. Then I show how to calculate `mean` and standard error, before dividing the "signal" by the "noise".


```{r}
#First create a vector of the values
x <- pollution$mgm3

#mean
mean(x)

#standard error of the mean
sd(x)/sqrt(length(x))

#Putting it all together
(mean(x) - 100) / (sd(x) / sqrt(length(x)))
```

This matches exactly with the t-statistic above!

One can obtain a p-value from a given t-statistic and degrees of freedom like this for a t-test like the one fitted above (the d.f. is the sample size minus one for a one-sample t-test):

```{r}
1 - pt(6.282373, 19)
```
Again, this matches the value from the `t.test` function above.


## Paired t-test 

It's actually quite hard to find examples of one-sample t-tests in biology. In most cases, the one-sample t-tests are really paired t-tests, which are a special case of the one sample test where rather than using the actual values measured, we use the difference between them instead (Figure \ref{fig:paired_t}). 

```{r paired_t,echo=FALSE,fig.width=3,fig.height=2,fig.align='center',fig.cap=""}
(ggplot(data = data.frame(x = c(-10, 90)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 40, sd = 20)) + ylab("Probability density") + 
   xlab("Difference between pairs") +
  scale_y_continuous(breaks = NULL) + 
  #geom_vline(xintercept = 0, linetype="dashed") +
  geom_vline(xintercept = 40, linetype="dashed",col="red") +
   geom_segment(data = data.frame(y=.002,yend=.002,x=0,xend=40),
                aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")))+
   geom_vline(xintercept = 0, linetype="dashed",colour="black") +
  #theme_minimal()+
   NULL
)
```

Here's a simple example. Anthropologists studied tool use in women from the indigenous Machinguenga of Peru. They estimated the amount of cassava obtained in kg/hour using either a wooden tool (a broken bow) or a metal machete. The study focused on 5 women who were randomly assigned to groups to use the wooden tool then the machete (or vice versa). 

<!--- [^ A.M. Hurtado, K. Hill (1989). "Experimental Studies of Tool Efficiency Among Machinguenga Women and Implications for Root-Digging Foragers", Journal of Anthropological Research, Vol.45,2,pp207-217.] --->

The anthropologists hypothesised that using different tools led to different harvesting efficiencies. The null hypothesis is that there is no difference between the two groups and that a woman was equally efficient at foraging using either tool. The alternative hypothesis was that there is a difference between the two tools. (NOTE - this could also be formulated as a directional hypothesis e.g. with the expectation that machete is more efficient than the bow.)

First let's import and look at the data. Make sure you understand it. A plot will be fairly useless to tell if the data are normally distributed, so we will simply have to assume that they are. In fact, t-tests are famously robust to non-normality.

```{r}
toolUse <- read.csv("CourseData/toolUse.csv")
toolUse
```

We should now plot our data (Figure \ref{fig:toolData}). A nice way of doing this for paired data is to plot points with lines joining the pairs. This way, the slope of the lines is a striking visual indication of the effect.

```{r toolData,fig.width=4.5,fig.height=3, fig.align='center',fig.cap="An interaction plot for the tool use data"}
ggplot(toolUse,aes(x=tool,y=amount,group=subjectID)) + 
  geom_line()+
  geom_point() +
  xlab("Tool used") +
  ylab("Casava harvested (kg/hr)")

```

We can also look at the mean values and standard deviations:

```{r}
toolUse %>%
  group_by(tool) %>%
  summarise(meanAmount = mean(amount),sdAmount = sd(amount))
```

Now lets do a paired t-test to compare the means using the two tools. There are several ways to do a paired t-test. The first is to use a model formula in the command. The formula takes the form `measurements ~ group`. You must also specify the name of the `data.frame` and that the data are paired (`paired = TRUE`). 

**IMPORTANT:** it is very important that the pairs are grouped together in the data frame so that the pairs match up when you filter the data to each group. It is therefore advisable to use `arrange` to sort the data by first the pairing variable (in this case, `subjectID`), and then the explanatory variable (the variable that defines the group - in this case, `tool`.

```{r}
toolUse <- toolUse %>% 
  arrange(subjectID, tool)

t.test(amount ~ tool, data = toolUse, paired = TRUE)
```

An alternative way is to give the two samples separately in the `t.test` command. To do this you will need to create two vectors containing the data from the two groups like this, using the `dplyr` command `pull` to extract the variable along with `filter` to subset the data:

```{r}
A <- toolUse %>% 
  filter(tool == "machete") %>% 
  pull(amount)

B <- toolUse %>% 
  filter(tool == "bow") %>% 
  pull(amount)

t.test(A, B, data = toolUse, paired = TRUE)
```

You would report these results something like this: *"Women harvested cassava more efficiently with a machete (168.2 kg/hr) than with a wooden tool (82.8kg/hr). The difference of 85.4 kg/hr (95% CI 72.2-98.6 kg) was statistically significant (paired t-test: t = 17.98, df = 4, p-value = 5.625e-05)."*

NOTE: you could add the argument `alternative = "less"` or `greater` to these t-tests to turn them into directional one-tailed hypotheses. However, you should also be aware that the p-value for a one-tailed t-test is always half that of the two-tailed test. Therefore, you could also simply half the p-value when you report it rather than adding the "alternative" argument.

## A paired t-test is a one-sample test.

A paired t-test is the same as a one-sample t-test really. Here's proof.

First we need to calculate the difference between the two measures

```{r}
difference <- A - B
```

Then we can fit the one-sample t-test from above, with the `mu` set as 0 (because the null hypothesis is that there is no difference between the groups). Compare this result with the paired t-test above.

```{r}
t.test(x = difference, mu = 0)
```


## Two sample t-test

The two sample t-test is used for comparing the means of two samples [no shit!? :)]

You can visualise this by picturing your two distributions (Figure \ref{fig:twosample}) and thinking about their overlap. If they overlap a lot the difference between means will not be significant. If they don't overlap very much then the difference between means *will* be significant. 

The underlying mathematical machinery for the two-sample t-test is similar to the one-sample and paired t-tests. 
Again, the important value is the t-statistic, which can be thought of as a measure of signal:noise ratio (*see above*). It is harder to detect a signal (the true difference between means) if there is a lot of noise (the variability, or spread of the distributions), or if the signal is small (the difference between means is small).


```{r twosample, echo=FALSE, fig.width=6,fig.height=2,fig.align='center',fig.cap="A two-sample t-test.", fig.pos="ht"}

d1<-c(40,10)
d2<-c(60,10)

A <- ggplot(data = data.frame(x = c(0, 120)), aes(x)) +
    stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
    stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
    ylab("Probability density") + 
    xlab("x") +
    scale_y_continuous(breaks = NULL) + 
    geom_segment(data = data.frame(y=.002,yend=.002,x=d1[1],xend=d2[1]),
                 aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="black") +
    #geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
    #            aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
    geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
    geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
    #theme_minimal()+
    NULL

d1<-c(40,10)
d2<-c(80,10)

B <- ggplot(data = data.frame(x = c(0, 120)), aes(x)) +
    stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
    stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
    ylab("Probability density") + 
    xlab("x") +
    scale_y_continuous(breaks = NULL) + 
    geom_segment(data = data.frame(y=.002,yend=.002,x=d1[1],xend=d2[1]),
                 aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="black") +
    #geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
    #            aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
    geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
    geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
    #theme_minimal()+
    NULL

ggpubr::ggarrange(A+ggtitle("A"),B+ggtitle("B"))
```

The mathematics involved with calculating the t-statistic is very similar to the one-sample t-test, except the numerator in the fraction is the difference between two means rather than between a mean and a fixed value.

$$t = \frac{\bar{x_1}-\bar{x_2}} {s/ \sqrt{n}}$$
So far so good... let's push on and use R to do some statistics.

In this example, we can revisit the class data and ask the question, *Is the reaction time of males different than that of females?* The null hypothesis for this question is that there is no difference in mean reaction times between the two groups. The alternative hypothesis is that there **is** a difference in the mean reaction time between the two groups.

Import the data in the usual way, and subset it to the right year.

```{r}
x <- read.csv("CourseData/classData.csv")
```

Then look at the data. Here I do this using a box plot with jittered points (a nice way of plotting data with small sample sizes) (Figure \ref{fig:reactionData}). From the Figure \ref{fig:reactionData} it looks like males have a faster reaction time than females, but there is a lot of variation. We need to apply the t-test in a similar way to above.

```{r reactionData, fig.width=4,fig.height=3,fig.align='center',fig.cap="", fig.pos="ht"}
ggplot(x,aes(x=Gender,y=Reaction)) +
  geom_boxplot() + 
  geom_jitter(col="grey60",width=0.2)
```



```{r echo=FALSE}
temp <- t.test(Reaction ~ Gender, data = x)
```

```{r}
t.test(Reaction ~ Gender, data = x,var.equal=F)
```

This output first tells us that we are using something called a "Welch Two Sample t-test". This is a form of the two-sample t-test that relaxes the assumption that the variance in the two groups is the same. This is a good thing. Although it *is* possible to fit a t-test with equal variances, I recommend that you stick with the default Welch's test and **not** make this limiting assumption. 

Then we are told the t-statistic (`r formatC(temp$statistic, digits = 3, format = "f")`), the degrees of freedom (`r formatC(temp$parameter, digits = 3, format = "f")`) and the p-value (`r formatC(temp$p.value, digits = 3, format = "f")`). We must therefore accept the null hypothesis: there is no significant difference between the two groups. Males are *not* faster than females. We could write report this something like this: 

"*Although females had a slightly slower reaction time than males (`r formatC(temp$estimate[1], digits = 3, format = "f")` seconds compared to `r formatC(temp$estimate[2], digits = 3, format = "f")` seconds), this difference was not statistically significant (Welch t-test: t= `r formatC(temp$statistic, digits = 3, format = "f")`, d.f.= `r formatC(temp$parameter, digits = 3, format = "f")`), p=`r formatC(temp$p.value, digits = 3, format = "f")`).*"

Note: With a t-test that *did* assume equal variances in the two groups, the d.f. is calculated as the sample size - 2 (the number of groups). You can do this by adding the argument "`var.equal = TRUE`" to the t-test command. With the Welch test, the appropriate degrees of freedom are estimated by looking at the sample sizes and variances in the two groups. The details of this are beyond the scope of this course.


## t-tests are linear models

It is also possible to formulate t-tests as linear models (using the `lm` function). To do this with the paired t-test you would specify a model that estimates an intercept. In R you can do this by writing the formula as `x ~ 1`. So, for the tool use example you can write the code like this: 

```{r}
mod1 <- lm(difference ~ 1)
summary(mod1)
```

If you look at the summary will notice that the estimate of the intercept (the average difference between the two pairs), the degrees of freedom and the t-value and the p-value are all the same as the value reported when using `t.test`.

In fact, all of the t-tests, and ANOVA (below) are kinds linear models and can be also fitted with `lm`.

Here is the paired t-test investigating gender differences in reaction time. You can see that the test statistics and coefficients match those obtained from `t.test`.

```{r}
mod <- lm(Reaction ~ Gender, data = x)
anova(mod)
summary(mod)
```


## Exercises - Two-sample t-test

Some people have suggested that there might be sex differences in fine motor skills in humans. Use the data collected on the class to address this topic using t-tests. The relevant data set is called `classData.csv`, and the columns of interest are `Gender` and `Precision`.

Carry out a two-sample t-test. 

1) Plot the data (e.g. with a box plot, or histogram)

2) Formulate null and alternative hypotheses. 

3) Use the `t.test` function to do the test.

4) Write a sentence or two describing the results.



### Paired t-test

A study was carried out looking at the effect of cognitive behavioural therapy on weight of people with anorexia. Weight was measured in week 1 and again in week 8. Use a paired t-test to assess whether the treatment is effective.

The data is called `anorexiaCBT.csv`

The data are in "wide format". You may wish to convert it to "long format" depending on how you use the data. You can do that with the `gather` function, which rearranges the data:

```{r,echo=FALSE,message=FALSE}
library(tidyverse)
anorexiaCBT <- read.csv("CourseData/anorexiaCBT.csv",header=TRUE)
```

```{r}
anorexiaCBT_long <- anorexiaCBT %>% 
  gather("week","weight",-Subject)
```
1) Plot the data (e.g. with an interaction plot like Figure 5 in the t-tests PDF)

2) Formulate a null and alternative hypothesis.

3) Use `t.test` to conduct a *paired* t-test.

4) Write a couple of sentences to report your result.


### Optional extra.

* Try re-fitting some of these tests as randomisation tests (or analyse the randomisation test data using `t.test`). Do they give the same results?

* Try answering the question - "*are people who prefer dogs taller than those who prefer cats?* " using the `courseData.csv`. Can you think of any problems with this analysis?

<!--MOVE TO RANDOMISATION TESTS. Try answering the question - "*do people who prefer cats travel less?* " using the `courseData.csv`-->


<!--chapter:end:08-t-tests.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Linear models with a single categorical explanatory variable

## Introduction

With the previous work on t-tests (and also with randomisation tests), you are now equipped to test for differences between two groups (or between one group and some fixed value). But what if there are more than two groups? The answer is to use a one-way analysis of variance (ANOVA). Conceptually, this works the same way as a t-test.


## One-way ANOVA 

The one-way ANOVA is illustrated below with two cases (Figure \ref{fig:anova_vis}). In both cases there are three groups. These could represent treatment groups in an experiment (e.g. different fertiliser addition to plants). In figure A, the three groups are very close, and the means are not significantly different from each other. In figure B, there is one group that stands apart from the others. The ANOVA will tell us whether *at least one* of the groups is different from the others.

```{r, echo=FALSE,message=FALSE}
library(tidyverse)
library(ggpubr)
```

```{r anova_vis, echo=FALSE,fig.width=7,fig.height=3,fig.align='left',fig.cap="", fig.pos = "ht"}

d1<-c(40,10)
d2<-c(45,10)
d3<-c(50,10)

A <- ggplot(data = data.frame(x = c(0, 120)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d3[1], sd = d3[2]),colour="blue") + 
  ylab("Probability density") + 
  xlab("y") +
  scale_y_continuous(breaks = NULL) + 
  #geom_segment(data = data.frame(y=.002,yend=.002,x=d1[1],xend=d2[1]),
  #             aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="black") +
  #geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
  #            aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
  geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
  geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
  geom_vline(xintercept = d3[1], linetype="dashed",colour="blue") +
  #theme_minimal()+
  NULL

d1<-c(40,10)
d2<-c(45,10)
d3<-c(80,10)

B <- ggplot(data = data.frame(x = c(0, 120)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d3[1], sd = d3[2]),colour="blue") + 
  ylab("Probability density") + 
  xlab("y") +
  scale_y_continuous(breaks = NULL) + 
  #geom_segment(data = data.frame(y=.002,yend=.002,x=d1[1],xend=d2[1]),
  #             aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="black") +
  #geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
  #            aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
  geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
  geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
  geom_vline(xintercept = d3[1], linetype="dashed",colour="blue") +
  #theme_minimal()+
  NULL

ggpubr::ggarrange(A+ggtitle("A"),B+ggtitle("B"))
```



```{r, echo = FALSE,fig.show="hide"}
#Makes boxplot version of the above for presentation
set.seed(124)
dfA <- data.frame(x = rep(LETTERS[1:3],each = 30),
                  y = c(rnorm(30,40,10),rnorm(30,45,10),rnorm(30,50,10)))

A<-ggplot(dfA, aes(x = x,y =y)) +
  geom_boxplot() + 
  ggtitle("A") + ylim(20,100)

dfB <- data.frame(x = rep(LETTERS[1:3],each = 30),
                  y = c(rnorm(30,40,10),rnorm(30,45,10),rnorm(30,80,10)))

B<-ggplot(dfB, aes(x = x,y =y)) +
  geom_boxplot() + 
  ggtitle("B")+ ylim(20,100)

ggpubr::ggarrange(A,B)
#ggsave("anovaExampleBoxplot.png",width = 6,height=2,units = "in",dpi=300)
```

After figuring out if at least one of the groups is significantly different from the others it is often enough to examine plots (or summary statistics) to see where the differences are (e.g. which group(s) are different from each other). In other cases it might be necessary to do follow up *post-hoc multiple comparison* tests. We will come to those later.


## Fitting an ANOVA in R

New coffee machines use "pods" to make espresso. These have become much more popular than the traditional "bar machines". This data looks at the amount of "crema" or foam produced (a sign of quality!) using three methods: bar machines (BM), Hyper Espresso Pods (HIP) and Illy Espresso System (IES). Are any of these methods better than the others?

Import the data and look at it.

```{r}
espresso <- read.csv("CourseData/espresso.csv", stringsAsFactors = TRUE)
head(espresso)
```

```{r coffee,echo=TRUE,fig.width=4,fig.height=3,fig.align='center', fig.cap="A box and whisker plot, with jittered points, for the espresso foam data."}
(ggplot(espresso,aes(x=method,y=foamIndx)) + 
geom_boxplot()+
geom_jitter(width=0.2))
```

You can see that the categorical explanatory variable ("method") defines the three treatment groups and has the three levels representing the different coffee types: BM, HIP and IES.

Let's first fit the ANOVA using R. One way ANOVAs are fitted using the `lm` function (`lm` stands for "linear model" - yes, an ANOVA is a type of linear model).

```{r}
foam_mod <- lm(foamIndx ~ method, data = espresso)
```

Before proceeding, we need to check the assumptions of the model. This can be done visually using the `autoplot` function in the `ggfortify` package. If you don't have the package installed, install it now (`install.packages("ggfortify"`)). 

```{r diagnostic,echo=TRUE,fig.width=5,fig.height=4,fig.align='center', fig.cap="Diagnostic plots for the ANOVA model. This looks great."}
library(ggfortify)
autoplot(foam_mod)
```

The main thing to look at here in Figure \ref{fig:diagnostic} is the "Q-Q" plot on the top right. We want those points to be approximately along the line. If that is the case, then it tells us that the model's residuals are normally distributed (this is one of the assumptions of ANOVA). We may cover these diagnostic plots more thoroughly later. You can find more details on pages 112-113 of the Beckerman et al textbook, or at the following if you are interested: https://data.library.virginia.edu/diagnostic-plots/.

Trust me, everything here looks great.

Now let's evaluate our ANOVA model. We do this using two functions: `anova` and `summary` (it sounds strange, but yes we do use a function called `anova` on our ANOVA model).

First, the `anova`. This gives us the following summary:

```{r}
anova(foam_mod)
```

```{r echo=FALSE}
x<-anova(foam_mod)
```

This gives some numbers (degrees of freedom, sum of squares, mean squares). These are the important values that go into calculating an *F value* (also called an F-statistic). We will not worry about these details now, except to say that large F-statistics mean that we are more certain that there is a difference between the groups (and that the p-value is smaller).

In this case, the F-value is  `r formatC(x$"F value"[1], digits = 3, format = "f")`. 


As with the t-test, R compares this value to a theoretical distribution (a "table"), based on *two* degrees of freedom. The first one is number of groups minus one, i.e. `r formatC(x$Df[1], digits = 3, format = "f")` in this case. The second one is the overall sample size, minus the number of groups, i.e.  `r formatC(x$Df[2], digits = 3, format = "f")`, in this case. 

This results in a p-value of `r formatC(x$"Pr(>F)"[1], digits = 10, format = "f")` (very highly significant!). 

Based on this p-value we can reject the null hypothesis that there is no difference between groups. We might report this simple result like this:

*The foam index varied significantly among groups (ANOVA: F = `r formatC(x$"F value"[1], digits = 3, format = "f")`, d.f. = `r x$Df[1]` and `r x$Df[2]`, p = `r formatC(x$"Pr(>F)"[1], digits = 3, format = "f")`).*

## Where are the differences?

This model output doesn't tell us *where* those differences are, nor does it tell us what the estimated mean values of foaminess are for the three groups: what are the effects?  We need to dig further into the model to get to these details.

There are several ways to do this and we'll look at one of them. 

We do this using the `summary` function.

```{r}
summary(foam_mod)
```

To properly interpret this output you need to understand something called "treatment contrasts". Essentially, contrasts define how model coefficients (the estimates made by the model) are presented in R outputs.

They are a bit hard to wrap your head aroudn and I STRONGLY recommend that you always do this with reference to a plot of the actual data, and the mean values for your groups. To do this you can use `group_by` and `summarise` to calculate the means for your the levels of your explanatory variable.

```{r}
espresso %>%
group_by(method) %>%
summarise(gp_mean = mean(foamIndx))
```


Look at the coefficients of the model. Remember that you have three levels in your explanatory variable, but only two levels are shown in the summary. Which one is missing? 

The "missing" group is the first one alphabetically (i.e. BM). The estimate (of the mean) for this group is labelled "`(Intercept)`" (with a value of `r foam_mod$coefficients[1]`. This is like a baseline or reference value, and the estimates for the other groups (HIP and IES), are *differences* between this baseline value and the estimated mean for those groups. In other words, the second group (HIP) is `r foam_mod$coefficients[2]` more than `r foam_mod$coefficients[1]` (`r foam_mod$coefficients[1]` + `r foam_mod$coefficients[2]` = `r sum(foam_mod$coefficients[1:2])`). Similarly, the third group (IES) is `r foam_mod$coefficients[3]` more than `r foam_mod$coefficients[1]` (`r foam_mod$coefficients[1]` + `r foam_mod$coefficients[3]` = `r sum(foam_mod$coefficients[c(1,3)])`). Compare these values with the ones you got above using `summarise` - they should be the same. 

This is illustrated below in Figure \ref{fig:diagnostic}A. You can see that the coefficients of the model are the same as the lengths of the arrows that run from 0 (for the first level of method (BM), the Intercept) or *from* this reference value. It is often a good idea to sketch something like this on paper when you are trying to understand your model outputs!

<!---
Source: A. Parenti, L. Guerrini, P. Masella, S. Spinelli, L. Calamai,
P. Spugnoli (2014). "Comparison of Espresso Coffee Brewing Techniques," 
Journal of Food Engineering, Vol. 121, pp. 112-117.
--->


Likewise, the t-values and p-values, are evaluating *differences between the focal group and this baseline*. Thus in this case, the comparisons (the "contrasts") are between the intercept (BM) and the second level (HIP), and the intercept (BM) and the third level (IES). There is no formal statistical comparison between HIP and IES.

You can see that it is very important to understand the levels of your explanatory variable, and how these relate to the summary outputs of the model. It can be useful to use the function `relevel` to manipulate the explanatory variable to make sure that the output gives you the comparisons you are interested in. Another simple trick would be to always ensure that your reference group (e.g. "control") comes first alphabetically and is therefore selected by R as the intercept (reference point).

For example, we can `relevel` the method variable so that the levels are re-ordered as HIP, BM, then IES so that the comparisons are between zero-HIP, HIP-BM and HIP-IES. (make sure that you understand this before proceeding).

```{r}
#This is what the original data looks like:
levels(espresso$method)

#releveling changes this by changing the reference.
espresso_2 <- espresso %>%
mutate(method = relevel(method,ref = "HIP"))
levels(espresso_2$method)
```

Now we can refit the model with this modified data set and see what difference that made:

```{r}
foam_mod2 <- lm(foamIndx ~ method, data = espresso_2)
anova(foam_mod2)
summary(foam_mod2)
```

Now the coefficients in the model summary look different, but the model is actually the same. Compare the two graphs in Figure \ref{fig:differentReferences} - can you see the differences/similarities?


```{r differentReferences, echo=FALSE,fig.width=8,fig.height=3,fig.align='center', fig.cap="Comparison illustrating the difference between ANOVA models using (A) BM and (B) HIP as references in the espresso foam data set."}


temp <- data.frame(level = names(foam_mod$coefficients),
levelNum = 1:3,
coef = foam_mod$coefficients) %>%
mutate(value = c(coef[1],coef[2:3]  +coef[1]))

A<- ggplot(espresso,aes(x=method,y=foamIndx)) + 
geom_jitter(width=0.2,colour = "grey45") + 
geom_point(data = temp, 
mapping = aes(x = levelNum,y= value),inherit.aes=F,colour="red") + 
geom_segment(data = temp,mapping = aes(x=1,y=0,xend=1,yend = value[1]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = temp$value[1],linetype="dashed",colour = "grey60")+
geom_segment(data = temp,mapping = aes(x=2,y=value[1],xend=2,yend = value[2]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_segment(data = temp,mapping = aes(x=3,y=value[1],xend=3,yend = value[3]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = 0) +
annotate("text", x = 1.3, y = temp$value[1]/2, label = temp$value[1])+
annotate("text", x = 2.3, y = sum(temp$value[1:2])/2, label = temp$coef[2])+
annotate("text", x = 3.3, y = sum(temp$value[c(1,3)])/2, label = formatC(temp$coef[3], digits = 1, format = "f"))+
ylim(0,75)  

temp <- data.frame(level = names(foam_mod2$coefficients),
levelNum = 1:3,
coef = foam_mod2$coefficients) %>%
mutate(value = c(coef[1],coef[2:3]  +coef[1]))

B<-ggplot(espresso_2,aes(x=method,y=foamIndx)) + 
geom_jitter(width=0.2,colour = "grey45") + 
geom_point(data = temp, 
mapping = aes(x = levelNum,y= value),inherit.aes=F,colour="red") + 
geom_segment(data = temp,mapping = aes(x=1,y=0,xend=1,yend = value[1]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = temp$value[1],linetype="dashed",colour = "grey60")+
geom_segment(data = temp,mapping = aes(x=2,y=value[1],xend=2,yend = value[2]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_segment(data = temp,mapping = aes(x=3,y=value[1],xend=3,yend = value[3]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = 0) +
annotate("text", x = 1.3, y = temp$value[1]/2, label = temp$value[1])+
annotate("text", x = 2.3, y = sum(temp$value[1:2])/2, label = temp$coef[2])+
annotate("text", x = 3.3, y = sum(temp$value[c(1,3)])/2, label = formatC(temp$coef[3], digits = 1, format = "f"))+
ylim(0,75)  

ggpubr::ggarrange(A+ggtitle("A - default,\nBM reference"),B+ggtitle("B - releveled,\nHIP reference"),ncol=2)
```


So, from the first of the model outputs above you can say that BM is significantly different than HIP (t= 7.248, p < 0.0001), but that BM is not significantly different from IES (t = 1.831, p = 0.0796). Then, from the second one you can see that HIP is significantly different from IES (t=-5.417, p < 0.0001). You could write this into the main text, include the information in a figure caption (i.e. add it to Figure \ref{fig:coffee}. 

e.g. *The foam index varied significantly among groups (ANOVA: F = `r formatC(x$"F value"[1], digits = 3, format = "f")`, d.f. = `r x$Df[1]` and `r x$Df[2]`, p = `r formatC(x$"Pr(>F)"[1], digits = 3, format = "f")`). The pairwise comparisons in the ANOVA model showed that means of BM and HIP were significantly different (t= 7.248, p < 0.0001), as were those of HIP and IES (t=-5.417, p < 0.0001), but the BM-IES comparison showed no significant difference (t= 1.831, p= 0.0796)*.

## Tukey's Honestly Significant Difference (HSD)

An alternative way to approach these comparisons between groups is to use something called a **post-hoc multiple comparison test**. The words "post-hoc" mean "after the event" -- i.e. after the ANOVA in this case -- while the "multiple comparison" refers to the (potentially many) pairwise comparisons that you would like to make with an ANOVA. One of the most widely used post-hoc tests is called Tukey’s Honestly Significant Difference (Tukey HSD) test. There is a convenient R package called `agricolae` that will do these for you.

```{r eval=FALSE}
#You only need to do this once!
install.packages("agricolae")
```
When you have the package installed, you can load it (using `library`). Then you can run the Tukey HSD test using the function `HSD.test`. The first argument for the function is the name of the model, followed by the name of the variable you are comparing (in this case `method`) and finally `console = TRUE` tells the function to print the output to your computer screen.

```{r}
library(agricolae)
HSD.test(foam_mod2, "method", console=TRUE)
```

The output is long-winded, and the main thing to look at is the part at the end. The key to understanding this is actually written in the output, "`Treatments with the same letter are not significantly different`". 

You could include these in a figure or a table with text like, *"Means followed by the same letter did not differ significantly (Tukey HSD test, p>0.05)"*.

## ANOVA calculation "by hand" (optional)

The ANOVA calculation involves calculating something called an F-value or F-statistic (the F stands for Fisher, who invented ANOVA). This is similar to the t-statistic in that it is a ratio between two quantities, in this case variances. In ANOVA, the F-statistic is calculated as the "**treatment variance**" divided by the "**error variance**".

What does that mean? Let's consider the espresso data set again. 

In the Figure \ref{fig:differentReferences}, you can see on the left (A) the black horizontal line which is the overall mean foam index. The vertical lines are the "errors" or departures from the overall mean value, colour coded by treatment (i.e. method). These can be quantified by summing up their square values (squaring ensures that the summed values are all positive). We call this quantity the *Total Sum of Squares (SSTotal)*. If there is a lot of variation, the sum of squares will be large, if there is little variation the sum of squares will be small. 

On the right hand side (Figure \ref{fig:differentReferences}B) we see the same data points. However, this time the horizontal lines represent the treatment-specific mean values, and the vertical lines illustrate the errors from these mean values. Again we can sum up these as sum of squares, which we call the *Error Sum of Squares (SSError)*. 

The difference between those values is called the *Treatment Sum of Squares (SSTreatment)* and is the key to ANOVA - it represents the importance of the treatment: $$SSTreatment = SSTotal - SSError$$.

If that doesn't make sense yet, picture the case where the treatment-specific means are all very similar, and are therefore very close to the overall mean. Now the difference between the *Total Sum of Squares* and the *Error Sum of Squares* will be small. Sketch out a couple of examples with pen on paper if that helps. You should now see that you can investigate differences among means by looking at variation.


```{r ANOVAbyhand, echo=FALSE,fig.width=8,fig.height=3,fig.align='center', message = FALSE, fig.cap="The relative size of the squared residual errors from the overall mean (SSTotal) (A) and from the treatment-specific means (SSError) (B) tell us about the importance of the treatment variable. The difference between the two values is the \"treatment sum of squares\".", fig.pos = "ht"}

(overallMean <- espresso %>%
summarise(overallMean = mean(foamIndx)) %>%
pull())

#Add index and group means to the data
espressoAugmented <- left_join(espresso,
espresso %>%
group_by(method) %>%
summarise(groupMean = mean(foamIndx))) %>% 
mutate(n = 1:n())


#Plot A = total sum of squares (residuals from overall mean)
SSTotalPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
geom_segment(aes(xend = n, yend = overallMean),alpha=0.4) +
geom_point() + 
geom_hline(yintercept = overallMean) + 
xlab("Index")+
NULL


#Plot B = error sum of squares (residuals from group means)
SSErrorPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
geom_segment(data = espressoAugmented %>% filter(method == "BM"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
geom_segment(data = espressoAugmented %>% filter(method == "HIP"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
geom_segment(data = espressoAugmented %>% filter(method == "IES"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
geom_point() + 
geom_segment(data = espressoAugmented %>% filter(method == "BM"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
geom_segment(data = espressoAugmented %>% filter(method == "HIP"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
geom_segment(data = espressoAugmented %>% filter(method == "IES"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
xlab("Index")+
NULL

ggpubr::ggarrange(SSTotalPlot + ggtitle("A - Total errors\n SSTotal"),SSErrorPlot + ggtitle("B - Treatment-specific error\n SSError"))


#ggsave("ANOVAmodel1.png",width=8,height=3.5,dpi=300,units = "in")
```


In the following I will show how these calculations can be done "by hand" in R. The purpose of showing you this is to demonstrate exactly how the `lm` model that you fitted above works, and prove to yourself that it is not rocket science... you will never need to do this in real life, because you have the wonderful `lm` function.

Here goes...

First, calculate the total sum of squares:

```{r}
(SSTotal = sum((overallMean-espresso$foamIndx)^2))
```

Now calculate the group-specific means:

```{r}
(groupMeans<-espresso %>%
group_by(method) %>%
summarise(groupMean = mean(foamIndx)) %>%
pull(groupMean))
```

Now add those group-specific mean values to the dataset using `left_join` so that you can calculate the group-specific errors.

```{r}
espresso <- left_join(espresso,espresso %>%
                        group_by(method) %>%
                        summarise(groupMean = mean(foamIndx))) %>%
  mutate(groupSS = (foamIndx - groupMean)^2)

head(espresso)
```

Then, to calculate the errors:

```{r}
(SSError <- espresso %>%
summarise(sum(groupSS)) %>%
pull())
```



From there, you can calculate the **Treatment Sum of Squares**

```{r}
(SSTreatment <- SSTotal - SSError)
```

So far, so good - but we can't just look at the ratio of SSTreatment/SSError, because sum of square errors always increase with sample size. We can account for this by dividing 

We need to take account of sample size (degrees of freedom) by dividing these sum of squares by the degrees of freedom to give us variances. There are 3 treatment groups and 9 samples per group. Therefore there are 2 degrees of freedom for the treatment, and 8 degrees of freedom per each of the three treatments, giving a total of 8*3 = 24 error degrees of freedom.

Now we need to correct for degrees of freedom, which will give us variances.

```{r}
(meanSSTreatment <- SSTreatment/2)
(meanSSError <- SSError/24)
```

The F-statistic is then the ratio of these values.

```{r}
(Fstat <- meanSSTreatment/meanSSError)
```

We can "look up" the p-value associated with this F-statistic using the `pf` function (`pf` stands for probability of f) like this:

```{r}
1-pf(Fstat,df1=2,df2=24)
```

As you can see, the method is a bit laborious and time consuming but it is conceptually fairly straightforward - it all hinges on the ratio of variation due to treatment effect vs. overall variation. Signal and noise.


<!-- the following is some code to make a plot with shuffled data as an illustration of how the ANOVA works --->
```{r, echo=FALSE}

espressoShuffled <- espresso %>%
  mutate(foamIndx = sample(foamIndx))

overallMean <- espressoShuffled %>%
  summarise(overallMean = mean(foamIndx)) %>%
  pull()

#Add index and group means to the data
espressoAugmented <- left_join(espressoShuffled,
                               espressoShuffled %>%
                                 group_by(method) %>%
                                 summarise(groupMean = mean(foamIndx))) %>% 
  mutate(n = 1:n())


#Plot A = total sum of squares (residuals from overall mean)
SSTotalPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
  geom_segment(aes(xend = n, yend = overallMean),alpha=0.4) +
  geom_point() + 
  geom_hline(yintercept = overallMean) + 
  xlab("Index")+
  NULL


#Plot B = error sum of squares (residuals from group means)
SSErrorPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
  geom_segment(data = espressoAugmented %>% filter(method == "BM"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
  geom_segment(data = espressoAugmented %>% filter(method == "HIP"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
  geom_segment(data = espressoAugmented %>% filter(method == "IES"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
  geom_point() + 
  geom_segment(data = espressoAugmented %>% filter(method == "BM"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
  geom_segment(data = espressoAugmented %>% filter(method == "HIP"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
  geom_segment(data = espressoAugmented %>% filter(method == "IES"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
  xlab("Index")+
  NULL

xx <-ggpubr::ggarrange(SSTotalPlot + ggtitle("A - Total errors\n SSTotal"),SSErrorPlot + ggtitle("B - Treatment-specific error\n SSError"))


#ggsave(plot = xx,filename = "ANOVAmodel2.png",width=8,height=3.5,dpi=300,units = "in")

```


<!--chapter:end:09-ANOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Linear models with a several categorical explanatory variables

```{r}
require(tidyverse)
require(ggpubr)

```

## Background

In the one-way ANOVA we covered in the previous worksheet we were interested in understanding the effect of a single categorical explanatory variable with two or more levels on a continuous response variable. Although the explanatory variable must be categorical (i.e. with discrete levels), it could represent a continuous variable. For example, the explanatory variable could be a two-level soil nutrient level (high or low), even though nutrient level is a continuous variable and one could measure the actual quantitative value of nutrients in mg/g.

The two-way ANOVA is an extension of one-way ANOVA that allows you to investigate the effect of **two** categorical variables. This can be useful in an experimental context. 

For example, one might have run an experiment investigating in the effect of two types of diet (*lowProtein* and *highProtein*), and genotype (*gt1* and *gt2*), on adult size of a pest species. It is worth thinking about what potential outcomes there are for this experiment. There may be no effect of diet, and no effect of genotype. There may be an effect of one of these variables but not the other. The effect of the diet might be the same for the different genotypes, or it might be different. Some of these different possibile outcomes are illustrated in Figure \ref{fig:possibilities}. the titles indicate with Y (yes) or N (no) whether the figure shows a significant diet, genotype (gt) or interaction (int) effect. The dotted lines joining the estimates for the two genotypes are a kind of **interaction plot**: where they are parallel, there is no interaction.


```{r possibilities, echo=FALSE,fig.width=10,fig.height=4,fig.align='center',fig.cap="Some potential results of the experiment. There may be a significant effect (or not) of both of the main effects (diet and genotype) and there may be a significant interaction effect (or not).", fig.pos = "ht",message=FALSE}
fakeData <- expand.grid(diet = c("lowP","highP"),genotype = c("gt1","gt2"))



#No effect of diet nor genotype
A<- ggplot(fakeData %>% 
         mutate(lengthMM = c(5,5,5,5)),
       aes(x =genotype,y = lengthMM,fill = diet, group=diet)) + 
  geom_col(position = position_dodge()) +
  ylim(0,9) + 
  geom_point(position = position_dodge(width = 1)) + 
  theme(axis.text.y = element_blank()) + 
  ggtitle("A) diet N; gt: N; int: N")

#Genotype significant, not diet
B<- ggplot(fakeData %>% 
         mutate(lengthMM = c(5,5,8,8)),
       aes(x =genotype,y = lengthMM,fill = diet, group=diet)) + 
  geom_col(position = position_dodge()) +
  ylim(0,9) + 
  geom_line(position = position_dodge(width = 1),linetype="dashed") + 
  geom_point(position = position_dodge(width = 1)) + 
  theme(axis.text.y = element_blank()) + 
  ggtitle("B) diet N; gt: Y; int: N")

#Diet significant, not genotype
C<- ggplot(fakeData %>% 
         mutate(lengthMM = c(5,8,5,8)),
       aes(x =genotype,y = lengthMM,fill = diet, group=diet)) + 
  geom_col(position = position_dodge()) +
  ylim(0,9) + 
  geom_line(position = position_dodge(width = 1),linetype="dashed") + 
  geom_point(position = position_dodge(width = 1)) + 
  theme(axis.text.y = element_blank()) + 
  ggtitle("C) diet Y; gt: N; int: N")



#Diet significant, genotype significant
#No interaction

D<- ggplot(fakeData %>% 
         mutate(lengthMM = c(3,4,7,8)),
       aes(x =genotype,y = lengthMM,fill = diet, group=diet)) + 
  geom_col(position = position_dodge(1)) +
  ylim(0,9) + 
  geom_line(position = position_dodge(width = 1),linetype="dashed") + 
  geom_point(position = position_dodge(width = 1)) + 
  theme(axis.text.y = element_blank())  + 
  ggtitle("D) diet Y; gt: Y; int: N")


#Diet significant, genotype  significant
#Significant interaction

E<- ggplot(fakeData %>% 
         mutate(lengthMM = c(3,4,5,8)),aes(x =genotype,y = lengthMM,fill = diet, group=diet)) + 
  geom_col(position = position_dodge(1)) +
  ylim(0,9) + 
  geom_line(position = position_dodge(width = 1),linetype="dashed") + 
  geom_point(position = position_dodge(width = 1)) + 
    theme(axis.text.y = element_blank()) + 
  ggtitle("E) diet Y; gt: Y; int: Y")

#Diet not significant, genotype not significant
#Significant interaction

Fa<- ggplot(fakeData %>% 
             mutate(lengthMM = c(3,6,6,3)),aes(x =genotype,y = lengthMM,fill = diet, group=diet)) + 
  geom_col(position = position_dodge(1)) +
  ylim(0,9) + 
  geom_line(position = position_dodge(width = 1),linetype="dashed") + 
  geom_point(position = position_dodge(width = 1)) + 
  theme(axis.text.y = element_blank()) + 
  ggtitle("E) diet N; gt: N; int: Y")


ggpubr::ggarrange(A,B, C,D, E, Fa, nrow=2,ncol=3)


```



In the model we aim to quantify these effects, and ask if they are statistically significant (i.e. if the effect sizes are >0). We divide the effects of the explanatory variables into two types: **main effects** and **interaction effects**. The main effects are the overall effect of the explanatory variables (genotype and diet in this case) while the interaction effect allows us to ask whether one main effect *depends on another*. In this case we are asking whether *the effect of diet depends on genotype (and vice versa)*. Make sure that you understand this important concept.


```{r dataPrep, echo=FALSE,fig.width=5,fig.height=3,fig.align='center', fig.pos = "ht",message=FALSE}

#Simulate some data.
set.seed(9724)
myData <- expand.grid(replicate = 1:12,diet = c("lowProtein","highProtein"),genotype = c("gt1","gt2")) %>%
  left_join(expand.grid(diet = c("lowProtein","highProtein"),genotype = c("gt1","gt2")) %>%
              mutate(meanSize = c(20,26,16,17.5),semSize = c(2.5,2.5,2,2))) %>%
  mutate(lengthMM = round(rnorm(48,meanSize,semSize)),2) %>%
  select(diet,genotype,lengthMM)

write.csv(myData,"CourseData/insectDiet.csv",row.names = FALSE)
rm(myData)
```




## Fitting a two-way ANOVA model

Let's use R to fit a two-way ANOVA model using data from the example I just described. As with one-way ANOVA, you can fit a two-way ANOVA model in R using `lm`. 

First, import the `insectDiet.csv` data and plot it, to produce a plot like in Figure \ref{fig:dietAndGenotype}. From looking at the graph in Figure \ref{fig:dietAndGenotype} you can see (a) genotype 1 tends to be larger than genotype 2; (b) insects raised on a  high protein diet tend to be larger than those on a low protein diet; and (c) the effect of the diet (i.e. the *difference* in size between the insects raised on the different diets) is larger for genotype 1 than it is for genotype 2. But are these differences statistically meaningful?




```{r dietAndGenotype,eval=TRUE,fig.width=5,fig.height=3,fig.align='center', fig.pos = "ht",message=FALSE,fig.cap="The effect of diet protein content and genotype on adult size of an insect species"}
insectDiet <- read.csv("CourseData/insectDiet.csv")

ggplot(insectDiet,aes(x = genotype,y = lengthMM,fill=diet)) +
  geom_boxplot() +
  xlab("Genotype")+
  ylab("Length (mm)")
```


To address this question, we will fit a linear model (the two-way ANOVA) to estimate the effects of diet and genotype.

The model formula is `lengthMM ~ genotype + diet + genotype:diet`. 

Let's try to understand this. The `genotype + diet` part represents the **main effects** of these two variables, and the `genotype:diet` part represents the **interaction effect** between them. This formula*can* be shortened to `lengthMM ~ genotype * diet` (i.e. this is exactly equivalent to the more complicated-looking formula), but I recommend to use the longer version because it is clearer.

So we fit the model like this - putting the formula first, then telling R which data to use:

```{r}
mod_A <- lm(lengthMM ~ genotype + diet + genotype:diet, data = insectDiet)
```

Then we can look at diagnostic plots, as with ANOVA etc.:

```{r}
library(ggfortify)
autoplot(mod_A)
```

These all look OK. The slightly odd structure in the QQ-plot is caused by the fact that the length data are rounded to the nearest millimeter. There is no evidence of heteroscedasticity (left hand plots) now any major outliers. 


## Summarising the model (`anova`)

Since we are satisfied with the diagnostic plots we can proceed by summarising the model using first `anova` and then `summary`.

```{r}
anova(mod_A)
```

This summary `Analysis of Variance Table` is similar to the ones you have already seen for one-way ANOVA and linear regression. It just has some extra rows because you have extra explanatory variables. It shows you the degrees of freedom for the different terms in the model (all 1, because they have two levels), the sum of squares (`Sum Sq`) and mean sum of squares (`Mean Sq`) and the associated `F value` and p-value (`Pr(>F)`). Those F values are all large, leading to highly-significant p-values. 

This means that all of those terms in the model explain a significant proportion of the variation in insect length. 

But as you know, this summary table doesn't tell you the direction of the effects. The obvious way to understand your data is to simply look at the plot you have already produced. You could also make an **interaction plot** which is a simplified version of the plot of the raw data. 

To do this you first need to create a summary table using `dplyr` tools `summarise` and `group_by` to get the mean and standard errors of the mean:


```{r}
insectDiet_means <- 
  insectDiet %>% 
  group_by(genotype, diet) %>% # <- remember to group by *both* factors
  summarise(MeanLength = mean(lengthMM),SELength = sd(lengthMM)/sqrt(n()))
insectDiet_means
```

Then you can make a simple plot of this information by plotting points, and lines joining them:

```{r interactionPlot,fig.width=5,fig.height=3,fig.align='center',fig.cap="", fig.pos = "ht",message=FALSE}

(A<-ggplot(insectDiet_means, 
       aes(x = genotype, y = MeanLength, colour = diet, group = diet)) +
  geom_point(size = 4) + 
  geom_line())
```

You could add error bars to the points by adding a line defining the `ymin` and `ymax` values from the data summary like this:

```{r errorBars,fig.width=5,fig.height=3,fig.align='center',fig.cap="", fig.pos = "ht",message=FALSE}

ggplot(insectDiet_means, 
       aes(x = genotype, y = MeanLength, colour = diet, group = diet,
           ymin = MeanLength - SELength, ymax = MeanLength + SELength)) +
  geom_point(size = 4) + 
  geom_line() + 
  geom_errorbar(width = 0.1)
```

But are these points statistically significantly different from each other?
To answer that question we need to use a post-hoc test 

```{r}
library(agricolae)
HSD.test(mod_A, trt = c("diet", "genotype"), console = TRUE)
```

The important part of this output is at the bottom where it tells us ` Treatments with the same letter are not significantly different.`. You can see that the mean lengths between diets for genotype 1 are significantly different (they do not share a letter). However, there is no significant difference between diets for genotype 2 (they share the same letter, `c`). The two genotypes are also significantly different from each other. 


## Summarising the model (`summary`)

This (above) is generally enough information for a complete write up of results.  However, you can ask R to provide the model summary that includes the $R^2$ values, coefficient estimates and standard errors using `summary`.

```{r}
summary(mod_A)
```

The most useful thing shown here is the $R^2$ value. Because we have several terms in the model we should use the `Adjusted R-squared` value of 0.742. This indicates that our model explains 74.2% of variation in insect length.

The next bit is not 100% necessary most of the time...

We already have a good idea of the mean values and standard errors for these data look because we calculated them above directly from the data. For completeness though I will now run through the coefficient estimates part of the summary table.

The coefficient `Estimates` here are interpreted in a similar way to a one-way ANOVA. Again, it is important to know what the reference point is. When you understand this you can reconstruct the mean values for the various levels of the variables that are estimated by the model. You will see that the model estimates lead to precisely the same estimates as obtained from summarising the data. 

Here you can see that: 

* The `(Intercept)` is 25.8333 and must refer to the point for *genotype 1* on a *high protein diet* (look at the value of the intercept and compare to the graph/summary table, and/or the output from the Tukey test).

* The second coefficient (`genotypegt2`) is -8.0833 which is the **difference** between the reference (intercept) and the value for *genotype 2* on a *high protein diet*:   (25.8333 + (-8.0833) = 17.75).

* The third coefficient (`dietlowProtein`) is -5.1667 which is the difference between the reference point and for *genotype 1* on a *low protein diet*: (25.8333 + (-5.1667) = 20.6666). 

* The final coefficient `dietlowProtein:genotypegt2` is 4.25 and is "interaction effect" of diet and genotype and represents the *additional* effect of genotype when it is on diet.  In other words, in comparison to the reference point (genotype 1 & high protein diet), the effect of a low protein diet is negative (-8.0833), as is the effect of being genotype 2 (-5.1667). However, having both a low protein diet **and** being genotype 2 leads to an additional positive effect (4.25) on length. The resulting estimate of mean length for *genotype 2* on a *low protein diet* is  25.8333 + (-8.0833) + (-5.1667) + 4.25 = 16.833.

This is a bit complicated so my advice is generally to refer to the figures and the outputs of the `Tukey.HSD` function to obtain the estimate in the different groups.



The logic and methods of the two-way ANOVA can be extended to produce $n$-way ANOVA with $n$ categorical variables. 






## A 2-way ANOVA on a fish behaviour study

### Background

Individual differences in animal personality and external appearance such as colouration patterns have both been extensively studied separately. A significant body of research has explored many of pertinent ecological and biological aspects that can be affected by them and their impact upon fitness. Currently little is known about how both factors interact and their effect on reproductive success.

Researchers carried out a study looking at differences in personality and its interaction with colour phenotype in zebrafish (*Danio rerio*). They used two colour morphs, "homogenous" which has clearly defined lateral stripes,  and "heterogenous" which has more variable and less clear patterns.

They also assigned individuals to two personality types which they called "Proactive" (adventurous, risk taking) and "Reactive" (timid, less risk taking). They did this by recording how they explore a new environment

The two variables of interest are:

* Colour pattern (homogenous and heterogenous)
* Personality (proactive and reactive)

The research questions are:

1) What is the relative influence of colour pattern and personality? Which is more important?
2) How do the variables interact to determine fitness? e.g. do proactive individuals do better than reactive ones, and does this depend on colour pattern? Or some other pattern?



### Your task

1) Import the data set, `fishPersonality.csv`

2) Plot the data (e.g. as a box plot)

3) Fit an ANOVA model using `lm`.

4) Look at diagnostic plots from this model (`autoplot`)

5) Use `anova` to get an Analysis of Variance summary table, and interpret the results.

6) Get the coefficient summary (`summary`) and interpret the output.

7) Do post-hoc Tukey tests (e.g. using `HSD.test` from the `agricolae` package). Interpret the results.


8) Sum up your findings with reference to the initial research questions.





<!--chapter:end:10-Two-way_ANOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Linear models with a single continuous explanatory variable

## Background

Linear regression models, at their simplest, are a method of estimating the linear (straight line) relationships between two continuous variables. As an example, picture the relationship between two variables height and hand width (Figure \ref{fig:exampleRegression}). In this figure there is a clear  relationship between the two variables, and the straight line running through the cloud of data points is the fitted linear regression model. 

The aim of linear regression is to (1) determine if there is a meaningful statistical relationship between the explanatory variable(s) and the response variable, and (2) to quantify those relationships by estimating the characteristics of those relationships. These characteristics include the slope and intercepts of fitted models, and the amount of variation explained by variables in the model.

```{r exampleRegression, echo=FALSE,fig.width=3,fig.height=3,fig.align='center',fig.cap="A linear regression model fitted through data points.", fig.pos = "ht"}
classData <- read.csv("CourseData/classData.csv")
ggplot(classData,aes(x=HandWidth,y=Height)) + 
  geom_point() +
  geom_smooth(se=FALSE,method = "lm")
```


## Some theory

To understand linear regression models it is important to know that the equation of a straight line is $y = ax+b$. In this equation, $y$ is the response variable and $x$ is the explanatory variable, and $a$ and $b$ are the slope and intercept of the line with the vertical axis (y-axis). These ($a$ and $b$) are called **coefficients**. These are illustrated  in Figure \ref{fig:lineEquation}.



```{r lineEquation, echo=FALSE,fig.width=7,fig.height=3,fig.align='center',fig.cap="The equation of straight lines.", fig.pos = "ht"}
#An example showing regression equation
a1 = 2; b1 = 1

df_reg <- data.frame(x = c(0,4,5,7)) %>%
mutate(y = a1 * x + b1)

A <- ggplot(df_reg)+
  geom_segment(aes(x=x[1],y=y[1],xend=x[4],yend=y[4]),colour = "#b20000") +
  geom_hline(yintercept = 0,colour="grey60")+
  geom_vline(xintercept = 0,colour="grey60") + 
  #geom_vline(xintercept = df_reg$x[2],colour="grey60") + 
  geom_hline(yintercept = b1,linetype="dashed",colour="grey40") + 
  geom_segment(aes(x=x[2],y=y[2],xend=x[3],yend=y[2]),linetype="dashed",colour="grey40")+
  geom_segment(aes(x=x[3],y=y[3],xend=x[3],yend=y[2]),linetype="dashed",colour="grey40")+
  geom_text(aes(x=mean(x[2:3]),y=y[2]-.5,label="1"))+
  geom_text(aes(x=mean(x[3]+.1),y=mean(y[2:3]),label=paste0(a1," (slope)"),hjust="left")) +
  xlab("x") + 
  ylab("y") +
  geom_text(aes(x=1,y=13,label="y = ax + b",hjust="left")) +
  geom_text(aes(x=1,y=12,label=paste0("y = ",a1,"x + ",b1),hjust="left")) +
  geom_text(aes(x=.5,y=b1/2,label=paste(b1,"(intercept)"),hjust="left")) +
  geom_segment(aes(x=.3,y=0,xend=.3,yend=b1),
               arrow=arrow(angle = 30, length = unit(.1, "cm"),
                                                        ends = "both", type = "open"),colour="grey40")+
  NULL

a2 = -2.3; b2 = 11

df_reg <- data.frame(x = c(0,4,5,7)) %>%
  mutate(y = a2 * x + b2)

B <- ggplot(df_reg)+
  geom_segment(aes(x=x[1],y=y[1],xend=x[4],yend=y[4]),colour = "#b20000") + 
  geom_hline(yintercept = 0,colour="grey60")+
  geom_vline(xintercept = 0,colour="grey60") + 
  #geom_vline(xintercept = df_reg$x[2],colour="grey60") + 
  geom_hline(yintercept = b2,linetype="dashed",colour="grey40") + 
  geom_segment(aes(x=x[2],y=y[2],xend=x[3],yend=y[2]),linetype="dashed",colour="grey40")+
  geom_segment(aes(x=x[3],y=y[3],xend=x[3],yend=y[2]),linetype="dashed",colour="grey40")+
  geom_text(aes(x=mean(x[2:3]),y=y[2]+.5,label="1"))+
  geom_text(aes(x=mean(x[3]+.1),y=mean(y[2:3]),label=paste0(a2," (slope)"),hjust="left")) +
  xlab("x") + 
  ylab("y") +
  geom_text(aes(x=1,y=13,label="y = ax + b",hjust="left")) +
  geom_text(aes(x=1,y=12,label=paste0("y = ",a2,"x + ",b2),hjust="left")) +
  geom_text(aes(x=.5,y=b2/2,label=paste(b2,"(intercept)"),hjust="left")) +
  geom_segment(aes(x=.3,y=0,xend=.3,yend=b2),
               arrow=arrow(angle = 30, length = unit(.1, "cm"),
                           ends = "both", type = "open"),colour="grey40")+
  NULL
ggpubr::ggarrange(A,B,nrow=1,ncol=2)

#ggsave("Equation_of_line.png",width = 15,height=8,dpi=300,units="cm")
```

When looking at data points on a graph, unless all of the data points are arranged perfectly along a straight line, there will be some distance between the points and the line. These distances, measured parallel to the vertical axis, are called residuals (you have encountered them before in this course). These residuals represent the variation left after fitting the line (a linear model) through the data. Because we want to fit a model that explains as much variation as possible, it is intuitive that we should wish to minimise this residual variation. 

One way of doing this is by minimising the sum of squares of the residuals (again, you have come across this concept a few times before). In other words, we add up the squares of each of the residuals. We square the values, rather than simply adding up the residuals themselves because we want to ensure that the positive and negative values don't cancel each other out (a square of a negative number is positive). This method is called **least squares** regression and is illustrated in Figure \ref{fig:leastSquares}: Which is the best fitting line?

```{r leastSquares, echo=FALSE,fig.width=4.8,fig.height=4,fig.align='center',fig.cap="Residuals and least squares: which is the best fitting line?",fig.pos = "ht"}
classData <- read.csv("CourseData/classData.csv")
#Black thread least squares
slopeCoef <-    c( 6.10, 4.66, -1, 0.00)
interceptCoef <-c(125, 134,  185 ,175) 

residualData <- classData %>% 
  select(Height,HandWidth) %>%
  mutate(Height_Pred = (slopeCoef[1] * HandWidth) + interceptCoef[1])

A<-ggplot(residualData,aes(x=HandWidth,y=Height,xend = HandWidth,yend = Height_Pred)) + 
  geom_segment(col="grey75") + 
  geom_point() +
 geom_segment(aes(x = min(HandWidth),
                   xend = max(HandWidth),
                   y = (slopeCoef[1] * min(HandWidth)) + interceptCoef[1],
                   yend = (slopeCoef[1] * max(HandWidth)) + interceptCoef[1]),inherit.aes = FALSE,colour = "#b20000") + ylim(158,195) + 
  ggtitle(paste0("A) slope:  ",slopeCoef[1],"; int: ",interceptCoef[1])) + theme_minimal()


residualData <- classData %>% 
  select(Height,HandWidth) %>%
  mutate(Height_Pred = (slopeCoef[2] * HandWidth) + interceptCoef[2])

B<-ggplot(residualData,aes(x=HandWidth,y=Height,xend = HandWidth,yend = Height_Pred)) + 
  geom_segment(col="grey75") + 
  geom_point() +
 geom_segment(aes(x = min(HandWidth),
                   xend = max(HandWidth),
                   y = (slopeCoef[2] * min(HandWidth)) + interceptCoef[2],
                   yend = (slopeCoef[2] * max(HandWidth)) + interceptCoef[2]),inherit.aes = FALSE,colour = "#b20000") + ylim(158,195) + 
  ggtitle(paste0("B) slope:  ",slopeCoef[2],"; int: ",interceptCoef[2])) + theme_minimal()

residualData <- classData %>% 
  select(Height,HandWidth) %>%
  mutate(Height_Pred = (slopeCoef[3] * HandWidth) + interceptCoef[3])

C<-ggplot(residualData,aes(x=HandWidth,y=Height,xend = HandWidth,yend = Height_Pred)) + 
  geom_segment(col="grey75") + 
  geom_point() +
  geom_segment(aes(x = min(HandWidth),
                   xend = max(HandWidth),
                   y = (slopeCoef[3] * min(HandWidth)) + interceptCoef[3],
                   yend = (slopeCoef[3] * max(HandWidth)) + interceptCoef[3]),inherit.aes = FALSE,colour = "#b20000") + ylim(158,195) + 
  ggtitle(paste0("C) slope:  ",slopeCoef[3],"; int: ",interceptCoef[3])) + theme_minimal()

residualData <- classData %>% 
  select(Height,HandWidth) %>%
  mutate(Height_Pred = (slopeCoef[4] * HandWidth) + interceptCoef[4])

D<-ggplot(residualData,aes(x=HandWidth,y=Height,xend = HandWidth,yend = Height_Pred)) + 
  geom_segment(col="grey75") + 
  geom_point() +
  geom_segment(aes(x = min(HandWidth),
                   xend = max(HandWidth),
                   y = (slopeCoef[4] * min(HandWidth)) + interceptCoef[4],
                   yend = (slopeCoef[4] * max(HandWidth)) + interceptCoef[4]),inherit.aes = FALSE,colour = "#b20000") + ylim(158,195) + 
  ggtitle(paste0("D) slope:  ",slopeCoef[4],"; int: ",interceptCoef[4])) + theme_minimal()


ggpubr::ggarrange(A,B,C,D,nrow=2,ncol=2)
#ggsave("Example_LeastSquares.png",width = 16.5,height=12,dpi=300,units="cm")


```

In fact, these residuals represent "error" caused by factors including measurement error, random variation, variation caused by unmeasured factors etc. This error term is given the label, $\epsilon$. Thus we can write the model equation as:

$$y = ax+b+\epsilon$$

Sometimes, this equation is written with using the beta symbol ($\beta$) for the coefficients, so that the slope is $\beta_0$ and the intercept is $\beta_1$ for example.

$$y = \beta_0 x+\beta_1+\epsilon$$

The idea is that this equation, and its coefficients and error estimates, describe the relationship we are interested in (including the error or uncertainty). 

Together this information allows us to not only determine if there **is** a statistically significant relationship, but also what the nature of the relationship is, and the uncertainty in our estimates.


## Evaluating a hypothesis with a linear regression model

Usually, the most important hypothesis test involved with a linear regression model relates to the slope: **is the slope coefficient significantly different from 0?**, or should we accept the null hypothesis that the slope is no different from 0.

Sometimes hypotheses like this are a bit boring, because we already know the answer before collecting and analysing the data. What we usually **don't** know is the nature of the relationship (the slope, intercept, their errors, and amount of variation explained). Usually it is more interesting and meaningful to focus on those details. 

The following example, where we focus on the relationship between hand width and height, is one of these "boring" cases: we already know there is a relationship. Nevertheless, we'll use this example because it helps us understand how this hypothesis test works.

The aim of this section is to give you some intuition on how the hypothesis test works.

We can address the slope hypothesis by calculating an F-value in a similar way to how we used them in ANOVA. Recall that F-values are ratios of variances. To understand how these work in the context of a linear regression we need to think clearly about the slope hypothesis: The **null hypothesis** is that the slope is **not** significantly different to 0 (that the data can be explained by random variation). The **alternative hypothesis** is that the slope is significantly different from 0. 

The first step in evaluating these hypotheses is to calculate what the **total sum of squares**^[sum of squares is simply a way to estimate variation.] is when the null hypothesis is true (Figure \ref{fig:slopeHypo}A) - this value is the total variation that the model is trying to explain. 

Then we fit our model using least squares and figure out what the **residual sum of squares** is from this model (Figure \ref{fig:slopeHypo}B). This is the amount of variation left after the model has explained *some* of the total variation - it is sometimes called *residual error*, or simply *error*.

The difference between these two values is the **explained sum of squares**, which measures the amount of variation in $y$ explained by variation in $x$. The rationale for this is that the model is trying to explain total variation. After fitting the model there will always be some unexplained variation ("*residual error*") left. If we can estimate total variation and unexplained variation, then the amount of variation explained can be calculated with a simple subtraction:

$$Total = Explained + Residual$$
... and, therefore ...
$$Explained = Total - Residual$$


Before using these values we need to standardise them to control for sample size. This is necessary because sum of squares will *always* increase with sample size. We make this correction by dividing our sum of squares measures by the degrees of freedom. The d.f. for the **explained sum of squares** is 1, and the d.f. for the **residual sum of squares** is the number of observations minus 2. The result of these calculations is the **mean explained sum of squares** (mESS) and the **mean residual sum of squares** (mRSS). These "mean" quantities are **variances**, and the ratio between them gives us the **F-value**. Notice that this is very similar to the variance ratio used in the ANOVA.

$$F = \frac{mESS}{mRSS}$$

If the *explained variance* (mESS) is large compared to the *residual error variance* (mRSS), then F will be large. The size of F tells us how likely or unlikely it is that the null hypothesis is true. When F is large, the probability that the slope is significantly different from 0 is high. To obtain the actual probabilities, the F-value must be compared to a theoretical distribution which depends on the two degrees of freedom (explained and residual d.f.). Once upon a time you would have looked this up in a printed table, but now R makes this very straightforward.

```{r slopeHypo, echo=FALSE,fig.width=8,fig.height=3,fig.align='center',fig.cap="(A) the total variation around the overall mean Height value (B) the residual error of the model.",fig.pos = "ht"}

classData <- read.csv("CourseData/classData.csv")


model <- lm(Height ~ HandWidth, data = classData)

slopeCoefTotal <-    0
interceptCoefTotal <- mean(classData$Height)

residualDataTotal <- classData %>% 
  select(Height,HandWidth) %>%
  mutate(Height_Pred = (slopeCoefTotal * HandWidth) + interceptCoefTotal)

A<-ggplot(residualDataTotal,aes(x=HandWidth,y=Height,xend = HandWidth,yend = Height_Pred)) + 
  geom_segment(col="grey75") + 
  geom_point() +
  geom_segment(aes(x = min(HandWidth),
                   xend = max(HandWidth),
                   y = (slopeCoefTotal * min(HandWidth)) + interceptCoefTotal,
                   yend = (slopeCoefTotal * max(HandWidth)) + interceptCoefTotal),inherit.aes = FALSE,colour = "#b20000")


slopeCoefError <-    model$coef[2]
interceptCoefError <-model$coef[1]

residualDataError <- classData %>% 
  select(Height,HandWidth) %>%
  mutate(Height_Pred = (slopeCoefError * HandWidth) + interceptCoefError)

B<-ggplot(residualDataError,aes(x=HandWidth,y=Height,xend = HandWidth,yend = Height_Pred)) + 
  geom_segment(col="grey75") + 
  geom_point() +
  geom_segment(aes(x = min(HandWidth),
                   xend = max(HandWidth),
                   y = (slopeCoefError * min(HandWidth)) + interceptCoefError,
                   yend = (slopeCoefError * max(HandWidth)) + interceptCoefError),inherit.aes = FALSE,colour = "#b20000")

ggarrange(A+ggtitle("A - total errors"),B+ggtitle("A - residual errors"),ncol=2)

```



## Assumptions

These models have similar assumptions to the other linear models^[t-tests, ANOVA and linear regression are all types of linear model, mathematically]. These are (1) that the relationship between the variables is linear (hence the name); (2) that the data are continuous variables; (3) that the observations are randomly sampled; (4) that the errors in the model (the "residuals") can be described by a normal distribution; and (5) and that the errors are "homoscedastic" (that they are constant through the range of the data). You can evaluate these things by looking at diagnostic plots after you have fitted the model. See page 112-113 in GSWR for a nice explanation. 

## An example using R

Let's now use R to fit a linear regression model to estimate the relationship between hand width and height. One application for such a model could be to predict height from a hand print, for example left at a crime scene.


First, load the data:
```{r}

classData <- read.csv("CourseData/classData.csv")
```

We should then plot the data to make sure it looks OK.

```{r fig.width=4.5,fig.height=3.5,fig.align='center', fig.pos = "ht"}
ggplot(classData,aes(x=HandWidth,y=Height)) + 
  geom_point()
```


This looks OK, and the relationship looks fairly linear. Now we can fit a model using the `lm` function (same as for ANOVA!).^[R knows that this is a linear regression rather than an ANOVA because the explanatory variable is numeric rather than categorical - smart!.]

The response variable is always the one we would like to predict,  in this case `Height`. The explanatory variable (sometimes called the predictor) is `HandWidth`. These are added to the model using a formula where they are separated with the `~` ("tilde") symbol: `Height ~ HandWidth`. In the model expression, we also need to tell R where the data are using the `data = ` argument. We can save the model as an R object by naming it e.g. `mod_A <- `.

```{r}
mod_A <- lm(Height ~ HandWidth, data = classData)
```

Before proceeding further we should evaluate the model using a diagnostic plot. We can do this using the `autoplot` function in the `ggfortify` package (you may need to install and/or load the package).

```{r,fig.width=5,fig.height=4.2,fig.align = 'center',fig.pos = "ht"}
library(ggfortify)
autoplot(mod_A)
```
These diagnostic plots allow you to check that the assumptions of the model are not violated. On the left are two plots which (more or less) show the same thing. They show how the residuals (the errors in the model) vary with the predicted value (height). Looking at the plots allows a visual test for constant variance (homoscedasticity) along the range of the data. In an ideal case, there should be no pattern (e.g. humps) in these points. On the top right is the QQ-plot which shows how well the residuals match up to a theoretical normal distribution. In an ideal case, these points should line up on the diagonal line running across the plot. The bottom right plot shows "leverage" which is a measure of how much influence individual data points have on the model. Outliers will have large leverage and can mess up your model. Ideally, the points here should be in a cloud, with no points standing out from the others. Please read the pages 112-113 in the textbook GSWR for more on these. In this case, the model looks pretty good.

Now that we are satisfied that the model doesn't violate the assumptions we can dig into the model to see what it is telling us. 


To test the (slightly boring) slope hypothesis we use the `anova` function (again, this is the same as with the ANOVA).

```{r}
anova(mod_A)
```

When you run this function, you get a summary table that looks exactly like the one you got with an ANOVA. There are degrees of freedom (`Df`), Sums of Squares (`Sum Sq`), Mean Sums of Squares (`Mean Sq`) and the `F value` and p-value (`Pr(>F)`). 
The most important parts of this table are the F value (`r formatC( anova(mod_A)$F[1], digits = 4, format = "f")`) and the p-value (`r formatC(anova(mod_A)$Pr[1], digits = 8, format = "f")`): as described above, large F values lead to small p-values. This tells us that it is VERY unlikely that the null hypothesis is true and we should accept the alternative hypothesis (that height is associated with hand width)

We could report the results of this hypothesis test like this: 
*There was a statistically significant association between hand width and height (F = `r formatC( anova(mod_A)$F[1], digits = 4, format = "f")`, d.f. = `r anova(mod_A)$Df[1]`,`r anova(mod_A)$Df[2]`, p < 0.001)*

Now we can dig deeper by asking for a `summary` of the model. 

```{r}
summary(mod_A)
```

This summary has a lot of information. First we see `Call` which reminds us what the formula we have used to fit the model. Then there is some summary information about the residuals. Ideally these should be fairly balanced around 0 (i.e. the `Min` value should be negative but with the same magnitude as `Max`). If they are **wildly** different, then you might want to check the data or model. In this case they look OK.

Then we get to the important part of the table - the `Coefficients`. This lists the coefficients of the model and shows first the `Intercept` and then the slope, which is given by the name of the explanatory variable (`HandWidth` here). For each coefficient we get the `Estimate` of its value, and the uncertainty in that estimate (the standard error (`Std. Error)).

These estimates and errors are each followed by a `t value` and a p-value (`Pr(>|t|`)). These values provide a test of whether the slope/intercept is different from zero. In this case they both are. The t-tests are indeed doing t-tests of these estimates, in the same way that a regular t-test works, so that the significance depends on the ratio between signal (the estimate) and the noise (the standard error).  This is illustrated for the coefficient estimates for our model in Figure \ref{fig:coefPlot}.


```{r coefPlot, echo=FALSE,fig.width=6,fig.height=3,fig.align='center',fig.cap="Illustration of the coefficient estimates for our model. The peak of the distribution is at the coefficient estimate, and the spread of the distribution indicates the standard error of the mean for the estimate. The statistical significance of the coefficient is determined by the degree of overlap with 0.", fig.pos = "ht"}

classData <- read.csv("CourseData/classData.csv")

#ggplot(classData,aes(x=Height,y=HandWidth)) + 
#  geom_point() +
#  geom_smooth(method="lm")

mod <- lm(Height~HandWidth,data=classData)
#summary(mod)

int<-summary(mod)$coefficients[1,1:2]
slope<-summary(mod)$coefficients[2,1:2]

interceptPlot <- ggplot(data = data.frame(x = c(-10, 180)), aes(x)) +
    stat_function(fun = dnorm, n = 101, args = list(mean = int[1], sd = int[2])) + ylab("Probability density") + 
    xlab("Intercept value") +
    scale_y_continuous(breaks = NULL) + 
    geom_vline(xintercept = int[1], linetype="dashed",col="red") +
    geom_segment(data = data.frame(y=.002,yend=.002,x=0,xend=int[1]),
                 aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")))+
    geom_vline(xintercept = 0, linetype="dashed",colour="black") +
    #theme_minimal()+
  ggtitle("Intercept coefficient")+
    NULL

slopePlot <- ggplot(data = data.frame(x = c(-.1, 9)), aes(x)) +
    stat_function(fun = dnorm, n = 101, args = list(mean = slope[1], sd = slope[2])) + ylab("Probability density") + 
    xlab("Slope value") +
    scale_y_continuous(breaks = NULL) + 
    geom_vline(xintercept = slope[1], linetype="dashed",col="red") +
    geom_segment(data = data.frame(y=.02,yend=.02,x=0,xend=slope[1]),
                 aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches"))) +
    geom_vline(xintercept = 0, linetype="dashed",colour="black") +
    #theme_minimal()+
  ggtitle("Slope coefficient")+
    NULL

ggpubr::ggarrange(slopePlot,interceptPlot,nrow=1,ncol=2)
#ggsave("Example_Coefficients.png",width = 13.5, height=5, dpi=300, units="cm")

```


The summary then gives some information about the amount of residual variation left after the model has been fitted (this is the $\epsilon$ term in the equations at the start of this worksheet). Then we are told what the $R^2$ value is `r formatC( summary(mod_A)$r.squared, digits = 4, format = "f")`. The adjusted $R^2$ is for use in multiple regression models, where there are many explanatory variables and should not be used for this simple regression model. So what does $R^2$ actually mean? 

$R^2$ is the square of the correlation coefficient $r$ and is a measure of the amount of variation in the response variable (Height) that is explained by the model. If all the points were sitting on the regression line, the $R^2$ value would be 1. This idea is illustrated in Figure \ref{fig:rSquared}.

We could describe the model like this:

*There is a statistically significant association between hand width and height (F = `r formatC( anova(mod_A)$F[1], digits = 4, format = "f")`, d.f. = `r anova(mod_A)$Df[1]`,`r anova(mod_A)$Df[2]`, p < 0.001) The equation of the fitted model is: Height = `r formatC(summary(mod_A)$coef[2,1], digits = 2,format = "f")`($\pm$ `r formatC(summary(mod_A)$coef[2,2], digits = 2,format = "f")`) $\times$ HandWidth + `r formatC(summary(mod_A)$coef[1,1], digits = 2,format = "f")`($\pm$ `r formatC(summary(mod_A)$coef[1,2], digits = 2,format = "f")`). The model explains `r formatC( summary(mod_A)$r.squared*100, digits = 0, format = "f")`% of the variation in height ($R^2$ = `r formatC( summary(mod_A)$r.squared, digits = 3, format = "f")`).*

... or maybe, *The model, which explained `r formatC( summary(mod_A)$r.squared*100, digits = 0, format = "f")`% of the variation in height, showed that the slope of the relationship between hand width and height is `r formatC(summary(mod_A)$coef[2,1], digits = 2,format = "f")` $\pm$ `r formatC(summary(mod_A)$coef[2,2], digits = 2,format = "f")` which is significantly greater than 0 (t = `r formatC(summary(mod_A)$coef[1,3], digits = 2,format = "f")`, p < 0.01)*


```{r rSquared, echo=FALSE,fig.width=4,fig.height=4,fig.align='center',fig.cap="An illustration of different R-squared values.", fig.pos = "ht"}
df1 <- data.frame(x = runif(30,1,10)) %>%
  mutate(y = (x * 1.5 + 0)+rnorm(30,0,0.05))
z<-summary(lm(y~x,data=df1))
cval <- cor(df1$x,df1$y)
R2<-z$r.squared

A<-ggplot(df1,aes(x,y))+
  geom_point()+
  geom_smooth(method = "lm", fullrange = FALSE) + 
  theme_minimal()+
  ggtitle(paste0("R-sq = ",formatC(R2, digits = 2, format = "f"))) +
  NULL
    
df1 <- data.frame(x = runif(30,1,10)) %>%
  mutate(y = (x * 1.5 + 0)+rnorm(30,0,1))
z<-summary(lm(y~x,data=df1))
cval <- cor(df1$x,df1$y)
R2<-z$r.squared

B<-ggplot(df1,aes(x,y))+
    geom_point()+
    geom_smooth(method = "lm", fullrange = FALSE) + 
    theme_minimal()+
    ggtitle(paste0("R-sq = ",formatC(R2, digits = 2, format = "f")))+
    NULL

df1 <- data.frame(x = runif(30,1,10)) %>%
  mutate(y = (x * 1.5 + 0)+rnorm(30,0,3))
z<-summary(lm(y~x,data=df1))
cval <- cor(df1$x,df1$y)
R2<-z$r.squared

C<-ggplot(df1,aes(x,y))+
    geom_point()+
    geom_smooth(method = "lm", fullrange = FALSE) + 
    theme_minimal()+
    ggtitle(paste0("R-sq = ",formatC(R2, digits = 2, format = "f"))) +
    NULL


df1 <- data.frame(x = runif(30,1,10)) %>%
  mutate(y = (x * 1.5 + 0)+rnorm(30,0,8))
z<-summary(lm(y~x,data=df1))
cval <- cor(df1$x,df1$y)
R2<-z$r.squared

D<-ggplot(df1,aes(x,y))+
    geom_point()+
    geom_smooth(method = "lm", fullrange = FALSE) + 
    theme_minimal()+
    ggtitle(paste0("R-sq = ",formatC(R2, digits = 2, format = "f"))) +
    NULL


ggpubr::ggarrange(A,B,C,D,nrow=2,ncol=2)
#ggsave("Example_R2.png",width = 13.5,height=10,dpi=300,units="cm")
```

A plot is usually a good idea because it is easier for the reader to interpret than an equation, or coefficients. The `ggplot2` package has a neat and simple function called `geom_smooth` which will add the fitted regression line to simple models like this. For linear regression models you simply need to tell it to use `method = "lm"`. This will plot the fitted regression model, and will add, by default" a shaded "ribbon" which represents the so called "95% confidence interval" for the fitted values. These are 2 time the standard error.

```{r, fig.width=4.5,fig.height=3.5,fig.align='center', fig.pos = 'ht'}
ggplot(classData,aes(x = HandWidth,y = Height)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

**Question:** If police find the 9.8cm wide hand print at a crime scene, what is your best guess of the height of the person involved?

<!--
predict(mod_A,list(HandWidth = 9.8))
-->
## Exercises - Linear Regression

### Background

Male crickets produce a "chirping" sound by rubbing the edges of their wings together: the male cricket rubs a sharp ridge on his wing against a series ridges on the other wing.  In a 1948 study on striped ground cricket (*Allonemobius fasciatus*), the biologist George W. Pierce recorded the frequency of chirps (vibrations per second) in different temperature conditions.  

Crickets are ectotherms so their physiology and metabolism is influenced by temperature. We therefore believe that temperature might have an effect on their chirp frequency.


### Your task

The data file `chirps.csv` contains data from Pierce's experiments. Your task is to analyse the data and find (1) whether there is a statistically significant relationship between temperature and chirp frequency and (2) what that relationship is.

The data has two columns - `chirps` (the frequency in Hertz) and `temperature` (the temperature in Fahrenheit). You should express the relationship in Celsius.



1) Import the data


2) Use `mutate` to convert Fahrenheit to Celsius (Google it)






3) Plot the data




4) Fit a linear regression model with `lm`



5) Look at diagnostic plots to evaluate the model




6) Use `anova` to figure out if the effect of temperature is statistically significant.




7) Use `summary` to obtain information about the coefficients and $R^2$-value.



8) Summarise the model in words.



9) Add model fit line to the plot.


10) Can I use cricket chirp frequency as a kind of thermometer?


<!--chapter:end:11-linearRegression.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Linear models with categorical and continuous explanatory variables

## Background

In the previous worksheet we looked at linear models where there is a continuous response variable and two categorical explanatory variables (we call this type of linear model two-way ANOVA). In this worksheet we will look at linear models where the explanatory variables are both continuous and categorical. You can think of these as a kind of cross between ANOVA and linear regression. These type of models are often given the name "*ANCOVA*" or *Analysis of Covariance*.

In a simple case, you might be interested in a model with a continuous response variable (e.g. height) and continous and a categorical explanatory variables (e.g. hand width and gender). The categoreical variable may have any number of levels, but the simplest case is with two (e.g. gender with male and female levels). 

Some of these different possible outcomes of this type of analysis are illustrated in Figure \ref{fig:possibilities}. We might see that neither of the two explanatory variables has a significant effect. We might see that one of them does but not the other one. We might see an interaction effect (where the effect of one variable (e.g. hand width) depends on the other (e.g gender). We might also see an interaction effect but no main effect.

```{r ANCOVApossibilities, echo=FALSE,fig.width=10,fig.height=4,fig.align='center',fig.cap="Some potential results of the experiment. There may be a significant effect (or not) of both of the main effects (diet and genotype) and there may be a significant interaction effect (or not).", fig.pos = "ht",message=FALSE}

fakeData <- expand.grid(contVar = c(1,5),categVar = c("A","B"))

#No effect of contVar nor categVar
A<- ggplot(fakeData %>% 
         mutate(y = c(5,5,5.1,5.1)),
       aes(x =contVar,y = y, group=categVar,colour=categVar)) + 
    geom_line(size=2) +
    ylim(0,10)+
  ggtitle("A) cont = N; categ = N; int = N")+
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  NULL

#Effect of contVar but not categVar
B<- ggplot(fakeData %>% 
              mutate(y = c(2.5,7.5,2.6,7.6)),
            aes(x =contVar,y = y, group=categVar,colour=categVar)) + 
    geom_line(size=2) +
    ylim(0,10)+
  ggtitle("B) cont = Y; categ = N; int = N")+
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  NULL


#No effect of contVar but effect of categVar
C<- ggplot(fakeData %>% 
              mutate(y = c(2.5,2.5,7.6,7.6)),
            aes(x =contVar,y = y, group=categVar,colour=categVar)) + 
    geom_line(size=2) +
    ylim(0,10)+
  ggtitle("C) cont = N; categ = Y; int = N")+
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  NULL


#No effect of contVar or categVar, but interaction effect
D<- ggplot(fakeData %>% 
              mutate(y = c(7.6,2.5,2.5,7.6)),
            aes(x =contVar,y = y, group=categVar,colour=categVar)) + 
    geom_line(size=2) +
    ylim(0,10)+
  ggtitle("D) cont = N; categ = N; int = Y")+
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  NULL


#Effect of both contVar and categVar, AND interaction effect
E<- ggplot(fakeData %>% 
              mutate(y = c(2.6,9.5,2.5,5.0)),
            aes(x =contVar,y = y, group=categVar,colour=categVar)) + 
    geom_line(size=2) +
    ylim(0,10)+
  ggtitle("E) cont = Y; categ = Y; int = Y")+
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  NULL


#Effect of both contVar and categVar, AND interaction effect
Fa<- ggplot(fakeData %>% 
              mutate(y = c(2.5,5.0,2.6,9.5)),
            aes(x =contVar,y = y, group=categVar,colour=categVar)) + 
    geom_line(size=2) +
    ylim(0,10)+
  ggtitle("F) cont = Y; categ = Y; int = Y")+
  theme(axis.text.x = element_blank()) + 
  theme(axis.text.y = element_blank()) + 
  NULL


ggpubr::ggarrange(A,B, C,D, E, Fa, nrow=2,ncol=3)



```



## The height ~ hand width example.

In a previous class (linear regression) you explored the relationship between hand width and height. The aim there was (1) to determine if the relationship (i.e. the slope) was significantly differnet from 0. and (2) to make an estimate of what the equation of the relationship would be so you could make predictions of height from hand width.

Here we will extend that example by asking whether there are differences between males and females.

We'll begin by plotting the data (Figure \ref{fig:exampleANCOVA}).


```{r exampleANCOVA, echo=TRUE,fig.width=4.5,fig.height=3,fig.align='center',fig.cap="ANCOVA on hand width vs. height data in males and females", fig.pos = "ht"}
classData <- read.csv("CourseData/classData.csv")
(A<-ggplot(classData,aes(x = HandWidth,y = Height,colour = Gender)) +
  geom_point() + 
  geom_smooth(method ="lm",se = FALSE)) #This shows the ANCOVA model 
                                       #before we have even fit it!
```



You can see that our two continuous variables, `Height` (the response variable) and `HandWidth` (one of the explanatory variables) are associated: There is an overall positive relationship between `HandWidth` and `Height` You can also see that `Gender` (the categorical explanatory variable) is important: males tend to be taller than females for any given hand width. For example, a female with hand width of 9cm is ~172cm tall while a male would be about 180cm tall. This shows us that males have a higher **intercept** than females. There is also a **slight** difference in the slope of the relationship, with males having a slightly steeper slope than females. We already know that the overall relationship between hand width and height is significant (from the regression worksheet). These new observations leave us with the following additional questions: (1) are the intercepts for males and females significantly different? (2) are the slopes for males and females significantly different (or would a model with a single slope, but different intercepts be better)?

Now we can fit our model using the `lm` function. The model formula is `Height ~ HandWidth + Gender + HandWidth:Gender`. The `HandWidth` and `Gender` are the so called **main effects** while `HandWidth:Gender` represents the interaction between them (i.e. it is used to address the question "*does the effect of hand width differ between the sexes?*"). R knows that is fitting an ANCOVA type model rather than a two-way ANOVA because it knows the type of variables that it is dealing with. You can see this if you ask R to tell you what the `class` of the variables are:

```{r}
class(classData$Gender)
class(classData$HandWidth)
```


```{r}
mod_A <- lm(Height ~ HandWidth + Gender + HandWidth:Gender, data = classData)
```


The first step should, as before, be to check out the diagnostic plots. We should not read to much into these in this case, because we have a small sample size.  Nevertheless, lets keep with good habits:

```{r}
library(ggfortify)
autoplot(mod_A)
```

These look good. No evidence of non-normality in the residuals, no heteroscedasticity and no weird outliers.

## Summarising with `anova`

Now we can get the `anova` table of our ANCOVA model (yes, I know that sounds strange.).

```{r}
anova(mod_A)
```

This type of *sequential sum of squares* Analysis of Variance table should be getting fairly familiar to you now, but let's unpack what this means. There are four rows in the summary table - one for each of the terms in the model (`HandWidth`, `Gender` and `HandWidth:Gender`), and one for the `Residuals` (the unexplained variation that remains after fitting the model). The table includes degrees of freedom (`Df`), sum of squares (`Sum Sq`), mean sum of squares (`Mean Sq`) and the associated F and p-values (`F value` and `Pr(>F)`. 


You can interpret the mean sum of squares column in terms of the amount of variation in the response variable (Height) that is explained by the term: The table first tells us the amount of variation (in terms of Mean Sum of Squares) in Height that is captured by a model that includes a common slope for both genders (1038.46). Then it tells us that an *additional* bit of variation (369.46) is captured if we allow the intercepts to vary with gender. Then it tells us that a small additional amount of variation is explained by allowing the slope to vary between the genders (4.89). Finally, there is a bit of unexplained variation left over (Residuals) (25.41). So you can see that hand width explains most variation, followed by gender, followed by the interaction between them.

You would report from this table something like this:

*Hand width and gender both explain a significant amount of the variation in height (ANCOVA - Handwidth: F = 40.862, d.f. = 1 and 27, p<0.001; Gender: F = 14.538, d.f. = 1 and 27, p<0.001). The interaction effect was not significant, which means that the slopes of the relationship between hand width and height are not significantly different (ANCOVA - F = 0.192, d.f. = 1 and 27, p = 0.665).*

It is of course useful to take the interpretation a bit further. You could do this with reference to the plot - e.g. *Figure X shows the clear positive relationship between hand width and height and shows that the intercept for females is smaller than that for males. This which means that, for a given hand width, males tend to be taller.*

## The summary of coefficients (`summary`)

To put some quantitative numbers on this description of the pattern we need to get the summary from R.

```{r}
summary(mod_A)
```

This summary table gives the coefficients of the statistical model, their standard errors, and the t-test results of whether the estimate is greater than 0. This is the same as the `summary` tables given for ANOVA and linear regression.

In the ANOVA `summary` tables, the estimates were given in relation to the *reference level* -- the `(Intercept)` and these ANCOVA `summary` tables are no difference. Interpreting is best done with reference to the graph of the data and fitted model outputs (the graph above).

The reference level (the `(Intercept)`) is the intercept for the line for the first level of the categorical variable (Females, in this case). Here the model estimates that the intercept for Females is at 154.9 (i.e. if you extended the line out to the left it would eventually cross the y-axis at this point). The next coefficient `HandWidth` is the slope of this Female line (1.7667). Then we have `GenderMale`: this coefficient is the difference in intercept between the Female and Male lines. This is followed by the intercept for the interaction term `HandWidth:GenderMale`: this is the difference between slopes for the two genders.

We can therefore do some simple arithmetic to get the equations (i.e. slopes and intercepts) of the lines for both genders. For females this is easy (they are reference level, so you can just read the values directly from the table) - the intercept is 154.93 and the slope is 1.77. 

For males the intercept is 154.93 + 1.37 = 156.30. The slope is 1.77 + 0.88 = 2.65

We could add these equations to our reporting of the results.

*Figure X shows the clear positive relationship between hand width and height and shows that the intercept for females is smaller than that for males. This which means that, for a given hand width, males tend to be taller. The model fit for males is Height = 2.65$\times$HandWidth + 156.30 and the fit for females is Height = 1.77$\times$HandWidth + 154.93*

You could check these by using `geom_abline` to add lines with those equations to the plot (just as a "sanity check").

```{r echo=TRUE,fig.width=4.5,fig.height=3,fig.align='center',fig.cap="", fig.pos = "ht"}
A + 
  geom_abline(intercept = 154.93, slope = 1.77) + 
  geom_abline(intercept = 156.30, slope = 2.65)
```

At the bottom of the `summary` output we are given the $R^2$ values. Because this model has several terms (i.e. variables) in it we should use the adjusted $R^2$ values. These have been corrected for the fact that the model has extra explanatory variables. So in this case, we could report that *the model explains 64% of variation in Height (Adjusted $R^2$ = 0.6368)* - not bad!

So, to describe this `summary` table more generally - the coefficients can be slopes, intercepts, differences between slopes, and differences between intercepts. They are slopes and intercepts for the first level of the categorical variable, and for the subsequent levels they are differences.  Piecing these together can be hard to figure out without reference to the plot of the data and model fits - another good reason to plot your data!


## Simplifying the model

Our results above showed that the interaction between the gender and hand width was not significant. Think about what that means? It means that the effect of hand width on height (the slope) does **not** depend on gender. Therefore, one can argue that we don't need to have a model that estimates both slopes - we could have a simpler model with one slope for both genders.

In fact, creating models that are as simple as possible to explain the observations is a useful goal that is captured by **the law of parsimony** or "*Occam's razor*", which essentially states that simple explanations for a phenomenon are favorable to complex explanations.


Let's refit the model without this non-significant interaction:

```{r}
mod_B <- lm(Height ~ HandWidth + Gender, data = classData)
anova(mod_B)
```

Now all the terms in the model are significant.

```{r}
summary(mod_B)
```

The coefficient summary now gives us a two intercept estimates (152.0964 for females and 152.0964 + 2.1179 = 154.2143 for males) and single estimate for a slope that applies to both genders (2.1179).

Unfortunately, the handy `geom_smooth` function cannot handle this simpler model! We must take a slightly different, and sadly mode complicated approach:

What we need to do is **predict** using the model what the height will be under different conditions. Think of this as "plugging values into an equation".


We want to predict heights across the range of hand widths (from 6.5cm to 11cm), and we need to do this for males and females.

We do this by creating a "fake" dataset to predict from using the useful function `expand.grid`. This function takes inputs from columns of data and "expands" them to ensure that all possible combinations are included.

```{r}
predictData <- expand.grid(HandWidth = c(6.5,11),Gender = c("Male","Female"))
predictData
```

Now we can use these values to `predict` what the heights will be for those particular combinations of values. The arguments for the `predict` function are the model name, then `newdata = ` to give the function the data that you want to predict from. Here we can use the function to add the models predicted fitted value (`fit`) to the `predictData` object we just created.

```{r}
predictData$Height <- predict(mod_B,newdata = predictData)
predictData
```

Now we can add lines for these predicted values to our plot. We do this using the `geom_smooth` function as before,  but this time we use the arguments `data = predictData` to tell R to use the new data, and `stat = "identity"` and to ensure that we plot the data rather than fitting any model.

You may wish to add an error ribbon to these lines. We will cover this in a later class (but see pages 159-164 in the GSWR textbook).


```{r,fig.width=4.5,fig.height=3,fig.align='center',fig.cap="", fig.pos = "ht"}
ggplot(classData,aes(x = HandWidth,y = Height,colour = Gender)) +
  geom_point() + 
  geom_smooth(data = predictData,stat="identity")           
```

We could report this in the usual way but first saying something like: *"The interaction term was not significant (F = 0.1922, d.f. 1 and 27, p = 0.665) and I therefore simplified the model to remove this term. The resulting model with just HandWidth and Gender ... "*






<!--chapter:end:12-ANCOVA.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Generalised linear models

```{r echo=FALSE,message=FALSE}
require(tidyverse)
require(ggpubr)
```

The models we have covered so far are ordinary linear models (including ANOVA, ANCOVA, ordinary linear regression etc.) that assume thatthe relationship between the explanatory variables and the response variable is linear, and that the systematic error in the model is constant (homoscedastic, i.e. the standard deviation of the data does not depend on the magnitude of the explanatory variable). 

In many cases this will not be the case. Non-linearity and heteroscedasticity tend to go hand-in-hand. Sometimes, for example, the data show an exponential growth type of pattern, and/or may be bounded in such a way that the errors cannot be homoscedastic. For example, counts of number of species on islands of different sizes have a lower bound at zero (you can't have negative numbers of species!) and increase exponentially while the standard deviations are are small for small islands and large for large islands.; ratio data or percentage data such as proportion of individuals dying/surviving is bounded between 0 and 1 (0 - 100%).

Transformation of the response variable could an option to linearise these data (although there would be problems with 0 values (because log(0) = -Infinity)), but a second problem is that the ordinary linear model assumes "homoscedasticity" - that the errors in the model are evenly distributed along the explanatory variables. This assumption will be violated in most cases. For example, with count data (e.g. number of species in relation to island size), the errors for very small islands will be smaller than those for large islands. In fact, even if we transform the response variable, for example by log transformation, the predictions of the model will allow errors that include negative counts. This is clearly a problem!

Generalised linear models (GLMs) solve these problems by not only applying a transformation but also explicitly altering the error assumption. They do this using a **link function** to carry out the transformation and by choosing an **error structure** (sometimes referred to as **family**, or **variance function**). The choice of link and error structure can be a bit confusing, but there are so-called "canonical links" that are commonly associated with particular error strucutres. For example, a model for count data would usually have a log link and a Poisson error structure.

The flexibility of the GLM approach means that one can fit GLM versions of all of the models you have already learned about until this points: ANOVA-like GLMs, ANCOVA-like GLMs, ordinary regression-like GLMs and so on. 

Here I will focus on this case, and in the next worksheet I will broaden the focus to illustrate uses of other error structures and links.

## Count data with Poisson errors.

The most common kind of count data where Poisson errors would be expected are frequencies of an event: we know how many times an event happened, but not how many times it did not happen (e.g. births, deaths, lightning strikes).

In these cases:

* Linear model could lead to negative counts.
* Variance of response likely to increase with mean (it usually does with count data).
* Errors are non-normal.
* Zeros difficult to deal with by transformation (e.g. log(0) = -Inf).
* Other error families do not allow zero values.

The standard ("canonical") link used with the Poisson error family is the log link. The log link ensures that all fitted (i.e. predicted) values are positive, while the Poisson errors take account of the fact that the data are integer and the variance scales 1:1 with the mean (i.e. variance increases linearly and is equal to the mean). There are other potential link and error families that *could* be used with this kind of data, but we'll stick with the standard ones here. Lets look at a couple of examples...


### Example: Number of offspring in foxes.

This example uses the `fox.csv` data set. This data set gives the number of offspring produced by a group of foxes, alongside the weight (in kg) of the mothers. Let's import and plot the data.

```{r  echo=FALSE,fig.width=5,fig.height=3,fig.align='center', fig.pos = "ht",message=FALSE}
fox <- read.csv("CourseData/fox.csv")

ggplot(fox, aes(x=weight,y=noffspring)) + 
  geom_point()
```

The first thing to notice is that, like all count data, the data are formed into horizontal rows of data reflecting the fact that the response data are integer values. There is clearly an increasing pattern, but how can we formally test for a statistical relationship. It is obvious that fitting an ordinary linear model though this data would not be the right approach: this would lead to the prediction of negative number of offspring for small foxes, and also, the variance appears to increase with weight/number of offpspring. Therefore this is a good candidate for a GLM. The data are bounded at 0, and are integer values, and for this reason the usual approach would be to fit a GLM with Poisson errors (and the standard log link).


```{r }
mod1 <- glm(noffspring ~ weight, data = fox, family = poisson)
```

After fitting the model it is a good idea to look at the model diagnostics, using `autoplot` from the `ggfortify` package.

```{r echo=FALSE,fig.width=4,fig.height=4,fig.align='center', fig.pos = "ht",message=FALSE}
library(ggfortify)
autoplot(mod1)
```


Now we can ask for the Analysis of Variance table for this model. This is exactly the same procedure as for the previous linear models (ANOVA, ANCOVA etc.) except for GLMs one must also specify that you would like to see the results of significance tests using hte `test = "F"` or `test = "Chi"`.  For Poisson and binomial GLMs the chi-squared test is most appropriate while for  gaussian (normal), quasibinomial and quasipoisson models the F test is most appropriate.


```{r}
anova(mod1,test = "Chi")
```

This summary table tells us that the single explanatory variable (`weight`) is fantastically important (p-value is very small indeed).

We can then ask for the coefficient summary using `summary`.

```{r}
summary(mod1)
```


The model coefficients and their standard errors are given on the scale of the linear predictor. They tell us that there is a significant association between the weight of the fox mother and the number of offspring she will produce: larger foxes produce more offspring. Because the coefficients are given on the scale of the linear predictor rather than on the real scale it is useful to plot predictions of the model to visualise the relationship.

To do that we must (1) tell the model what to predict `from` i.e. we must provide a suitable sequence of numbers to predict from using `seq`, (2) use the `predict` function to predict values (`fit`) from the model. We use the argument `type = "response"` to tell the function that we want the preductions on the backtransformed (real) scale rather than on the scale of the linear predictor. We add the argument `se.fit = TRUE` to tell the function to give us the standard error estimates of the fit. The `se.fit` values are added or subtracted from the fit to obtain the plus/minus standard errors. We can multiply these by 1.96 to get the 95% confidence intervals of the fitted values.

```{r}
#Vector to predict from
newData <- data.frame(weight = seq(1.7,4.4,0.01))

#Predicted values (and SE)
predVals <- predict(mod1,newData,type="response",se.fit = TRUE)

#Create new data for the predicted fit line
newData <- newData %>% 
  mutate(noffspring = predVals$fit) %>% 
  mutate(ymin = predVals$fit - 1.96*predVals$se.fit) %>% 
  mutate(ymax = predVals$fit + 1.96*predVals$se.fit)
```

Take a look at this data to make sure it looks OK.

```{r}
head(newData)
```
This looks OK. Now we can plot the data and add a the model fit line, and a "ribbon" representing the errors (the 95% confidence interval for the line).


```{r echo=FALSE,fig.width=5,fig.height=3,fig.align='center', fig.pos = "ht",message=FALSE}
(A <- ggplot(fox,aes(x= weight,y=noffspring)) + 
  geom_ribbon(data = newData,aes(x =weight,ymin=ymin,ymax=ymax),fill="grey75")+
  geom_point()+
  geom_smooth(data = newData,stat="identity"))

```


So we could summarise this something like this:

Methods: *I modelled the association between mother's weight and number of pups produced using a generalised linear model with a log link and Poisson error structure. This is appropriate because the data are count data (number of pups) that are bounded at 0 with increasing variance with increased maternal weight.*

Results: *The GLM showed that maternal weight was significantly associated with the number of pups produced (GLM: Null Deviance = 166.8, Residual Deviance = 122.7, d.f. = 1 and 98, p <0.001). The slope of the relationship was 0.63 (on the log scale). The equation of the best fit line was log(nOffspring) = -0.75 + 0.63$\times$MotherWeight (see Figure XXX)*




### Example: Cancer clusters

This data show counts of prostate cancer and distance from a nuclear processing plant. Lets take a look at the data.

Let's first import the data and use summary to examine it by plotting  it:

First we can see that there are no negative count values.

```{r  echo=FALSE,fig.width=5,fig.height=3,fig.align='center', fig.pos = "ht",message=FALSE}
cancer <- read.csv("CourseData/cancer.csv")

ggplot(cancer,aes(x = Distance,y = Cancers))+
  geom_point()
```

Again, you will notice that the data are formed into horizontal rows of integer response values. There are lots of zero values at all distances, but the biggest cluster (6 cases), is very close to the plant. But is there a relationship between the distance from the nuclear plant and the number of cancers?

Let's fit a Generalised Linear Model to find out. As before will assume that the error is Poisson (that they variance increases directly in proportion to the mean), and we will use the standard log link to ensure that we don't predict negative values:

```{r }
mod1 <- glm(Cancers ~ Distance, data = cancer, family = poisson)
```

Next, plot the diagnostic plots.

```{r echo=FALSE,fig.width=4,fig.height=4,fig.align='center', fig.pos = "ht",message=FALSE}
autoplot(mod1)
```

These look a bit dodgy, but we'll stick with it for the moment.

Next ask for the Analysis of Variance table.

```{r}
anova(mod1,test = "Chi")
```


The ANOVA table tells us that there is no significant effect of the Distance variable. In other words a model that includes the `Distance` term does not explain significantly more variation than the `NULL` model that includes no terms and instead assumes that variation in cancer incidence is simply caused by random variation.

We needn't go further with this model, but go ahead and plot the model in any case (just for practice). 

```{r}
summary(mod1)
```

Use the approach from the fox example as guidance to make a plot with a fit line.

<!--chapter:end:12-GLM_Poisson.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Extending use cases of GLM

In the previous chapter we used the case of modelling count data, which is bounded at 0 and takes integer values, to understand how Generalised Linear Models work. In this chapter we extend our understanding by looking at some other type of data. 

We will look at three types of binomial data:

* data coded as 0/1
* data on number of successes/failures
* percentage or proportion data

## Logistic regression with binary (0/1) data

Sometimes we are confronted with data where the response variable is simply a success or a failure, or a presence or absence. This kind of data is often best-analysed using a Generalised Linear Model with a binomial error structure (strictly speaking the errors are Bernoulli errors, but these are simply one kind of binomial error). This kind of analysis is often called *logistic regression*. Some examples dead/alive, occupied/empty, healthy/diseased etc.

Again: 

* Linear models could lead to unreasonable values (probabilities \>1 or \<0).
* Variance of response likely to be n-shaped (small near 0 and 1, large inbetween).
* Errors are non-normal.


* Can you think of any other examples of this kind of data?

## Example: species presence/absence data

Our example concerns the presence or absence on a set of islands of a particular species of bird. The response variable we are interested in is called "incidence" and a value of 1 means that the bird is present and breeding on the island, while a value of 0 means that the island has no breeding population there. The explanatory variable is the size of the island in $km^2$ and isolation (the distance of the island from the mainland in km).

Lets take a look at the data.

```{r }
island <- read.csv("CourseData/isolation.csv")

names(island)
```

In this case there are two explanatory variables and we are therefore going to fit a *multiple regression* model with two variables using the GLM framework. 

```{r }
model1 <- glm(incidence ~ area+isolation, data = island, family = binomial)
summary(model1)
```

Looking at coefficients and standard errors (which are on the logit scale) we can see that area has a significant positive effect on the probability of occupation (larger islands are more likely to be occupied) and isolation has a strong negative effect (isolated islands are much less likely to be occupied). To visualise this we can plot the model fits through a scatter plot of the data. It is best to do this separately for each variable so first we must make two separate models, one for each of the variables.

```{r }
modela <- glm(incidence ~ area, data = island, family = binomial)
modeli <- glm(incidence ~ isolation, data = island, family = binomial)
```

Then we can plot the two relationships in graphs side-by-side using the same approach as above. In these plots the fitted values (the modelled line) can be interpreted as a probability.

```{r }
par(mfrow=c(1,2))
plot(island$area,island$incidence)
newData <- data.frame(area = seq(0,9,0.01))
newData$fit <- predict(modela,newData,type="response")
lines(newData$area,newData$fit,col="red")

plot(island$isolation,island$incidence)
newData <- data.frame(isolation = seq(0,10,0.01))
newData$fit <- predict(modeli,newData,type="response")
lines(newData$isolation,newData$fit,col="red")
```

## Binomial regression with proportion/ratio data

Another important class of binomial data is proportion or success/failure data. These data are characterised by the fact that we have information on how many times an event occurred *and* how many times it did *not* occur. These data include infection rates of diseases, sex ratios, percentage mortality etc. These data are bounded in a similar way to the data above, and are also best-analysed using binomial errors.

* Can you think of any other examples of this kind of data?

### Example: sex ratios

In the following example we will look at data from an experiment on sex ratios in an insect species. The experimenter wanted to know whether the population density at which the insect was held had a role in determining the sex ratio. Lets take a look at the data:

```{r }
sexRatio <- read.csv("CourseData/sexratio.csv")
str(sexRatio)
```

You can see that the data are integer counts of males and females. In this case, the density is simply the sum of the females and males.

Let's plot the data as proportion males to see if we can see any pattern. First we can need to add a new column with that information:

```{r }
sexRatio$propMale <- sexRatio$males/(sexRatio$males+sexRatio$females)
```

Now we can plot the data. In this case I use the `ylim` argument to specify that the y-axis should go from 0 to 1 (since this is a proportion).

```{r , tidy=TRUE, results='markup',eval=TRUE, echo=TRUE,fig.cap=('Sex ratio as a function of density')}
plot(sexRatio$density,sexRatio$propMale,ylab = "Proportion male",ylim=c(0,1))
```
It looks like there is a relationship, and it is definitely non-linear so we are justified in using a GLM for the regression.
This regression is different from the last `binomial` GLM because we are not dealing with a simple 0 or 1 response. Instead we have a number in two catefories (male and female). These are analysed by binding (using the `cbind` function) the two vectors of male and female counts into a single object that we will use as the response variable.


```{r }
y = cbind(sexRatio$males, sexRatio$females)
```

Lets have a look:

```{r }
y
```
You can see that this is simply a matrix with two columns.

Now lets fit the model and take a look at the summary output:

```{r }
model <- glm(y ~ density, data = sexRatio,family=binomial)
summary(model)  
```

We can now plot the data and the model's fitted values through it:

```{r }
plot(sexRatio$density,sexRatio$propMale,ylab = "Proportion male",ylim=c(0,1))
newData <- data.frame(density = seq(0,450,1))
newData$fit <- predict(model,newData,type="response")
lines(newData$density,newData$fit,col="red")
```

Hmmm. This doesn't look so great. The model doesn't really capture the initial increase in male sex ratio. Let's see if we can improve it by logging the density variable before fitting.

```{r }
model2 <- glm(y ~ log(density), data = sexRatio,family=binomial)
summary(model2)  
```

There are two ways of easily comparing two GLM models that have the same response variable. Firstly you can look at the residual deviance. Resdual deviance is simply the amount of variation in the response variable that is NOT explained by the model: models with smaller residual deviance are better. You can also look at the AIC (Akaike Information Criterion). The AIC is a summary statistic that tries to capture how well the model fits the data while accounting for how complex the model is. It is too complicated to get into right now except to say that smaller AIC values are better.

By both of these methods, the second model (`model2`) is better than model 1. Lets plot it to confirm that visually.

```{r , tidy=TRUE, results='markup',eval=TRUE, echo=TRUE,fig.cap=('Sex ratio as a function of log density')}
plot(log(sexRatio$density),sexRatio$propMale,ylab = "Proportion male",ylim=c(0,1))
newData <- data.frame(density = seq(0,400,1))
newData$fit <- predict(model2,newData,type="response")
lines(log(newData$density),newData$fit,col="red")
```

Yes, that looks much better. That just goes to show that one should visually inspect the models and consider other ways of fitting the data: there are often ways of improving the fit.

In the next session we'll get some more practice of choosing modelling approaches and interpreting outputs...




<!--chapter:end:13-GLM-otherFamilies.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Power and study design

This chapter will cover aspects of statistical power and study design. It will first focus on answering questions like "what sample size should I use in my experiment?" and "with this sample size, what difference could I detect?" 

The chapter will then focus on study designs such as nested or blocked designs, and what this means in terms of analytical approach. 

## Power analysis by simulation

As you learned in the chapters on t-tests and ANOVA, the detection of a significant difference between treatment groups (if there is one) depends on two things: (1) the actual difference between mean values for the groups (the "signal") and (2) the amount of variation there is in the groups (the "noise"). When there is a lot of noise it is hard to detect the signal.

In most cases we will already have some idea about what to expect when doing a study. Previous work on similar topics, or pilot studies, will have given us an idea of typical values for the response variable, and will give us a ballpark estimate of the amount of variation to expect. This information can be put to use to conduct a power analysis by simulation. 

The gist of this approach is to draw numbers from appropriate distributions to simulate the experiment before actually carrying out the experiment. For example, consider a planned study on bird song volume (amplitude) in relation to ambient noise. Previous work has shown that the amplitude of the particular species we're interested in has a mean value of X with a standard distribution of Y. We could simulate an experiment, with a sample size of 20, by drawing from a normal distribution like this `rnorm(20,mean = X, sd = Y)`

We could then figure out what difference we could detect using a t-test (or linear model) by repeating this sampling many times with different values of X using the `replicate` funtion to repeat the sampling procedure many times. I will illustrate this by using :

```{r}
rnorm(20,mean = 250, sd = 40)
```






<!--chapter:end:14-StudyDesignAndPower.Rmd-->

```{r include=FALSE, cache=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
```
# Coming soon (maybe)!

## Multivariate statistics

This will cover Principal Component Analysis (PCA) which is a handy statistical tool to reduce the complexity of high-dimensional data (i.e. data sets with many columns).

## Transforming variables

It is often desirable to transform the response variable in order to linearise the relationship between it and the explanatory variables. Generalised Linear Models (GLMs) with non-normal error distributions do this, but sometimes it is useful to do this for ordinary linear regression. It is also useful to understand the different transformations that are part of a GLM. I will cover common transformations, and include "Tukey's ladder" of transformations.

## Non-linear regression

Most of the statistics shown in this book focusses on modelling linear relationships between response variables (or their transformed values) and explanatory variables. Sometimes it is useful to explicitly model a known functional form and non-linear regression is a way of doing that.


## Nested or blocked study designs

Sometimes a study will have a design that has some inherent structure. This chapter covers how to account for this with linear models.

<!--chapter:end:99-ComingSoon.Rmd-->


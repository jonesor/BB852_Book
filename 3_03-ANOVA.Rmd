# Linear models with a single categorical explanatory variable

With the previous work on t-tests (and also with randomisation tests), you are now equipped to test for differences between two groups, or between one group and some fixed value. But what if there are more than two groups? 

The answer is to use a one-way analysis of variance (ANOVA). Conceptually, this works the same way as a t-test.

## One-way ANOVA 

The one-way ANOVA is illustrated below with two cases (Figure \@ref(fig:anovaVis)). In both cases there are three groups. These could represent treatment groups in an experiment (e.g. different fertiliser addition to plants). In figure A, the three groups are very close, and the means are not significantly different from each other. In figure B, there is one group that stands apart from the others. The ANOVA will tell us whether *at least one* of the groups is different from the others.


```{r anovaVis, echo=FALSE,fig.width=7,fig.height=3,fig.align='left',fig.cap="Visualising an ANOVA.", fig.pos = "ht",cache=TRUE}

d1<-c(40,10)
d2<-c(45,10)
d3<-c(50,10)

A <- ggplot(data = data.frame(x = c(0, 120)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d3[1], sd = d3[2]),colour="blue") + 
  ylab("Probability density") + 
  xlab("y") +
  scale_y_continuous(breaks = NULL) + 
  #geom_segment(data = data.frame(y=.002,yend=.002,x=d1[1],xend=d2[1]),
  #             aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="black") +
  #geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
  #            aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
  geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
  geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
  geom_vline(xintercept = d3[1], linetype="dashed",colour="blue") +
  #theme_minimal()+
  NULL

d1<-c(40,10)
d2<-c(45,10)
d3<-c(80,10)

B <- ggplot(data = data.frame(x = c(0, 120)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = d1[1], sd = d1[2]),colour = "#007F00") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d2[1], sd = d2[2]),colour="red") + 
  stat_function(fun = dnorm, n = 101, args = list(mean = d3[1], sd = d3[2]),colour="blue") + 
  ylab("Probability density") + 
  xlab("y") +
  scale_y_continuous(breaks = NULL) + 
  #geom_segment(data = data.frame(y=.002,yend=.002,x=d1[1],xend=d2[1]),
  #             aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="black") +
  #geom_segment(data = data.frame(y=.003,yend=.003,x=100,xend=d2[1]),
  #            aes(x=x,xend=xend,y=y,yend=yend),arrow=arrow(ends="both",length = unit(0.1, "inches")),colour="red")+
  geom_vline(xintercept = d1[1], linetype="dashed",colour="#007F00") +
  geom_vline(xintercept = d2[1], linetype="dashed",colour="red") +
  geom_vline(xintercept = d3[1], linetype="dashed",colour="blue") +
  #theme_minimal()+
  NULL

A+ggtitle("A") + B+ggtitle("B")
```



```{r, echo = FALSE,fig.show="hide",cache=TRUE}
#Makes boxplot version of the above for presentation
set.seed(124)
dfA <- data.frame(x = rep(LETTERS[1:3],each = 30),
                  y = c(rnorm(30,40,10),rnorm(30,45,10),rnorm(30,50,10)))

A<-ggplot(dfA, aes(x = x,y =y)) +
  geom_boxplot() + 
  ggtitle("A") + ylim(20,100)

dfB <- data.frame(x = rep(LETTERS[1:3],each = 30),
                  y = c(rnorm(30,40,10),rnorm(30,45,10),rnorm(30,80,10)))

B<-ggplot(dfB, aes(x = x,y =y)) +
  geom_boxplot() + 
  ggtitle("B")+ ylim(20,100)

A+B

```

After figuring out if at least one of the groups is significantly different from the others it is often enough to examine plots (or summary statistics) to see where the differences are (e.g. which group(s) are different from each other). In other cases it might be necessary to do follow up *post-hoc multiple comparison* tests. We will come to those later.


## Fitting an ANOVA in R

New coffee machines use "pods" to make espresso. These have become much more popular than the traditional "bar machines". This data looks at the amount of "crema" or foam produced (a sign of quality!) using three methods: bar machines (BM), Hyper Espresso Pods (HIP) and Illy Espresso System (IES). Are any of these methods better than the others?

```{block, type="do-something"}
Remember to load the `dplyr`, `magrittr` and `ggplot` packages, and to set your working directory correctly.
```


Import the data (`espresso.csv`) and look at it.

```{r}
espresso <- read.csv("CourseData/espresso.csv", stringsAsFactors = TRUE)
head(espresso)
```

```{r coffee,echo=-1,fig.width=4,fig.height=3,fig.align='center', fig.cap="A box and whisker plot, with jittered points, for the espresso foam data.",cache=TRUE}
set.seed(43)
(ggplot(espresso,aes(x=method,y=foamIndx)) + 
geom_boxplot()+
geom_jitter(width=0.2))
```

You can see that the categorical explanatory variable ("method") defines the three treatment groups and has the three levels representing the different coffee types: BM, HIP and IES.

Let's first fit the ANOVA using R. One way ANOVAs are fitted using the `lm` function (`lm` stands for "linear model" - yes, an ANOVA is a type of linear model).

```{r}
foam_mod <- lm(foamIndx ~ method, data = espresso)
```

Before proceeding, we need to check the assumptions of the model. This can be done visually using the `autoplot` function in the `ggfortify` package. If you don't have the package installed, install it now (`install.packages("ggfortify"`)). 

```{r diagnostic,echo=TRUE,fig.width=5,fig.height=4,fig.align='center', fig.cap="Diagnostic plots for the ANOVA model. This looks great.",message = FALSE, warning = FALSE}
library(ggfortify)
autoplot(foam_mod)
```

The main thing to look at here in Figure \@ref(fig:diagnostic) is the "Q-Q" plot on the top right. We want those points to be approximately along the line. If that is the case, then it tells us that the model's residuals are normally distributed (this is one of the assumptions of ANOVA). We may cover these diagnostic plots more thoroughly later. You can find more details on pages 112-113 of the Beckerman et al textbook, or at the following if you are interested: https://data.library.virginia.edu/diagnostic-plots/.

Trust me, everything here looks great.

Now let's evaluate our ANOVA model. We do this using two functions: `anova` and `summary` (it sounds strange, but yes we do use a function called `anova` on our ANOVA model).

First, the `anova`. This gives us the following summary:

```{r}
anova(foam_mod)
```

```{r, echo=FALSE}
x<-anova(foam_mod)
```

This gives some numbers (degrees of freedom, sum of squares, mean squares). These are the important values that go into calculating an *F value* (also called an F-statistic). We will not worry about these details now, except to say that large F-statistics mean that we are more certain that there is a difference between the groups (and that the p-value is smaller).

In this case, the F-value is  `r formatC(x$"F value"[1], digits = 3, format = "f")`. 


As with the t-test, R compares this value to a theoretical distribution (a "table"), based on *two* degrees of freedom. The first one is number of groups minus one, i.e. `r formatC(x$Df[1], digits = 3, format = "f")` in this case. The second one is the overall sample size, minus the number of groups, i.e.  `r formatC(x$Df[2], digits = 3, format = "f")`, in this case. 

This results in a p-value of `r formatC(x$"Pr(>F)"[1], digits = 10, format = "f")` (very highly significant!). 

Based on this p-value we can reject the null hypothesis that there is no difference between groups. We might report this simple result like this:

*The foam index varied significantly among groups (ANOVA: F = `r formatC(x$"F value"[1], digits = 3, format = "f")`, d.f. = `r x$Df[1]` and `r x$Df[2]`, p = `r formatC(x$"Pr(>F)"[1], digits = 3, format = "f")`).*

### Where are the differences?

This model output doesn't tell us *where* those differences are, nor does it tell us what the estimated mean values of foaminess are for the three groups: what are the effects?  We need to dig further into the model to get to these details.

There are several ways to do this and we'll look at one of them. 

We do this using the `summary` function.

```{r}
summary(foam_mod)
```

To properly interpret this output you need to understand something called "treatment contrasts". Essentially, contrasts define how model coefficients (the estimates made by the model) are presented in R outputs.

They are a bit hard to wrap your head aroudn and I STRONGLY recommend that you always do this with reference to a plot of the actual data, and the mean values for your groups. To do this you can use `group_by` and `summarise` to calculate the means for your the levels of your explanatory variable.

```{r}
espresso %>%
group_by(method) %>%
summarise(gp_mean = mean(foamIndx))
```


Look at the coefficients of the model. Remember that you have three levels in your explanatory variable, but only two levels are shown in the summary. Which one is missing? 

The "missing" group is the first one alphabetically (i.e. BM). The estimate (of the mean) for this group is labelled "`(Intercept)`" (with a value of `r foam_mod$coefficients[1]`. This is like a baseline or reference value, and the estimates for the other groups (HIP and IES), are *differences* between this baseline value and the estimated mean for those groups. In other words, the second group (HIP) is `r foam_mod$coefficients[2]` more than `r foam_mod$coefficients[1]` (`r foam_mod$coefficients[1]` + `r foam_mod$coefficients[2]` = `r sum(foam_mod$coefficients[1:2])`). Similarly, the third group (IES) is `r foam_mod$coefficients[3]` more than `r foam_mod$coefficients[1]` (`r foam_mod$coefficients[1]` + `r foam_mod$coefficients[3]` = `r sum(foam_mod$coefficients[c(1,3)])`). Compare these values with the ones you got above using `summarise` - they should be the same. 

This is illustrated below in Figure \@ref(fig:diagnostic)A. You can see that the coefficients of the model are the same as the lengths of the arrows that run from 0 (for the first level of method (BM), the Intercept) or *from* this reference value. It is often a good idea to sketch something like this on paper when you are trying to understand your model outputs!

<!---
Source: A. Parenti, L. Guerrini, P. Masella, S. Spinelli, L. Calamai,
P. Spugnoli (2014). "Comparison of Espresso Coffee Brewing Techniques," 
Journal of Food Engineering, Vol. 121, pp. 112-117.
--->


Likewise, the t-values and p-values, are evaluating *differences between the focal group and this baseline*. Thus in this case, the comparisons (the "contrasts") are between the intercept (BM) and the second level (HIP), and the intercept (BM) and the third level (IES). There is no formal statistical comparison between HIP and IES.

You can see that it is very important to understand the levels of your explanatory variable, and how these relate to the summary outputs of the model. It can be useful to use the function `relevel` to manipulate the explanatory variable to make sure that the output gives you the comparisons you are interested in. Another simple trick would be to always ensure that your reference group (e.g. "control") comes first alphabetically and is therefore selected by R as the intercept (reference point).

For example, we can `relevel` the method variable so that the levels are re-ordered as HIP, BM, then IES so that the comparisons are between zero-HIP, HIP-BM and HIP-IES. (make sure that you understand this before proceeding).

```{r}
#This is what the original data looks like:
levels(espresso$method)

#releveling changes this by changing the reference.
espresso_2 <- espresso %>%
mutate(method = relevel(method,ref = "HIP"))
levels(espresso_2$method)
```

Now we can refit the model with this modified data set and see what difference that made:

```{r}
foam_mod2 <- lm(foamIndx ~ method, data = espresso_2)
anova(foam_mod2)
summary(foam_mod2)
```

Now the coefficients in the model summary look different, but the model is actually the same. Compare the two graphs in Figure \@ref(fig:differentReferences) - can you see the differences/similarities?


```{r differentReferences, echo=FALSE,fig.width=8,fig.height=3,fig.align='center', fig.cap="Comparison illustrating the difference between ANOVA models using (A) BM and (B) HIP as references in the espresso foam data set.",cache=TRUE}
set.seed(31)

temp <- data.frame(level = names(foam_mod$coefficients),
levelNum = 1:3,
coef = foam_mod$coefficients) %>%
mutate(value = c(coef[1],coef[2:3]  +coef[1]))

A<- ggplot(espresso,aes(x=method,y=foamIndx)) + 
geom_jitter(width=0.2,colour = "grey45") + 
geom_point(data = temp, 
mapping = aes(x = levelNum,y= value),inherit.aes=F,colour="red") + 
geom_segment(data = temp,mapping = aes(x=1,y=0,xend=1,yend = value[1]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = temp$value[1],linetype="dashed",colour = "grey60")+
geom_segment(data = temp,mapping = aes(x=2,y=value[1],xend=2,yend = value[2]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_segment(data = temp,mapping = aes(x=3,y=value[1],xend=3,yend = value[3]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = 0) +
annotate("text", x = 1.3, y = temp$value[1]/2, label = temp$value[1])+
annotate("text", x = 2.3, y = sum(temp$value[1:2])/2, label = temp$coef[2])+
annotate("text", x = 3.3, y = sum(temp$value[c(1,3)])/2, label = formatC(temp$coef[3], digits = 1, format = "f"))+
ylim(0,75)  

temp <- data.frame(level = names(foam_mod2$coefficients),
levelNum = 1:3,
coef = foam_mod2$coefficients) %>%
mutate(value = c(coef[1],coef[2:3]  +coef[1]))

B<-ggplot(espresso_2,aes(x=method,y=foamIndx)) + 
geom_jitter(width=0.2,colour = "grey45") + 
geom_point(data = temp, 
mapping = aes(x = levelNum,y= value),inherit.aes=F,colour="red") + 
geom_segment(data = temp,mapping = aes(x=1,y=0,xend=1,yend = value[1]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = temp$value[1],linetype="dashed",colour = "grey60")+
geom_segment(data = temp,mapping = aes(x=2,y=value[1],xend=2,yend = value[2]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_segment(data = temp,mapping = aes(x=3,y=value[1],xend=3,yend = value[3]),arrow = arrow(length = unit(0.2,"cm"))) + 
geom_hline(yintercept = 0) +
annotate("text", x = 1.3, y = temp$value[1]/2, label = temp$value[1])+
annotate("text", x = 2.3, y = sum(temp$value[1:2])/2, label = temp$coef[2])+
annotate("text", x = 3.3, y = sum(temp$value[c(1,3)])/2, label = formatC(temp$coef[3], digits = 1, format = "f"))+
ylim(0,75)  

A+ggtitle("A - default,\nBM reference") + B+ggtitle("B - releveled,\nHIP reference")
```


So, from the first of the model outputs above you can say that BM is significantly different than HIP (t= 7.248, p < 0.0001), but that BM is not significantly different from IES (t = 1.831, p = 0.0796). Then, from the second one you can see that HIP is significantly different from IES (t=-5.417, p < 0.0001). You could write this into the main text, include the information in a figure caption (i.e. add it to Figure \@ref(fig:coffee). 

e.g. *The foam index varied significantly among groups (ANOVA: F = `r formatC(x$"F value"[1], digits = 3, format = "f")`, d.f. = `r x$Df[1]` and `r x$Df[2]`, p = `r formatC(x$"Pr(>F)"[1], digits = 3, format = "f")`). The pairwise comparisons in the ANOVA model showed that means of BM and HIP were significantly different (t= 7.248, p < 0.0001), as were those of HIP and IES (t=-5.417, p < 0.0001), but the BM-IES comparison showed no significant difference (t= 1.831, p= 0.0796)*.

### Tukey's Honestly Significant Difference (HSD)

An alternative way to approach these comparisons between groups is to use something called a **post-hoc multiple comparison test**. The words "post-hoc" mean "after the event" -- i.e. after the ANOVA in this case -- while the "multiple comparison" refers to the (potentially many) pairwise comparisons that you would like to make with an ANOVA. One of the most widely used post-hoc tests is called Tukey’s Honestly Significant Difference (Tukey HSD) test. There is a convenient R package called `agricolae` that will do these for you.

```{r eval=FALSE}
#You only need to do this once!
install.packages("agricolae")
```
When you have the package installed, you can load it (using `library`). Then you can run the Tukey HSD test using the function `HSD.test`. The first argument for the function is the name of the model, followed by the name of the variable you are comparing (in this case `method`) and finally `console = TRUE` tells the function to print the output to your computer screen.

```{r}
library(agricolae)
HSD.test(foam_mod2, "method", console=TRUE)
```

The output is long-winded, and the main thing to look at is the part at the end. The key to understanding this is actually written in the output, "`Treatments with the same letter are not significantly different`". 

You could include these in a figure or a table with text like, *"Means followed by the same letter did not differ significantly (Tukey HSD test, p>0.05)"*.

## ANOVA calculation "by hand".


```{block, type="do-something"}
The following is an optional exercise designed to help you understand how ANOVA works.
```

The ANOVA calculation involves calculating something called an F-value or F-statistic (the F stands for Fisher, who invented ANOVA). This is similar to the t-statistic in that it is a ratio between two quantities, in this case variances. In ANOVA, the F-statistic is calculated as the **treatment variance** divided by the **error variance**.

What does that mean? Let's consider the espresso data set again. 

In the Figure \@ref(fig:differentReferences), you can see on the left (A) the black horizontal line which is the overall mean foam index. The vertical lines are the "errors" or departures from the overall mean value, colour coded by treatment (i.e. method). These can be quantified by summing up their square values (squaring ensures that the summed values are all positive). We call this quantity the *Total Sum of Squares (SSTotal)*. If there is a lot of variation, the sum of squares will be large, if there is little variation the sum of squares will be small. 

On the right hand side (Figure \@ref(fig:differentReferences)B) we see the same data points. However, this time the horizontal lines represent the treatment-specific mean values, and the vertical lines illustrate the errors from these mean values. Again we can sum up these as sum of squares, which we call the *Error Sum of Squares (SSError)*. 

The difference between those values is called the *Treatment Sum of Squares (SSTreatment)* and is the key to ANOVA - it represents the importance of the treatment: $$SSTreatment = SSTotal - SSError$$.

If that doesn't make sense yet, picture the case where the treatment-specific means are all very similar, and are therefore very close to the overall mean. Now the difference between the *Total Sum of Squares* and the *Error Sum of Squares* will be small. Sketch out a couple of examples with pen on paper if that helps. You should now see that you can investigate differences among means by looking at variation.


```{r ANOVAbyhand, echo=FALSE,fig.width=8,fig.height=3,fig.align='center', message = FALSE, fig.cap="The relative size of the squared residual errors from the overall mean (SSTotal) (A) and from the treatment-specific means (SSError) (B) tell us about the importance of the treatment variable. The difference between the two values is the \"treatment sum of squares\".", fig.pos = "ht"}

overallMean <- espresso %>%
summarise(overallMean = mean(foamIndx)) %>%
pull()

#Add index and group means to the data
espressoAugmented <- left_join(espresso,
espresso %>%
group_by(method) %>%
summarise(groupMean = mean(foamIndx))) %>% 
mutate(n = 1:n())


#Plot A = total sum of squares (residuals from overall mean)
SSTotalPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
geom_segment(aes(xend = n, yend = overallMean),alpha=0.4) +
geom_point() + 
geom_hline(yintercept = overallMean) + 
xlab("Index")+
NULL


#Plot B = error sum of squares (residuals from group means)
SSErrorPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
geom_segment(data = espressoAugmented %>% filter(method == "BM"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
geom_segment(data = espressoAugmented %>% filter(method == "HIP"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
geom_segment(data = espressoAugmented %>% filter(method == "IES"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
geom_point() + 
geom_segment(data = espressoAugmented %>% filter(method == "BM"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
geom_segment(data = espressoAugmented %>% filter(method == "HIP"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
geom_segment(data = espressoAugmented %>% filter(method == "IES"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
xlab("Index")+
NULL

SSTotalPlot + ggtitle("A - Total errors\n SSTotal") + SSErrorPlot + ggtitle("B - Treatment-specific error\n SSError")


```


In the following I will show how these calculations can be done "by hand" in R. The purpose of showing you this is to demonstrate exactly how the `lm` model that you fitted above works, and prove to yourself that it is not rocket science... you will never need to do this in real life, because you have the wonderful `lm` function.

Here goes...

First, calculate the total sum of squares:

```{r}
(SSTotal = sum((overallMean-espresso$foamIndx)^2))
```

Now calculate the group-specific means:

```{r}
(groupMeans<-espresso %>%
group_by(method) %>%
summarise(groupMean = mean(foamIndx)) %>%
pull(groupMean))
```

Now add those group-specific mean values to the dataset using `left_join` so that you can calculate the group-specific errors.

```{r}
espresso <- left_join(espresso,espresso %>%
                        group_by(method) %>%
                        summarise(groupMean = mean(foamIndx))) %>%
  mutate(groupSS = (foamIndx - groupMean)^2)

head(espresso)
```

Then, to calculate the errors:

```{r}
(SSError <- espresso %>%
summarise(sum(groupSS)) %>%
pull())
```



From there, you can calculate the **Treatment Sum of Squares**

```{r}
(SSTreatment <- SSTotal - SSError)
```

So far, so good - but we can't just look at the ratio of SSTreatment/SSError, because sum of square errors always increase with sample size. We can account for this by dividing 

We need to take account of sample size (degrees of freedom) by dividing these sum of squares by the degrees of freedom to give us variances. There are 3 treatment groups and 9 samples per group. Therefore there are 2 degrees of freedom for the treatment, and 8 degrees of freedom per each of the three treatments, giving a total of 8*3 = 24 error degrees of freedom.

Now we need to correct for degrees of freedom, which will give us variances.

```{r}
(meanSSTreatment <- SSTreatment/2)
(meanSSError <- SSError/24)
```

The F-statistic is then the ratio of these values.

```{r}
(Fstat <- meanSSTreatment/meanSSError)
```

We can "look up" the p-value associated with this F-statistic using the `pf` function (`pf` stands for probability of f) like this:

```{r}
1-pf(Fstat,df1=2,df2=24)
```

As you can see, the method is a bit laborious and time consuming but it is conceptually fairly straightforward - it all hinges on the ratio of variation due to treatment effect vs. overall variation. Signal and noise.


<!-- the following is some code to make a plot with shuffled data as an illustration of how the ANOVA works --->
```{r, echo=FALSE}

espressoShuffled <- espresso %>%
  mutate(foamIndx = sample(foamIndx))

overallMean <- espressoShuffled %>%
  summarise(overallMean = mean(foamIndx)) %>%
  pull()

#Add index and group means to the data
espressoAugmented <- left_join(espressoShuffled,
                               espressoShuffled %>%
                                 group_by(method) %>%
                                 summarise(groupMean = mean(foamIndx))) %>% 
  mutate(n = 1:n())


#Plot A = total sum of squares (residuals from overall mean)
SSTotalPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
  geom_segment(aes(xend = n, yend = overallMean),alpha=0.4) +
  geom_point() + 
  geom_hline(yintercept = overallMean) + 
  xlab("Index")+
  NULL


#Plot B = error sum of squares (residuals from group means)
SSErrorPlot <- ggplot(espressoAugmented,aes(x=n,y=foamIndx,colour=method)) +
  geom_segment(data = espressoAugmented %>% filter(method == "BM"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
  geom_segment(data = espressoAugmented %>% filter(method == "HIP"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
  geom_segment(data = espressoAugmented %>% filter(method == "IES"), aes(xend = n, yend = groupMean,colour = method),alpha=0.4) +
  geom_point() + 
  geom_segment(data = espressoAugmented %>% filter(method == "BM"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
  geom_segment(data = espressoAugmented %>% filter(method == "HIP"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
  geom_segment(data = espressoAugmented %>% filter(method == "IES"),aes(x = min(n),y=mean(foamIndx),xend=max(n),yend=mean(foamIndx))) +
  xlab("Index")+
  NULL

xx <- SSTotalPlot + ggtitle("A - Total errors\n SSTotal") + SSErrorPlot + ggtitle("B - Treatment-specific error\n SSError")

```

## Exercise: Apple tree crop yield

<!-- to produce the dataset
```{r}
#require(agridat)
#apples <- agridat::archbold.apple %>%
#  select(spacing,yield)
#write.csv(file= "CourseData/apples.csv",x = apples,row.names = FALSE)
```
-->

An experiment was conducted to look at the effect of tree spacing on apple crop yield (total kg per tree of apples between 1975-1979) in different spacing conditions (i.e. distances between trees). There were 40 trees in each treatment group. The spacing was 6, 10 and 14 feet, and should be treated as a categorical variable. There may be some `NA` missing yield values.

Import the data (`apples.csv`) and analyse it using the techniques you have learned in the ANOVA lecture, and the previous chapter, to answer the question "What is the effect of tree spacing on apple yields?"

<!--
```{r}
apples <- read.csv("CourseData/apples.csv")
summary(apples)
```
-->

1) Import and look at the data (e.g. `summary` or `str` or `head`)

<!--
```{r}
apples <- read.csv("CourseData/apples.csv")
summary(apples)
```
-->

2) Ensure that the explanatory variable (`spacing`) is defined as a categorical variable (i.e. a "factor", in R-speak). You can use `mutate` and `as.factor` functions for this.

<!--
```{r}
apples <- apples %>%
  mutate(spacing = as.factor(spacing))
```
-->


3) Plot the data using `ggplot` (a box plot with (optionally) added jittered points would be good).

<!--
```{r}
ggplot(apples,aes(x = spacing,y = yield)) +
  geom_boxplot() + 
  geom_jitter(width=0.2) +
  ylab("Yield (kg per tree)")
```
-->

4) Fit an ANOVA model using `lm`. 

<!--
```{r}
apple_mod <- lm(yield~spacing,data = apples)
```
-->

5) Check the model using a diagnostic plot (i.e. using `autoplot` from the `ggfortify` package).

<!--
```{r}
library(ggfortify)
autoplot(apple_mod)
```
-->

6) Use `anova` to get the ANOVA summary.

<!--
```{r}
anova(apple_mod)
```
-->

7) You should see that there are differences among treatments. But where are those differences? Use `summary` on  your model to find out.

<!--
```{r}
summary(apple_mod)
```
-->

8) Use a Tukey test to make all the pairwise comparisons among the treatments.

<!--
```{r}
library(agricolae)
HSD.test(apple_mod,"spacing",console=TRUE)
```
-->

9) Write a few sentences that summarise your findings. What biological processes do you think drive the effects that you have detected?

10) Optional. Instead of using a Tukey test, use the alternative "relevel" approach to make the missing comparison.


```{block, type="do-something"}
If you get this far, try using the ANOVA approach on one of the previous t-test examples (remember that ANOVA can be used when your single explanatory variable has TWO or more levels). You should notice that the results are the same whether you use the `t.test` function or the ANOVA approach with `lm`.
```

